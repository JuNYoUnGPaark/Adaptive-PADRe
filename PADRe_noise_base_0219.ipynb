{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Seed / Device\n",
        "# ---------------------------\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Seed: {seed}\")\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset\n",
        "# ---------------------------\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_dir, split=\"train\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split = split\n",
        "        self.X, self.y = self._load_data()\n",
        "        self.X = torch.FloatTensor(self.X)               # (N, C, T)\n",
        "        self.y = torch.LongTensor(self.y) - 1            # 1..6 -> 0..5\n",
        "\n",
        "    def _load_data(self):\n",
        "        split_dir = self.data_dir / self.split\n",
        "        signal_types = [\n",
        "            \"body_acc_x\", \"body_acc_y\", \"body_acc_z\",\n",
        "            \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\",\n",
        "            \"total_acc_x\", \"total_acc_y\", \"total_acc_z\",\n",
        "        ]\n",
        "        signals = []\n",
        "        for st in signal_types:\n",
        "            fname = split_dir / \"Inertial Signals\" / f\"{st}_{self.split}.txt\"\n",
        "            arr = np.loadtxt(fname)                      # (N, 128)\n",
        "            signals.append(arr)\n",
        "        X = np.stack(signals, axis=1)                    # (N, 9, 128)\n",
        "        y = np.loadtxt(split_dir / f\"y_{self.split}.txt\", dtype=int)\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Gate\n",
        "# ---------------------------\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self, channels, max_degree=3, gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0, temperature_min=0.5):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            try:\n",
        "                self.gate[-1].bias.data[1] = 0.4\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, temperature):\n",
        "        self.temperature.fill_(max(float(temperature), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)  # (B, K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_one_hot = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_selection = hard_one_hot - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_selection = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_selection, logits, soft_probs\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# PADRe Block\n",
        "# ---------------------------\n",
        "class TrueComputeBudgetedPADReBlock_Slim(nn.Module):\n",
        "    def __init__(self, channels, seq_len, max_degree=3, kernel_size=11, gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0, temperature_min=0.5):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.seq_len = seq_len\n",
        "        self.max_degree = max_degree\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min,\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=kernel_size,\n",
        "                      padding=kernel_size // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1)\n",
        "            for _ in range(max_degree - 1)\n",
        "        ])\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=kernel_size,\n",
        "                      padding=kernel_size // 2, groups=channels)\n",
        "            for _ in range(max_degree - 1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm([channels, seq_len])\n",
        "        self.degree_usage_stats = defaultdict(float)\n",
        "        self.num_forward_calls = 0\n",
        "\n",
        "    def _compute_prefix(self, x, selected_degrees):\n",
        "        B, C, T = x.shape\n",
        "        max_degree_needed = int(selected_degrees.max().item()) + 1\n",
        "        max_degree_needed = max(1, min(max_degree_needed, self.max_degree))\n",
        "\n",
        "        Y = []\n",
        "        for i in range(max_degree_needed):\n",
        "            y = self.channel_mixing[i](x)\n",
        "            y = self.token_mixing[i](y)\n",
        "            Y.append(y)\n",
        "\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_degree_needed):\n",
        "            z_prev = self.pre_hadamard_channel[i - 1](Z[-1])\n",
        "            z_prev = self.pre_hadamard_token[i - 1](z_prev)\n",
        "            z_curr = z_prev * Y[i]\n",
        "            Z.append(z_curr)\n",
        "\n",
        "        Z_stack = torch.stack(Z, dim=0)                  # (D, B, C, T)\n",
        "        idx_b = torch.arange(B, device=x.device)\n",
        "        output = Z_stack[selected_degrees, idx_b]        # (B, C, T)\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        degree_selection, logits, soft_probs = self.degree_gate(x)\n",
        "        selected_degrees = degree_selection.argmax(dim=-1)\n",
        "\n",
        "        out = self._compute_prefix(x, selected_degrees)\n",
        "        out = self.norm(out)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for deg in selected_degrees:\n",
        "                self.degree_usage_stats[int(deg.item())] += 1.0\n",
        "            self.num_forward_calls += x.shape[0]\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_selection,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (selected_degrees + 1).float().mean().item(),\n",
        "            }\n",
        "        return out\n",
        "\n",
        "    def get_degree_usage(self):\n",
        "        if self.num_forward_calls == 0:\n",
        "            return {i: 0.0 for i in range(self.max_degree)}\n",
        "        return {deg: usage / self.num_forward_calls for deg, usage in self.degree_usage_stats.items()}\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Model\n",
        "# ---------------------------\n",
        "class ComputeBudgetedPADReHAR_Slim(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.seq_len = seq_len\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            TrueComputeBudgetedPADReBlock_Slim(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                kernel_size=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm([hidden_dim, seq_len]) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm([hidden_dim, seq_len]) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, temperature):\n",
        "        for block in self.padre_blocks:\n",
        "            block.degree_gate.set_temperature(temperature)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)                           # (B, H, T)\n",
        "\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute_cost = 0.0\n",
        "\n",
        "        for padre_block, ffn, norm1, norm2 in zip(self.padre_blocks, self.ffn, self.norms1, self.norms2):\n",
        "            residual = x\n",
        "            if return_gate_info:\n",
        "                x, gate_info = padre_block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gate_info)\n",
        "                total_compute_cost += gate_info[\"compute_cost\"]\n",
        "            else:\n",
        "                x = padre_block(x)\n",
        "            x = norm1(x + residual)\n",
        "\n",
        "            residual = x\n",
        "            x = ffn(x)\n",
        "            x = norm2(x + residual)\n",
        "\n",
        "        feat = self.global_pool(x).squeeze(-1)\n",
        "        logits = self.classifier(feat)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return logits, gate_info_list, total_compute_cost\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Corruptions\n",
        "# ---------------------------\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "def add_bias_drift(X, beta, mode=\"constant\", per_channel=True):\n",
        "    B, C, T = X.shape\n",
        "    device = X.device\n",
        "\n",
        "    if mode == \"constant\":\n",
        "        if per_channel:\n",
        "            b = torch.randn(B, C, 1, device=device) * beta\n",
        "        else:\n",
        "            b = torch.randn(B, 1, 1, device=device) * beta\n",
        "        return X + b\n",
        "\n",
        "    if mode == \"brownian\":\n",
        "        if per_channel:\n",
        "            eps = torch.randn(B, C, T, device=device) * beta\n",
        "        else:\n",
        "            eps = torch.randn(B, 1, T, device=device) * beta\n",
        "        drift = torch.cumsum(eps, dim=-1)\n",
        "        return X + drift\n",
        "\n",
        "    raise ValueError(\"mode must be one of ['constant','brownian']\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def expected_degree_from_gateinfo(gate_info_list):\n",
        "    layer_exp = []\n",
        "    for gi in gate_info_list:\n",
        "        sp = gi[\"soft_probs\"]                            # (B,K)\n",
        "        K = sp.shape[1]\n",
        "        deg_vals = torch.arange(1, K + 1, device=sp.device).float()\n",
        "        exp = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "        layer_exp.append(exp)\n",
        "    return layer_exp, float(np.mean(layer_exp))\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Robust Evaluator\n",
        "# ---------------------------\n",
        "class RobustEvaluator:\n",
        "    def __init__(self, trainer):\n",
        "        self.trainer = trainer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_once(self, snr_db=None, bias_beta=None, bias_mode=\"constant\", measure_gate=True):\n",
        "        model = self.trainer.model\n",
        "        model.eval()\n",
        "\n",
        "        all_preds, all_labels = [], []\n",
        "        layer_sum = None\n",
        "        layer_count = 0\n",
        "\n",
        "        for X, y in self.trainer.val_loader:\n",
        "            X = X.to(self.trainer.device)\n",
        "            y = y.to(self.trainer.device)\n",
        "\n",
        "            if snr_db is not None:\n",
        "                X = add_gaussian_noise(X, snr_db)\n",
        "            if bias_beta is not None:\n",
        "                X = add_bias_drift(X, beta=bias_beta, mode=bias_mode, per_channel=True)\n",
        "\n",
        "            if measure_gate:\n",
        "                logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "                layer_exp, _ = expected_degree_from_gateinfo(gate_info_list)\n",
        "                if layer_sum is None:\n",
        "                    layer_sum = np.zeros(len(layer_exp), dtype=np.float64)\n",
        "                layer_sum += np.array(layer_exp, dtype=np.float64)\n",
        "                layer_count += 1\n",
        "            else:\n",
        "                logits = model(X)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_preds = np.array(all_preds)\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "        per_class_f1 = f1_score(all_labels, all_preds, average=None)\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        gate_layer_exp = None\n",
        "        if measure_gate and layer_count > 0:\n",
        "            gate_layer_exp = (layer_sum / layer_count).tolist()\n",
        "\n",
        "        return {\n",
        "            \"acc\": acc,\n",
        "            \"macro_f1\": macro_f1,\n",
        "            \"per_class_f1\": per_class_f1,\n",
        "            \"cm\": cm,\n",
        "            \"gate_layer_expected_degree\": gate_layer_exp,\n",
        "        }\n",
        "\n",
        "    def instant_noise_sweep(self, snr_list_db):\n",
        "        results = []\n",
        "        for snr in snr_list_db:\n",
        "            out = self.eval_once(snr_db=snr, measure_gate=True)\n",
        "            out[\"snr_db\"] = (\"inf\" if snr is None else snr)\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "    def linear_snr_drift(self, snr_start=30, snr_end=0, steps=10):\n",
        "        snrs = np.linspace(snr_start, snr_end, steps).tolist()\n",
        "        time_series = []\n",
        "        for t, snr in enumerate(snrs):\n",
        "            out = self.eval_once(snr_db=float(snr), measure_gate=True)\n",
        "            time_series.append({\n",
        "                \"t\": t,\n",
        "                \"snr_db\": float(snr),\n",
        "                \"acc\": out[\"acc\"],\n",
        "                \"macro_f1\": out[\"macro_f1\"],\n",
        "                \"gate_layer_expected_degree\": out[\"gate_layer_expected_degree\"],\n",
        "            })\n",
        "        return time_series\n",
        "\n",
        "    def bias_drift_sweep(self, beta_list, mode=\"constant\"):\n",
        "        results = []\n",
        "        for beta in beta_list:\n",
        "            out = self.eval_once(bias_beta=beta, bias_mode=mode, measure_gate=True)\n",
        "            out[\"bias_beta\"] = beta\n",
        "            out[\"bias_mode\"] = mode\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Trainer\n",
        "# ---------------------------\n",
        "class SimpleTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, device=\"cuda\"):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50, eta_min=1e-5)\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        all_preds, all_labels = [], []\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for X, y in self.train_loader:\n",
        "            X = X.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            logits = self.model(X)\n",
        "            loss = self.criterion(logits, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        self.scheduler.step()\n",
        "        return total_loss / max(1, len(self.train_loader)), acc\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        for X, y in self.val_loader:\n",
        "            X = X.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "            logits = self.model(X)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "        return acc, macro_f1\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Gate stats (FIXED): degree histogram + classwise expected degree\n",
        "# ---------------------------\n",
        "@torch.no_grad()\n",
        "def collect_gate_stats(\n",
        "    model,\n",
        "    loader,\n",
        "    device,\n",
        "    num_classes=6,\n",
        "    max_degree=3,\n",
        "    snr_db=None,\n",
        "    bias_beta=None,\n",
        "    bias_mode=\"constant\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      hist_counts: np.ndarray shape (L, K)  # per-layer degree selection counts\n",
        "      hist_ratio : np.ndarray shape (L, K)  # per-layer selection ratios\n",
        "      class_exp  : np.ndarray shape (num_classes, L)  # per-class expected degree (avg over samples)\n",
        "      class_cnt  : np.ndarray shape (num_classes,)    # sample counts per class\n",
        "\n",
        "    NOTE (FIX):\n",
        "      class_cnt must be counted ONCE per sample (per batch), not once per layer.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    L = len(model.padre_blocks)  # num_layers\n",
        "    K = max_degree\n",
        "\n",
        "    hist_counts = np.zeros((L, K), dtype=np.int64)\n",
        "    class_sum = np.zeros((num_classes, L), dtype=np.float64)\n",
        "    class_cnt = np.zeros((num_classes,), dtype=np.int64)\n",
        "\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, snr_db)\n",
        "        if bias_beta is not None:\n",
        "            X = add_bias_drift(X, beta=bias_beta, mode=bias_mode, per_channel=True)\n",
        "\n",
        "        _, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "\n",
        "        # --- FIX: count samples per class ONCE per batch ---\n",
        "        y_cpu = y.detach().cpu().numpy()\n",
        "        for c in range(num_classes):\n",
        "            class_cnt[c] += int((y_cpu == c).sum())\n",
        "\n",
        "        # per-layer accumulation\n",
        "        for li, gi in enumerate(gate_info_list):\n",
        "            deg_sel = gi[\"degree_selection\"].argmax(dim=-1)  # 0..K-1\n",
        "            bc = torch.bincount(deg_sel, minlength=K).detach().cpu().numpy()\n",
        "            hist_counts[li] += bc\n",
        "\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            deg_vals = torch.arange(1, K + 1, device=sp.device).float()\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1)  # (B,)\n",
        "\n",
        "            exp_cpu = exp_deg.detach().cpu().numpy()\n",
        "            for c in range(num_classes):\n",
        "                mask = (y_cpu == c)\n",
        "                if mask.any():\n",
        "                    class_sum[c, li] += exp_cpu[mask].sum()\n",
        "\n",
        "    hist_ratio = hist_counts / np.maximum(hist_counts.sum(axis=1, keepdims=True), 1)\n",
        "    class_exp = class_sum / np.maximum(class_cnt[:, None], 1)\n",
        "\n",
        "    return hist_counts, hist_ratio, class_exp, class_cnt\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_snr_suite_report(\n",
        "    model,\n",
        "    loader,\n",
        "    device,\n",
        "    snr_list_db,\n",
        "    num_classes=6,\n",
        "    max_degree=3,\n",
        "    class_names=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    For each SNR:\n",
        "      - Acc / Macro-F1\n",
        "      - Hard avg degree (per-layer) and total hard compute (= sum over layers)\n",
        "      - Per-layer degree histogram (hard selections)\n",
        "\n",
        "    Notes:\n",
        "      - degree index: 0..K-1  => degree = 1..K (we report as degree=1..K)\n",
        "      - \"Hard\" means argmax(degree_selection), not soft_probs expectation.\n",
        "    \"\"\"\n",
        "    if class_names is None:\n",
        "        class_names = [f\"class_{i}\" for i in range(num_classes)]\n",
        "\n",
        "    model.eval()\n",
        "    L = len(model.padre_blocks)\n",
        "    K = max_degree\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"[SNR Suite Report]\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    for snr_db in snr_list_db:\n",
        "        # accumulators\n",
        "        all_preds, all_labels = [], []\n",
        "        hist_counts = np.zeros((L, K), dtype=np.int64)\n",
        "        deg_sum = np.zeros((L,), dtype=np.float64)\n",
        "        sample_count = 0\n",
        "\n",
        "        for X, y in loader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # apply noise\n",
        "            if snr_db is not None:\n",
        "                X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "            logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "            B = X.shape[0]\n",
        "            sample_count += B\n",
        "\n",
        "            # per-layer hard degree stats\n",
        "            for li, gi in enumerate(gate_info_list):\n",
        "                # hard selection: 0..K-1\n",
        "                deg_idx = gi[\"degree_selection\"].argmax(dim=-1)  # (B,)\n",
        "                bc = torch.bincount(deg_idx, minlength=K).detach().cpu().numpy()\n",
        "                hist_counts[li] += bc\n",
        "\n",
        "                # hard degree (1..K)\n",
        "                deg_sum[li] += (deg_idx.float() + 1.0).sum().item()\n",
        "\n",
        "        # metrics\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_preds = np.array(all_preds)\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "        # hard avg degree\n",
        "        deg_mean_per_layer = (deg_sum / max(sample_count, 1)).tolist()\n",
        "        hard_total_compute = float(np.sum(deg_mean_per_layer))  # sum over layers (like total cost)\n",
        "\n",
        "        # print header\n",
        "        snr_str = \"inf\" if snr_db is None else f\"{snr_db:g}\"\n",
        "        print(\"\\n--------------------------------\")\n",
        "        print(f\"SNR={snr_str} dB\")\n",
        "        print(f\"  Acc={acc:.4f} | Macro-F1={macro_f1:.4f}\")\n",
        "        print(f\"  HardAvgDegree(per-layer)={[round(x,4) for x in deg_mean_per_layer]}\")\n",
        "        print(f\"  HardTotalCompute(sum layers)={hard_total_compute:.4f}\")\n",
        "\n",
        "        # histogram ratios\n",
        "        hist_ratio = hist_counts / np.maximum(hist_counts.sum(axis=1, keepdims=True), 1)\n",
        "\n",
        "        print(\"  Degree histogram per layer (hard)\")\n",
        "        for li in range(L):\n",
        "            cnt = hist_counts[li].tolist()\n",
        "            rat = [f\"{x:.3f}\" for x in hist_ratio[li].tolist()]\n",
        "            print(f\"    Layer {li}: counts={cnt} | ratio={rat} (deg=1..{K})\")\n",
        "\n",
        "\n",
        "def print_gate_stats(title, hist_counts, hist_ratio, class_exp, class_cnt, class_names=None):\n",
        "    L, K = hist_counts.shape\n",
        "    if class_names is None:\n",
        "        class_names = [f\"class_{i}\" for i in range(class_exp.shape[0])]\n",
        "\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"[Gate Stats] {title}\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    print(\"\\n(1) Degree histogram per layer\")\n",
        "    for li in range(L):\n",
        "        cnt = hist_counts[li].tolist()\n",
        "        rat = [f\"{x:.3f}\" for x in hist_ratio[li].tolist()]\n",
        "        print(f\"  Layer {li}: counts={cnt} | ratio={rat}  (deg=1..{K})\")\n",
        "\n",
        "    print(\"\\n(2) Class-wise expected degree (per layer)\")\n",
        "    for ci, name in enumerate(class_names):\n",
        "        if class_cnt[ci] == 0:\n",
        "            continue\n",
        "        vals = [f\"{v:.3f}\" for v in class_exp[ci].tolist()]\n",
        "        print(f\"  {name:>12s} (n={class_cnt[ci]}): {vals}\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Main\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_DIR = \"/content/drive/MyDrive/Colab Notebooks/HAR/DONE/har_orig_datasets/UCI_HAR\"\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 30\n",
        "\n",
        "    train_ds = UCIHARDataset(DATA_DIR, split=\"train\")\n",
        "    val_ds = UCIHARDataset(DATA_DIR, split=\"test\")\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = ComputeBudgetedPADReHAR_Slim(\n",
        "        in_channels=9, seq_len=128, num_classes=6,\n",
        "        hidden_dim=48, num_layers=3, max_degree=3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    trainer = SimpleTrainer(model, train_loader, val_loader, device=DEVICE)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    for ep in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "        train_loss, train_acc = trainer.train_epoch()\n",
        "        val_acc, val_f1 = trainer.evaluate()\n",
        "        t1 = time.time()\n",
        "\n",
        "        print(f\"Epoch {ep+1:03d}/{EPOCHS} | Train Loss {train_loss:.4f} Acc {train_acc:.4f} | \"\n",
        "              f\"Val Acc {val_acc:.4f} Macro-F1 {val_f1:.4f} | {t1-t0:.1f}s\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            torch.save(trainer.model.state_dict(), \"best_padre_slim.pt\")\n",
        "\n",
        "    print(\"Done. Best Val Macro-F1:\", best_f1)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Robustness Experiments Start\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    # 1) Load best\n",
        "    trainer.model.load_state_dict(torch.load(\"best_padre_slim.pt\", map_location=DEVICE))\n",
        "    trainer.model.eval()\n",
        "\n",
        "    # 2) SNR list (include 0)\n",
        "    snr_list = [None, 30, 20, 10, 5, 0]\n",
        "\n",
        "    # 3) \"Hard\" report: Acc/Macro-F1 + Hard degree + histogram  ✅(너가 원하는 핵심)\n",
        "    eval_snr_suite_report(\n",
        "        trainer.model,\n",
        "        val_loader,\n",
        "        DEVICE,\n",
        "        snr_list_db=snr_list,\n",
        "        num_classes=6,\n",
        "        max_degree=3,\n",
        "        class_names=[\"WALK\", \"UP\", \"DOWN\", \"SIT\", \"STAND\", \"LAY\"],\n",
        "    )\n",
        "\n",
        "    # 4) Optional: 기존 gate 통계(soft expected degree + classwise expected degree)\n",
        "    CLASS_NAMES = [\"WALK\", \"UP\", \"DOWN\", \"SIT\", \"STAND\", \"LAY\"]\n",
        "\n",
        "    hc, hr, ce, cc = collect_gate_stats(\n",
        "        trainer.model, val_loader, DEVICE,\n",
        "        num_classes=6, max_degree=3,\n",
        "        snr_db=None, bias_beta=None\n",
        "    )\n",
        "    print_gate_stats(\"CLEAN (soft exp + classwise)\", hc, hr, ce, cc, class_names=CLASS_NAMES)\n",
        "\n",
        "    hc, hr, ce, cc = collect_gate_stats(\n",
        "        trainer.model, val_loader, DEVICE,\n",
        "        num_classes=6, max_degree=3,\n",
        "        snr_db=10, bias_beta=None\n",
        "    )\n",
        "    print_gate_stats(\"GAUSSIAN NOISE (SNR=10) (soft exp + classwise)\", hc, hr, ce, cc, class_names=CLASS_NAMES)\n",
        "\n",
        "    # 5) RobustEvaluator (표에 넣을 숫자용)  ✅0 포함해서 통일\n",
        "    evaluator = RobustEvaluator(trainer)\n",
        "\n",
        "    A = evaluator.instant_noise_sweep(snr_list)\n",
        "    print(\"\\n[A] Instant noise sweep (SNR) [SOFT expected degree]\")\n",
        "    for r in A:\n",
        "        print(f\"  SNR={r['snr_db']} | Acc={r['acc']:.4f} | Macro-F1={r['macro_f1']:.4f} | \"\n",
        "              f\"GateExp(per-layer)={r['gate_layer_expected_degree']}\")\n",
        "\n",
        "    B = evaluator.linear_snr_drift(snr_start=30, snr_end=0, steps=10)\n",
        "    print(\"\\n[B] Linear SNR drift over time\")\n",
        "    for row in B:\n",
        "        print(f\"  t={row['t']:02d} | SNR={row['snr_db']:.1f} | Acc={row['acc']:.4f} | \"\n",
        "              f\"Macro-F1={row['macro_f1']:.4f} | GateExp={row['gate_layer_expected_degree']}\")\n",
        "\n",
        "    beta_list = [0.05, 0.10, 0.20]\n",
        "    C = evaluator.bias_drift_sweep(beta_list, mode=\"constant\")\n",
        "    print(\"\\n[C] Bias drift sweep\")\n",
        "    for r in C:\n",
        "        print(f\"  beta={r['bias_beta']:.2f} ({r['bias_mode']}) | Acc={r['acc']:.4f} | \"\n",
        "              f\"Macro-F1={r['macro_f1']:.4f} | GateExp={r['gate_layer_expected_degree']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb9Nt65eZINw",
        "outputId": "3c2b6bd9-af40-4b04-c73e-bf859d6049af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: 42\n",
            "Device: cuda\n",
            "Epoch 001/30 | Train Loss 0.7401 Acc 0.7724 | Val Acc 0.8979 Macro-F1 0.8980 | 3.5s\n",
            "Epoch 002/30 | Train Loss 0.2036 Acc 0.9368 | Val Acc 0.9203 Macro-F1 0.9195 | 2.9s\n",
            "Epoch 003/30 | Train Loss 0.1520 Acc 0.9490 | Val Acc 0.9138 Macro-F1 0.9133 | 3.2s\n",
            "Epoch 004/30 | Train Loss 0.1347 Acc 0.9471 | Val Acc 0.9169 Macro-F1 0.9166 | 2.5s\n",
            "Epoch 005/30 | Train Loss 0.1296 Acc 0.9501 | Val Acc 0.9057 Macro-F1 0.9066 | 2.9s\n",
            "Epoch 006/30 | Train Loss 0.1263 Acc 0.9509 | Val Acc 0.9199 Macro-F1 0.9205 | 2.6s\n",
            "Epoch 007/30 | Train Loss 0.1147 Acc 0.9532 | Val Acc 0.9304 Macro-F1 0.9304 | 3.3s\n",
            "Epoch 008/30 | Train Loss 0.1211 Acc 0.9528 | Val Acc 0.9226 Macro-F1 0.9232 | 2.9s\n",
            "Epoch 009/30 | Train Loss 0.1118 Acc 0.9543 | Val Acc 0.9040 Macro-F1 0.9046 | 2.7s\n",
            "Epoch 010/30 | Train Loss 0.1122 Acc 0.9535 | Val Acc 0.9125 Macro-F1 0.9136 | 2.6s\n",
            "Epoch 011/30 | Train Loss 0.1033 Acc 0.9559 | Val Acc 0.9169 Macro-F1 0.9175 | 2.6s\n",
            "Epoch 012/30 | Train Loss 0.0990 Acc 0.9618 | Val Acc 0.9111 Macro-F1 0.9107 | 3.8s\n",
            "Epoch 013/30 | Train Loss 0.0894 Acc 0.9684 | Val Acc 0.9362 Macro-F1 0.9358 | 2.6s\n",
            "Epoch 014/30 | Train Loss 0.0760 Acc 0.9727 | Val Acc 0.9365 Macro-F1 0.9367 | 2.6s\n",
            "Epoch 015/30 | Train Loss 0.0656 Acc 0.9739 | Val Acc 0.9399 Macro-F1 0.9393 | 2.7s\n",
            "Epoch 016/30 | Train Loss 0.0613 Acc 0.9791 | Val Acc 0.9454 Macro-F1 0.9451 | 3.0s\n",
            "Epoch 017/30 | Train Loss 0.0473 Acc 0.9831 | Val Acc 0.9416 Macro-F1 0.9413 | 3.3s\n",
            "Epoch 018/30 | Train Loss 0.0363 Acc 0.9871 | Val Acc 0.9450 Macro-F1 0.9447 | 2.7s\n",
            "Epoch 019/30 | Train Loss 0.0304 Acc 0.9894 | Val Acc 0.9335 Macro-F1 0.9336 | 2.6s\n",
            "Epoch 020/30 | Train Loss 0.0262 Acc 0.9910 | Val Acc 0.9359 Macro-F1 0.9350 | 2.7s\n",
            "Epoch 021/30 | Train Loss 0.0213 Acc 0.9920 | Val Acc 0.9389 Macro-F1 0.9385 | 6.6s\n",
            "Epoch 022/30 | Train Loss 0.0251 Acc 0.9918 | Val Acc 0.9491 Macro-F1 0.9486 | 2.8s\n",
            "Epoch 023/30 | Train Loss 0.0217 Acc 0.9933 | Val Acc 0.9467 Macro-F1 0.9466 | 2.6s\n",
            "Epoch 024/30 | Train Loss 0.0152 Acc 0.9955 | Val Acc 0.9396 Macro-F1 0.9393 | 2.6s\n",
            "Epoch 025/30 | Train Loss 0.0194 Acc 0.9939 | Val Acc 0.9433 Macro-F1 0.9431 | 3.6s\n",
            "Epoch 026/30 | Train Loss 0.0133 Acc 0.9956 | Val Acc 0.9406 Macro-F1 0.9399 | 2.8s\n",
            "Epoch 027/30 | Train Loss 0.0077 Acc 0.9977 | Val Acc 0.9522 Macro-F1 0.9522 | 2.6s\n",
            "Epoch 028/30 | Train Loss 0.0036 Acc 0.9988 | Val Acc 0.9457 Macro-F1 0.9453 | 2.6s\n",
            "Epoch 029/30 | Train Loss 0.0040 Acc 0.9989 | Val Acc 0.9467 Macro-F1 0.9464 | 2.7s\n",
            "Epoch 030/30 | Train Loss 0.0014 Acc 0.9997 | Val Acc 0.9467 Macro-F1 0.9463 | 3.9s\n",
            "Done. Best Val Macro-F1: 0.9521833733944546\n",
            "\n",
            "==============================\n",
            "Robustness Experiments Start\n",
            "==============================\n",
            "\n",
            "==============================\n",
            "[SNR Suite Report]\n",
            "==============================\n",
            "\n",
            "--------------------------------\n",
            "SNR=inf dB\n",
            "  Acc=0.9522 | Macro-F1=0.9522\n",
            "  HardAvgDegree(per-layer)=[2.0, 2.055, 1.8164]\n",
            "  HardTotalCompute(sum layers)=5.8714\n",
            "  Degree histogram per layer (hard)\n",
            "    Layer 0: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000'] (deg=1..3)\n",
            "    Layer 1: counts=[0, 2785, 162] | ratio=['0.000', '0.945', '0.055'] (deg=1..3)\n",
            "    Layer 2: counts=[541, 2406, 0] | ratio=['0.184', '0.816', '0.000'] (deg=1..3)\n",
            "\n",
            "--------------------------------\n",
            "SNR=30 dB\n",
            "  Acc=0.9433 | Macro-F1=0.9436\n",
            "  HardAvgDegree(per-layer)=[2.0, 2.0461, 1.8164]\n",
            "  HardTotalCompute(sum layers)=5.8626\n",
            "  Degree histogram per layer (hard)\n",
            "    Layer 0: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000'] (deg=1..3)\n",
            "    Layer 1: counts=[0, 2811, 136] | ratio=['0.000', '0.954', '0.046'] (deg=1..3)\n",
            "    Layer 2: counts=[541, 2406, 0] | ratio=['0.184', '0.816', '0.000'] (deg=1..3)\n",
            "\n",
            "--------------------------------\n",
            "SNR=20 dB\n",
            "  Acc=0.9053 | Macro-F1=0.9073\n",
            "  HardAvgDegree(per-layer)=[2.0, 2.0214, 1.8164]\n",
            "  HardTotalCompute(sum layers)=5.8378\n",
            "  Degree histogram per layer (hard)\n",
            "    Layer 0: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000'] (deg=1..3)\n",
            "    Layer 1: counts=[0, 2884, 63] | ratio=['0.000', '0.979', '0.021'] (deg=1..3)\n",
            "    Layer 2: counts=[541, 2406, 0] | ratio=['0.184', '0.816', '0.000'] (deg=1..3)\n",
            "\n",
            "--------------------------------\n",
            "SNR=10 dB\n",
            "  Acc=0.8076 | Macro-F1=0.7951\n",
            "  HardAvgDegree(per-layer)=[2.0, 2.0, 1.8317]\n",
            "  HardTotalCompute(sum layers)=5.8317\n",
            "  Degree histogram per layer (hard)\n",
            "    Layer 0: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000'] (deg=1..3)\n",
            "    Layer 1: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000'] (deg=1..3)\n",
            "    Layer 2: counts=[496, 2451, 0] | ratio=['0.168', '0.832', '0.000'] (deg=1..3)\n",
            "\n",
            "--------------------------------\n",
            "SNR=5 dB\n",
            "  Acc=0.7268 | Macro-F1=0.6807\n",
            "  HardAvgDegree(per-layer)=[2.0, 2.0, 1.8894]\n",
            "  HardTotalCompute(sum layers)=5.8894\n",
            "  Degree histogram per layer (hard)\n",
            "    Layer 0: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000'] (deg=1..3)\n",
            "    Layer 1: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000'] (deg=1..3)\n",
            "    Layer 2: counts=[326, 2621, 0] | ratio=['0.111', '0.889', '0.000'] (deg=1..3)\n",
            "\n",
            "--------------------------------\n",
            "SNR=0 dB\n",
            "  Acc=0.6641 | Macro-F1=0.5974\n",
            "  HardAvgDegree(per-layer)=[2.0, 2.0, 1.9783]\n",
            "  HardTotalCompute(sum layers)=5.9783\n",
            "  Degree histogram per layer (hard)\n",
            "    Layer 0: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000'] (deg=1..3)\n",
            "    Layer 1: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000'] (deg=1..3)\n",
            "    Layer 2: counts=[64, 2883, 0] | ratio=['0.022', '0.978', '0.000'] (deg=1..3)\n",
            "\n",
            "==============================\n",
            "[Gate Stats] CLEAN (soft exp + classwise)\n",
            "==============================\n",
            "\n",
            "(1) Degree histogram per layer\n",
            "  Layer 0: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000']  (deg=1..3)\n",
            "  Layer 1: counts=[0, 2785, 162] | ratio=['0.000', '0.945', '0.055']  (deg=1..3)\n",
            "  Layer 2: counts=[541, 2406, 0] | ratio=['0.184', '0.816', '0.000']  (deg=1..3)\n",
            "\n",
            "(2) Class-wise expected degree (per layer)\n",
            "          WALK (n=496): ['2.000', '2.007', '2.003']\n",
            "            UP (n=471): ['1.999', '2.005', '1.998']\n",
            "          DOWN (n=420): ['2.000', '2.009', '2.002']\n",
            "           SIT (n=491): ['2.001', '2.002', '1.992']\n",
            "         STAND (n=532): ['2.000', '2.029', '2.004']\n",
            "           LAY (n=537): ['1.999', '1.979', '1.975']\n",
            "\n",
            "==============================\n",
            "[Gate Stats] GAUSSIAN NOISE (SNR=10) (soft exp + classwise)\n",
            "==============================\n",
            "\n",
            "(1) Degree histogram per layer\n",
            "  Layer 0: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000']  (deg=1..3)\n",
            "  Layer 1: counts=[0, 2947, 0] | ratio=['0.000', '1.000', '0.000']  (deg=1..3)\n",
            "  Layer 2: counts=[505, 2442, 0] | ratio=['0.171', '0.829', '0.000']  (deg=1..3)\n",
            "\n",
            "(2) Class-wise expected degree (per layer)\n",
            "          WALK (n=496): ['2.000', '2.007', '2.003']\n",
            "            UP (n=471): ['1.999', '2.005', '1.998']\n",
            "          DOWN (n=420): ['2.000', '2.009', '2.002']\n",
            "           SIT (n=491): ['2.001', '2.001', '1.998']\n",
            "         STAND (n=532): ['2.000', '2.010', '2.002']\n",
            "           LAY (n=537): ['1.999', '1.981', '1.976']\n",
            "\n",
            "[A] Instant noise sweep (SNR) [SOFT expected degree]\n",
            "  SNR=inf | Acc=0.9522 | Macro-F1=0.9522 | GateExp(per-layer)=[1.9997664182744128, 2.0052208951179016, 1.995353952367255]\n",
            "  SNR=30 | Acc=0.9430 | Macro-F1=0.9433 | GateExp(per-layer)=[1.999766671911199, 2.0048902719578843, 1.99550813056053]\n",
            "  SNR=20 | Acc=0.9060 | Macro-F1=0.9076 | GateExp(per-layer)=[1.9997668342387422, 2.0034421428721, 1.9958380054920277]\n",
            "  SNR=10 | Acc=0.8093 | Macro-F1=0.7976 | GateExp(per-layer)=[1.9997605288282354, 2.00171807471742, 1.9964048634184168]\n",
            "  SNR=5 | Acc=0.7258 | Macro-F1=0.6816 | GateExp(per-layer)=[1.9997618350576847, 2.001590812459905, 1.996859999413186]\n",
            "  SNR=0 | Acc=0.6535 | Macro-F1=0.5850 | GateExp(per-layer)=[1.9997501449382051, 2.00183367729187, 1.9973906861974837]\n",
            "\n",
            "[B] Linear SNR drift over time\n",
            "  t=00 | SNR=30.0 | Acc=0.9464 | Macro-F1=0.9466 | GateExp=[1.9997665349473344, 2.004898134698259, 1.9954990326090063]\n",
            "  t=01 | SNR=26.7 | Acc=0.9345 | Macro-F1=0.9352 | GateExp=[1.9997667505386028, 2.0045905138583895, 1.9955962312982438]\n",
            "  t=02 | SNR=23.3 | Acc=0.9240 | Macro-F1=0.9250 | GateExp=[1.9997658247643328, 2.0040957192157176, 1.995716358752961]\n",
            "  t=03 | SNR=20.0 | Acc=0.9033 | Macro-F1=0.9050 | GateExp=[1.9997652185724137, 2.0034410040429296, 1.9958549408202475]\n",
            "  t=04 | SNR=16.7 | Acc=0.8890 | Macro-F1=0.8907 | GateExp=[1.9997651044358598, 2.0027304202952285, 1.9959507622617356]\n",
            "  t=05 | SNR=13.3 | Acc=0.8677 | Macro-F1=0.8665 | GateExp=[1.9997674378942936, 2.002119990105325, 1.996152900634928]\n",
            "  t=06 | SNR=10.0 | Acc=0.8076 | Macro-F1=0.7950 | GateExp=[1.9997654316273141, 2.001699100149439, 1.9963858178321352]\n",
            "  t=07 | SNR=6.7 | Acc=0.7445 | Macro-F1=0.7085 | GateExp=[1.9997589004800675, 2.0015855139874397, 1.9966481518238148]\n",
            "  t=08 | SNR=3.3 | Acc=0.7017 | Macro-F1=0.6479 | GateExp=[1.999762661913608, 2.001613847752835, 1.9970073497041743]\n",
            "  t=09 | SNR=0.0 | Acc=0.6546 | Macro-F1=0.5864 | GateExp=[1.9997558213294822, 2.0018768133001124, 1.9974447235148003]\n",
            "\n",
            "[C] Bias drift sweep\n",
            "  beta=0.05 (constant) | Acc=0.9311 | Macro-F1=0.9317 | GateExp=[1.999766190001305, 2.004331832236432, 1.9948008161910036]\n",
            "  beta=0.10 (constant) | Acc=0.8605 | Macro-F1=0.8619 | GateExp=[1.9997042772617746, 2.0031962394714355, 1.9943446199944679]\n",
            "  beta=0.20 (constant) | Acc=0.7672 | Macro-F1=0.7661 | GateExp=[1.9994854977790346, 2.0022626328975597, 1.9941348877358944]\n"
          ]
        }
      ]
    }
  ]
}