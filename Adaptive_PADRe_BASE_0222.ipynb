{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UCI-HAR"
      ],
      "metadata": {
        "id": "AwHqtlJtnbpO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "377P8888sxdW",
        "outputId": "c229abff-aa26-49e9-e319-3f3872261721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "\n",
            "Model params: 78,591\n",
            "\n",
            "Epoch 05/100 | LR=0.0020 | Train Loss=0.1880 | TestF1=0.9301 | BestF1=0.9301 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0020 | Train Loss=0.1557 | TestF1=0.9303 | BestF1=0.9376 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0019 | Train Loss=0.1084 | TestF1=0.9470 | BestF1=0.9524 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0018 | Train Loss=0.0860 | TestF1=0.9525 | BestF1=0.9616 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0017 | Train Loss=0.0868 | TestF1=0.9568 | BestF1=0.9616 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0016 | Train Loss=0.0766 | TestF1=0.9611 | BestF1=0.9641 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0015 | Train Loss=0.0696 | TestF1=0.9512 | BestF1=0.9641 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0013 | Train Loss=0.0708 | TestF1=0.9561 | BestF1=0.9641 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0012 | Train Loss=0.0688 | TestF1=0.9605 | BestF1=0.9641 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0010 | Train Loss=0.0678 | TestF1=0.9630 | BestF1=0.9641 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0008 | Train Loss=0.0678 | TestF1=0.9638 | BestF1=0.9642 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0007 | Train Loss=0.0675 | TestF1=0.9594 | BestF1=0.9642 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0006 | Train Loss=0.0676 | TestF1=0.9608 | BestF1=0.9642 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0004 | Train Loss=0.0676 | TestF1=0.9614 | BestF1=0.9642 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0003 | Train Loss=0.0677 | TestF1=0.9607 | BestF1=0.9642 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0002 | Train Loss=0.0674 | TestF1=0.9590 | BestF1=0.9642 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0673 | TestF1=0.9597 | BestF1=0.9642 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0001 | Train Loss=0.0673 | TestF1=0.9594 | BestF1=0.9642 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0673 | TestF1=0.9594 | BestF1=0.9642 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0675 | TestF1=0.9590 | BestF1=0.9642 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9642\n"
          ]
        }
      ],
      "source": [
        "import os, copy, random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "CLASS_NAMES = [\"WALK\", \"UP\", \"DOWN\", \"SIT\", \"STAND\", \"LAY\"]\n",
        "\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_dir, split=\"train\", normalize=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split    = split\n",
        "        self.X, self.y = self._load_data()\n",
        "        self.X = torch.FloatTensor(self.X)\n",
        "        self.y = torch.LongTensor(self.y) - 1\n",
        "\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def _load_data(self):\n",
        "        split_dir    = self.data_dir / self.split\n",
        "        signal_types = [\n",
        "            \"body_acc_x\",\"body_acc_y\",\"body_acc_z\",\n",
        "            \"body_gyro_x\",\"body_gyro_y\",\"body_gyro_z\",\n",
        "            \"total_acc_x\",\"total_acc_y\",\"total_acc_z\",\n",
        "        ]\n",
        "        signals = []\n",
        "        for st in signal_types:\n",
        "            fname = split_dir / \"Inertial Signals\" / f\"{st}_{self.split}.txt\"\n",
        "            signals.append(np.loadtxt(fname))\n",
        "        X = np.stack(signals, axis=1)\n",
        "        y = np.loadtxt(split_dir / f\"y_{self.split}.txt\", dtype=int)\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):  return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[idx]\n",
        "        y = self.y[idx]\n",
        "        if self.normalize is not None:\n",
        "            mean, std = self.normalize\n",
        "            X = (X - mean.squeeze(0)) / std.squeeze(0)\n",
        "        return X, y\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "            Z.append(Z_ * Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        dw, logits, sp = self.degree_gate(x)\n",
        "        sel = dw.argmax(dim=-1)\n",
        "        out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": dw,\n",
        "                \"soft_probs\": sp,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (sel + 1).float().mean().item()\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, token_kernel=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res)\n",
        "\n",
        "            res2 = x\n",
        "            x = self.ffn[i](x)\n",
        "            x = self._ln(self.norms2[i], x + res2)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_train_stats(train_loader, device=\"cpu\", eps=1e-6):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      mean: (C,1) tensor\n",
        "      std : (C,1) tensor\n",
        "    Note:\n",
        "      X shape from loader: (B,C,T)\n",
        "      We compute stats over (B,T) for each channel.\n",
        "    \"\"\"\n",
        "    sum_x = None\n",
        "    sum_x2 = None\n",
        "    n = 0\n",
        "\n",
        "    for X, _ in train_loader:\n",
        "        X = X.to(device)  # (B,C,T)\n",
        "        B, C, T = X.shape\n",
        "        if sum_x is None:\n",
        "            sum_x = torch.zeros(C, device=device)\n",
        "            sum_x2 = torch.zeros(C, device=device)\n",
        "\n",
        "        # sum over batch and time\n",
        "        sum_x  += X.sum(dim=(0, 2))                 # (C,)\n",
        "        sum_x2 += (X * X).sum(dim=(0, 2))           # (C,)\n",
        "        n += B * T\n",
        "\n",
        "    mean = (sum_x / n)                              # (C,)\n",
        "    var  = (sum_x2 / n) - mean * mean               # (C,)\n",
        "    std  = torch.sqrt(torch.clamp(var, min=eps))    # (C,)\n",
        "\n",
        "    # reshape for broadcasting: (1,C,1) or (C,1)\n",
        "    mean = mean.view(1, -1, 1)\n",
        "    std  = std.view(1, -1, 1)\n",
        "    return mean.detach().cpu(), std.detach().cpu()\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/UCI_HAR\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 100\n",
        "    NUM_CLASSES = 6\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.3\n",
        "    LR = 2e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    train_dataset_raw = UCIHARDataset(DATA_PATH, split=\"train\", normalize=None)\n",
        "\n",
        "    stats_loader = DataLoader(\n",
        "        train_dataset_raw,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=False\n",
        "    )\n",
        "\n",
        "    mean, std = compute_train_stats(stats_loader, device=DEVICE if USE_GPU else \"cpu\")\n",
        "    train_dataset = UCIHARDataset(DATA_PATH, split=\"train\", normalize=(mean, std))\n",
        "    test_dataset  = UCIHARDataset(DATA_PATH, split=\"test\",  normalize=(mean, std))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    set_seed(SEED)\n",
        "    model = PADReHAR(in_channels=9,\n",
        "                     seq_len=128,\n",
        "                     num_classes=NUM_CLASSES,\n",
        "                     hidden_dim=HIDDEN_DIM,\n",
        "                     num_layers=NUM_LAYERS,\n",
        "                     max_degree=MAX_DEGREE,\n",
        "                     gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "                     dropout=DROPOUT,\n",
        "                     temperature_initial=5.0,\n",
        "                     temperature_min=0.5\n",
        "                     ).to(DEVICE)\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\\n\")\n",
        "\n",
        "    model = train_model(model, train_loader, test_loader, DEVICE,\n",
        "                        lr=LR, weight_decay=WD, epochs=EPOCHS, seed=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PAMAP2"
      ],
      "metadata": {
        "id": "QPDVZ-DWTIr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def create_pamap2_windows(df: pd.DataFrame, window_size: int, step_size: int):\n",
        "    feature_cols = [\n",
        "        # hand\n",
        "        \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "        \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "        \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "        # chest\n",
        "        \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "        \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "        \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "        # ankle\n",
        "        \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "        \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "        \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "    ]  # C = 27\n",
        "\n",
        "    ORDERED_IDS = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
        "    old2new = {\n",
        "        1: 0,   # Lying\n",
        "        2: 1,   # Sitting\n",
        "        3: 2,   # Standing\n",
        "        4: 3,   # Walking\n",
        "        5: 4,   # Running\n",
        "        6: 5,   # Cycling\n",
        "        7: 6,   # Nordic walking\n",
        "        12: 7,  # Ascending stairs\n",
        "        13: 8,  # Descending stairs\n",
        "        16: 9,  # Vacuum cleaning\n",
        "        17: 10, # Ironing\n",
        "        24: 11, # Rope jumping\n",
        "    }\n",
        "    label_names = [\n",
        "        \"Lying\", \"Sitting\", \"Standing\", \"Walking\",\n",
        "        \"Running\", \"Cycling\", \"Nordic walking\",\n",
        "        \"Ascending stairs\", \"Descending stairs\",\n",
        "        \"Vacuum cleaning\", \"Ironing\", \"Rope jumping\",\n",
        "    ]\n",
        "\n",
        "    X_list, y_list, subj_list = [], [], []\n",
        "\n",
        "    for subj_id, g in df.groupby(\"subject_id\"):\n",
        "        if \"timestamp\" in g.columns:\n",
        "            g = g.sort_values(\"timestamp\")\n",
        "        else:\n",
        "            g = g.sort_index()\n",
        "\n",
        "        data_arr  = g[feature_cols].to_numpy(dtype=np.float32)\n",
        "        label_arr = g[\"activityID\"].to_numpy(dtype=np.int64)\n",
        "        L = data_arr.shape[0]\n",
        "\n",
        "        start = 0\n",
        "        while start + window_size <= L:\n",
        "            end = start + window_size\n",
        "            last_label_orig = int(label_arr[end - 1])\n",
        "\n",
        "            if last_label_orig == 0:\n",
        "                start += step_size\n",
        "                continue\n",
        "            if last_label_orig not in old2new:\n",
        "                start += step_size\n",
        "                continue\n",
        "\n",
        "            window_ct = data_arr[start:end].T\n",
        "            X_list.append(window_ct)\n",
        "            y_list.append(old2new[last_label_orig])\n",
        "            subj_list.append(int(subj_id))\n",
        "            start += step_size\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        raise RuntimeError(\"No windows created. Check window_size/step_size and label filtering.\")\n",
        "\n",
        "    X = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y = np.asarray(y_list, dtype=np.int64)\n",
        "    subj_ids = np.asarray(subj_list, dtype=np.int64)\n",
        "    return X, y, subj_ids, label_names\n",
        "\n",
        "\n",
        "class PAMAP2Dataset(Dataset):\n",
        "    def __init__(self, data_dir, window_size, step_size):\n",
        "        super().__init__()\n",
        "\n",
        "        csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
        "        if len(csv_files) == 0:\n",
        "            raise RuntimeError(f\"No CSV files found under {data_dir}\")\n",
        "\n",
        "        dfs = []\n",
        "        for fpath in sorted(csv_files):\n",
        "            df_i = pd.read_csv(fpath)\n",
        "\n",
        "            if \"subject_id\" not in df_i.columns:\n",
        "                m = re.findall(r\"\\d+\", os.path.basename(fpath))\n",
        "                subj_guess = int(m[0]) if len(m) > 0 else 0\n",
        "                df_i[\"subject_id\"] = subj_guess\n",
        "\n",
        "            dfs.append(df_i)\n",
        "\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        df = df.dropna(subset=[\"activityID\"])\n",
        "        df[\"activityID\"] = df[\"activityID\"].astype(np.int64)\n",
        "        df[\"subject_id\"] = df[\"subject_id\"].astype(np.int64)\n",
        "        if \"timestamp\" in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "        feature_cols = [\n",
        "            # hand\n",
        "            \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "            \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "            \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "            # chest\n",
        "            \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "            \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "            \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "            # ankle\n",
        "            \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "            \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "            \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "        ]\n",
        "\n",
        "        def _fill_subject_group(g):\n",
        "            if \"timestamp\" in g.columns:\n",
        "                g = g.sort_values(\"timestamp\")\n",
        "            else:\n",
        "                g = g.sort_index()\n",
        "            g[feature_cols] = (\n",
        "                g[feature_cols]\n",
        "                .interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
        "                .ffill()\n",
        "                .bfill()\n",
        "            )\n",
        "            return g\n",
        "\n",
        "        df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n",
        "        df[feature_cols] = df[feature_cols].fillna(0.0)\n",
        "\n",
        "        X, y, subj_ids, label_names = create_pamap2_windows(df, window_size, step_size)\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y\n",
        "        self.subject_ids = subj_ids\n",
        "        self.label_names = label_names\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]\n",
        "        N, C, T = Xtr.shape\n",
        "\n",
        "        X2 = np.transpose(Xtr, (0, 2, 1)).reshape(-1, C)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = self.scaler\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = np.transpose(X, (0, 2, 1)).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        X_scaled = X2.reshape(N, T, C).transpose(0, 2, 1)\n",
        "\n",
        "        self.X = X_scaled.astype(np.float32)\n",
        "\n",
        "        print(\"Loaded PAMAP2 dataset\")\n",
        "        print(f\"X shape : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"Classes : {len(self.label_names)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long),\n",
        "            int(self.subject_ids[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "            Z.append(Z_ * Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        dw, logits, sp = self.degree_gate(x)\n",
        "        sel = dw.argmax(dim=-1)\n",
        "        out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": dw,\n",
        "                \"soft_probs\": sp,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (sel + 1).float().mean().item()\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, token_kernel=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False, return_details=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res)\n",
        "\n",
        "            res2 = x\n",
        "            x = self.ffn[i](x)\n",
        "            x = self._ln(self.norms2[i], x + res2)\n",
        "\n",
        "        h = self.global_pool(x).squeeze(-1)\n",
        "        logits = self.classifier(h)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return logits, gate_info_list, total_compute\n",
        "        if return_details:\n",
        "            return logits, {\"h\": h}\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model, best_f1\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    NUM_CLASSES = 12\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE = 50\n",
        "\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = PAMAP2Dataset(\n",
        "        data_dir=DATA_PATH,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        step_size=STEP_SIZE\n",
        "    )\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    model = PADReHAR(in_channels=27,\n",
        "                     seq_len=100,\n",
        "                     num_classes=NUM_CLASSES,\n",
        "                     hidden_dim=HIDDEN_DIM,\n",
        "                     num_layers=NUM_LAYERS,\n",
        "                     max_degree=MAX_DEGREE,\n",
        "                     gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "                     dropout=DROPOUT,\n",
        "                     temperature_initial=5.0,\n",
        "                     temperature_min=0.5\n",
        "                     ).to(DEVICE)\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\\n\")\n",
        "\n",
        "    model, best_f1 = train_model(model, train_loader, test_loader, DEVICE,\n",
        "                        lr=LR, weight_decay=WD, epochs=EPOCHS, seed=SEED)"
      ],
      "metadata": {
        "id": "xS3qcxBjTJ2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MHELATH"
      ],
      "metadata": {
        "id": "ctBUtzpIToTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, random, glob\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def _load_single_mhealth_log(path: str, feature_cols: list[str]):\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        names=feature_cols + [\"label\"],\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def load_mhealth_dataframe(data_dir: str):\n",
        "    feature_cols = [\n",
        "        \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",      # 0,1,2\n",
        "        \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",      # 5,6,7\n",
        "        \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",   # 8,9,10\n",
        "        \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",            # 14,15,16\n",
        "        \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",         # 17,18,19\n",
        "    ]  # total 15 channels\n",
        "\n",
        "    log_files = glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\"))\n",
        "    if not log_files:\n",
        "        raise FileNotFoundError(f\"No mHealth_subject*.log files found in {data_dir}\")\n",
        "    print(f\"Found {len(log_files)} log files in {data_dir}\")\n",
        "\n",
        "    dfs = []\n",
        "    for fp in sorted(log_files):\n",
        "        dfs.append(_load_single_mhealth_log(fp, feature_cols))\n",
        "\n",
        "    full_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    full_df = full_df[full_df[\"label\"] != 0].copy()\n",
        "\n",
        "    full_df.loc[:, \"label\"] = full_df[\"label\"] - 1\n",
        "\n",
        "    return full_df, feature_cols\n",
        "\n",
        "\n",
        "def create_mhealth_windows(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: list[str],\n",
        "    window_size: int,\n",
        "    step_size: int,\n",
        "):\n",
        "    data_arr = df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    labels_arr = df[\"label\"].to_numpy(dtype=np.int64)\n",
        "    L = data_arr.shape[0]\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "    start = 0\n",
        "    while start + window_size <= L:\n",
        "        end = start + window_size\n",
        "        window_x = data_arr[start:end]\n",
        "        window_label = labels_arr[end - 1]\n",
        "        X_list.append(window_x.T)\n",
        "        y_list.append(int(window_label))\n",
        "        start += step_size\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No windows created. Check window_size / step_size / dataset length.\")\n",
        "\n",
        "    X_np = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y_np = np.array(y_list, dtype=np.int64)\n",
        "    return X_np, y_np\n",
        "\n",
        "\n",
        "class MHEALTHDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, window_size: int = 128, step_size: int = 64):\n",
        "        super().__init__()\n",
        "\n",
        "        full_df, feature_cols = load_mhealth_dataframe(data_dir)\n",
        "        X, y = create_mhealth_windows(full_df, feature_cols, window_size, step_size)\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.subjects = np.zeros(len(self.y), dtype=int)\n",
        "\n",
        "        self.label_names = [\n",
        "            \"Standing still\", \"Sitting and relaxing\", \"Lying down\",\n",
        "            \"Walking\", \"Climbing stairs\", \"Waist bends forward\",\n",
        "            \"Frontal elevation of arms\", \"Knees bending\", \"Cycling\",\n",
        "            \"Jogging\", \"Running\", \"Jump front & back\",\n",
        "        ]\n",
        "\n",
        "        print(\"Loaded MHEALTH dataset\")\n",
        "        print(f\"X shape : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"Classes : {len(self.label_names)}\")\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]\n",
        "        N, C, T = Xtr.shape\n",
        "        X2 = Xtr.transpose(0, 2, 1).reshape(-1, C)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = self.scaler\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = X.transpose(0, 2, 1).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        self.X = X2.reshape(N, T, C).transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx]).long(),\n",
        "            int(self.subjects[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "            Z.append(Z_ * Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        dw, logits, sp = self.degree_gate(x)\n",
        "        sel = dw.argmax(dim=-1)\n",
        "        out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": dw,\n",
        "                \"soft_probs\": sp,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (sel + 1).float().mean().item()\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, token_kernel=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False, return_details=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res)\n",
        "\n",
        "            res2 = x\n",
        "            x = self.ffn[i](x)\n",
        "            x = self._ln(self.norms2[i], x + res2)\n",
        "\n",
        "        h = self.global_pool(x).squeeze(-1)\n",
        "        logits = self.classifier(h)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return logits, gate_info_list, total_compute\n",
        "        if return_details:\n",
        "            return logits, {\"h\": h}\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model, best_f1\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    NUM_CLASSES = 12\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE = 50\n",
        "\n",
        "    full_dataset = MHEALTHDataset(DATA_PATH, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.asarray(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    set_seed(SEED)\n",
        "    model = PADReHAR(in_channels=15,\n",
        "                     seq_len=WINDOW_SIZE,\n",
        "                     num_classes=NUM_CLASSES,\n",
        "                     hidden_dim=HIDDEN_DIM,\n",
        "                     num_layers=NUM_LAYERS,\n",
        "                     max_degree=MAX_DEGREE,\n",
        "                     gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "                     dropout=DROPOUT,\n",
        "                     temperature_initial=5.0,\n",
        "                     temperature_min=0.5\n",
        "                     ).to(DEVICE)\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\\n\")\n",
        "\n",
        "    model, best_f1 = train_model(model, train_loader, test_loader, DEVICE,\n",
        "                        lr=LR, weight_decay=WD, epochs=EPOCHS, seed=SEED)"
      ],
      "metadata": {
        "id": "R4IuTetaTpDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WISDM"
      ],
      "metadata": {
        "id": "20HADCs4T483"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class WISDMDataset(Dataset):\n",
        "    def __init__(self, file_path: str, window_size: int = 80, step_size: int = 40):\n",
        "        super().__init__()\n",
        "        self.file_path = file_path\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            raise FileNotFoundError(f\"WISDM txt file not found: {file_path}\")\n",
        "\n",
        "        df = self._load_file(file_path)\n",
        "        self.X, self.y, self.subjects = self._create_windows(df)\n",
        "        self.unique_subjects = sorted(np.unique(self.subjects))\n",
        "\n",
        "        self.n_classes = int(len(np.unique(self.y)))\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Loaded WISDM dataset (single txt)\")\n",
        "        print(f\"  X shape       : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"  y shape       : {self.y.shape}  (N,)\")\n",
        "        print(f\"  subjects shape: {self.subjects.shape} (N,)\")\n",
        "        print(f\"  num classes   : {self.n_classes}\")\n",
        "        print(f\"  unique subjects: {self.unique_subjects[:10]} ... (total {len(self.unique_subjects)})\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def _load_file(self, file_path: str) -> pd.DataFrame:\n",
        "        WISDM_LABEL_MAP = {\n",
        "            \"walking\": 0,\n",
        "            \"jogging\": 1,\n",
        "            \"sitting\": 2,\n",
        "            \"standing\": 3,\n",
        "            \"upstairs\": 4,\n",
        "            \"downstairs\": 5,\n",
        "        }\n",
        "\n",
        "        with open(file_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        rows = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            line = line.replace(\";\", \"\")\n",
        "            parts = line.split(\",\")\n",
        "\n",
        "            if len(parts) != 6:\n",
        "                continue\n",
        "\n",
        "            subj, act, ts, x, y, z = parts\n",
        "            if x.strip() == \"\" or y.strip() == \"\" or z.strip() == \"\":\n",
        "                continue\n",
        "\n",
        "            act_norm = act.strip().lower()\n",
        "            if act_norm not in WISDM_LABEL_MAP:\n",
        "                continue\n",
        "\n",
        "            rows.append([subj, act_norm, ts, x, y, z])\n",
        "\n",
        "        if not rows:\n",
        "            raise ValueError(f\"No valid rows parsed from file: {file_path}\")\n",
        "\n",
        "        df = pd.DataFrame(rows, columns=[\"subject\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"])\n",
        "        df = df.replace([\"\", \"NaN\", \"nan\"], np.nan).dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        df[\"subject\"] = pd.to_numeric(df[\"subject\"], errors=\"coerce\")\n",
        "        df[\"x\"] = pd.to_numeric(df[\"x\"], errors=\"coerce\")\n",
        "        df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\")\n",
        "        df[\"z\"] = pd.to_numeric(df[\"z\"], errors=\"coerce\")\n",
        "        df = df.dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "        if df.empty:\n",
        "            raise ValueError(\"After cleaning, WISDM DataFrame is empty. Check file format.\")\n",
        "\n",
        "        df[\"subject\"] = df[\"subject\"].astype(int)\n",
        "        df[\"activity_id\"] = df[\"activity\"].map(WISDM_LABEL_MAP).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_windows(self, df: pd.DataFrame):\n",
        "        X_list, y_list, s_list = [], [], []\n",
        "\n",
        "        for subj_id in sorted(df[\"subject\"].unique()):\n",
        "            df_sub = df[df[\"subject\"] == subj_id]\n",
        "            data = df_sub[[\"x\", \"y\", \"z\"]].to_numpy(dtype=np.float32)\n",
        "            labels = df_sub[\"activity_id\"].to_numpy(dtype=np.int64)\n",
        "            L = len(df_sub)\n",
        "\n",
        "            start = 0\n",
        "            while start + self.window_size <= L:\n",
        "                end = start + self.window_size\n",
        "                window_x = data[start:end]\n",
        "                window_y = labels[end - 1]\n",
        "\n",
        "                X_list.append(window_x.T)\n",
        "                y_list.append(window_y)\n",
        "                s_list.append(subj_id)\n",
        "\n",
        "                start += self.step_size\n",
        "\n",
        "        if len(X_list) == 0:\n",
        "            raise ValueError(\"[WISDMDataset] No windows created. Try smaller window_size or check data.\")\n",
        "\n",
        "        X = np.stack(X_list, axis=0).astype(np.float32)\n",
        "        y = np.array(y_list, dtype=np.int64)\n",
        "        s = np.array(s_list, dtype=np.int64)\n",
        "        return X, y, s\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]  # (N,C,T)\n",
        "        N, C, T = Xtr.shape\n",
        "        X2 = np.transpose(Xtr, (0, 2, 1)).reshape(-1, C)  # (N*T, C)\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = getattr(self, \"scaler\", None)\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = np.transpose(X, (0, 2, 1)).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        self.X = X2.reshape(N, T, C).transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.FloatTensor(self.X[idx]),\n",
        "            torch.LongTensor([self.y[idx]])[0],\n",
        "            int(self.subjects[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "            Z.append(Z_ * Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        dw, logits, sp = self.degree_gate(x)\n",
        "        sel = dw.argmax(dim=-1)\n",
        "        out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": dw,\n",
        "                \"soft_probs\": sp,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (sel + 1).float().mean().item()\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, token_kernel=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False, return_details=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res)\n",
        "\n",
        "            res2 = x\n",
        "            x = self.ffn[i](x)\n",
        "            x = self._ln(self.norms2[i], x + res2)\n",
        "\n",
        "        h = self.global_pool(x).squeeze(-1)\n",
        "        logits = self.classifier(h)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return logits, gate_info_list, total_compute\n",
        "        if return_details:\n",
        "            return logits, {\"h\": h}\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model, best_f1\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/WISDM_ar_v1.1_raw.txt\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    NUM_CLASSES = 6\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE=80\n",
        "    STEP_SIZE=40\n",
        "\n",
        "    full_dataset = WISDMDataset(DATA_PATH, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    set_seed(SEED)\n",
        "    model = PADReHAR(in_channels=3,\n",
        "                     seq_len=WINDOW_SIZE,\n",
        "                     num_classes=NUM_CLASSES,\n",
        "                     hidden_dim=HIDDEN_DIM,\n",
        "                     num_layers=NUM_LAYERS,\n",
        "                     max_degree=MAX_DEGREE,\n",
        "                     gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "                     dropout=DROPOUT,\n",
        "                     temperature_initial=5.0,\n",
        "                     temperature_min=0.5\n",
        "                     ).to(DEVICE)\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\\n\")\n",
        "\n",
        "    model, best_f1 = train_model(model, train_loader, test_loader, DEVICE,\n",
        "                        lr=LR, weight_decay=WD, epochs=EPOCHS, seed=SEED)"
      ],
      "metadata": {
        "id": "_swMlLZHT5wt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
