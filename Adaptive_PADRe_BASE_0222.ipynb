{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UCI-HAR"
      ],
      "metadata": {
        "id": "AwHqtlJtnbpO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "377P8888sxdW",
        "outputId": "45a1ed74-d583-4f01-bc47-bc2787791533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "\n",
            "Model params: 78,591\n",
            "\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1297 | TestF1=0.8998 | BestF1=0.9133 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.1057 | TestF1=0.9009 | BestF1=0.9224 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0601 | TestF1=0.9328 | BestF1=0.9425 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0293 | TestF1=0.9463 | BestF1=0.9463 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0208 | TestF1=0.9401 | BestF1=0.9463 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0131 | TestF1=0.9336 | BestF1=0.9463 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0073 | TestF1=0.9470 | BestF1=0.9474 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0011 | TestF1=0.9518 | BestF1=0.9617 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0019 | TestF1=0.9493 | BestF1=0.9617 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0010 | TestF1=0.9486 | BestF1=0.9617 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9491 | BestF1=0.9617 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0006 | TestF1=0.9516 | BestF1=0.9617 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0002 | TestF1=0.9502 | BestF1=0.9617 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9479 | BestF1=0.9617 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9456 | BestF1=0.9617 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0000 | TestF1=0.9547 | BestF1=0.9617 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9475 | BestF1=0.9617 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9506 | BestF1=0.9617 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9520 | BestF1=0.9617 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9513 | BestF1=0.9617 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9617\n"
          ]
        }
      ],
      "source": [
        "import os, copy, random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "CLASS_NAMES = [\"WALK\", \"UP\", \"DOWN\", \"SIT\", \"STAND\", \"LAY\"]\n",
        "\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_dir, split=\"train\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split    = split\n",
        "        self.X, self.y = self._load_data()\n",
        "        self.X = torch.FloatTensor(self.X)\n",
        "        self.y = torch.LongTensor(self.y) - 1\n",
        "\n",
        "    def _load_data(self):\n",
        "        split_dir    = self.data_dir / self.split\n",
        "        signal_types = [\n",
        "            \"body_acc_x\",\"body_acc_y\",\"body_acc_z\",\n",
        "            \"body_gyro_x\",\"body_gyro_y\",\"body_gyro_z\",\n",
        "            \"total_acc_x\",\"total_acc_y\",\"total_acc_z\",\n",
        "        ]\n",
        "        signals = []\n",
        "        for st in signal_types:\n",
        "            fname = split_dir / \"Inertial Signals\" / f\"{st}_{self.split}.txt\"\n",
        "            signals.append(np.loadtxt(fname))\n",
        "        X = np.stack(signals, axis=1)\n",
        "        y = np.loadtxt(split_dir / f\"y_{self.split}.txt\", dtype=int)\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):  return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "            Z.append(Z_ * Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        dw, logits, sp = self.degree_gate(x)\n",
        "        sel = dw.argmax(dim=-1)\n",
        "        out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": dw,\n",
        "                \"soft_probs\": sp,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (sel + 1).float().mean().item()\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, token_kernel=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res)\n",
        "\n",
        "            res2 = x\n",
        "            x = self.ffn[i](x)\n",
        "            x = self._ln(self.norms2[i], x + res2)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/UCI_HAR\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    NUM_CLASSES = 6\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 5e-4\n",
        "\n",
        "    train_dataset = UCIHARDataset(DATA_PATH, split=\"train\")\n",
        "    test_dataset = UCIHARDataset(DATA_PATH, split=\"test\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    set_seed(SEED)\n",
        "    model = PADReHAR(in_channels=9,\n",
        "                     seq_len=128,\n",
        "                     num_classes=NUM_CLASSES,\n",
        "                     hidden_dim=HIDDEN_DIM,\n",
        "                     num_layers=NUM_LAYERS,\n",
        "                     max_degree=MAX_DEGREE,\n",
        "                     gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "                     dropout=DROPOUT,\n",
        "                     temperature_initial=5.0,\n",
        "                     temperature_min=0.5\n",
        "                     ).to(DEVICE)\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\\n\")\n",
        "\n",
        "    model = train_model(model, train_loader, test_loader, DEVICE,\n",
        "                        lr=LR, weight_decay=WD, epochs=EPOCHS, seed=SEED)"
      ]
    }
  ]
}