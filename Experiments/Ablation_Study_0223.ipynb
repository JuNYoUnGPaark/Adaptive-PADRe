{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Variants:\n",
        "  1) Full PADRe (Ours)\n",
        "  2) w/o Gate (Fixed Deg 1/2/3)\n",
        "  3) Gate w/o STE (Soft weighted sum)\n",
        "  4) w/o Hadamard\n",
        "  5) w/o FFN\n",
        "  6) w/o Residual\n",
        "\n",
        "Metrics:\n",
        "  - Clean Val Acc / Macro-F1\n",
        "  - Noisy Val Acc / Macro-F1 (SNR=10 dB)\n",
        "  - Robustness Drop (Clean F1 - Noisy F1)\n",
        "  - Normalized Compute\n",
        "  - Degree Entropy (gate diversity)"
      ],
      "metadata": {
        "id": "7uDu41wCVOhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UCI-HAR"
      ],
      "metadata": {
        "id": "qwqQYpw5VSDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "CLASS_NAMES = [\"WALK\", \"UP\", \"DOWN\", \"SIT\", \"STAND\", \"LAY\"]\n",
        "\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_dir, split=\"train\", normalize=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split    = split\n",
        "        self.X, self.y = self._load_data()\n",
        "        self.X = torch.FloatTensor(self.X)\n",
        "        self.y = torch.LongTensor(self.y) - 1\n",
        "\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def _load_data(self):\n",
        "        split_dir    = self.data_dir / self.split\n",
        "        signal_types = [\n",
        "            \"body_acc_x\",\"body_acc_y\",\"body_acc_z\",\n",
        "            \"body_gyro_x\",\"body_gyro_y\",\"body_gyro_z\",\n",
        "            \"total_acc_x\",\"total_acc_y\",\"total_acc_z\",\n",
        "        ]\n",
        "        signals = []\n",
        "        for st in signal_types:\n",
        "            fname = split_dir / \"Inertial Signals\" / f\"{st}_{self.split}.txt\"\n",
        "            signals.append(np.loadtxt(fname))\n",
        "        X = np.stack(signals, axis=1)\n",
        "        y = np.loadtxt(split_dir / f\"y_{self.split}.txt\", dtype=int)\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):  return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[idx]\n",
        "        y = self.y[idx]\n",
        "        if self.normalize is not None:\n",
        "            mean, std = self.normalize\n",
        "            X = (X - mean.squeeze(0)) / std.squeeze(0)\n",
        "        return X, y\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions (SNR=10 uses this)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate  (★ Variant behavior matches \"예전버전\")\n",
        "#   - use_ste=True  : train=STE(hard fwd, soft bwd), eval=hard onehot\n",
        "#   - use_ste=False : always soft_probs (train/eval 동일)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits = self.gate(x)  # (B,K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx = logits.argmax(dim=-1)\n",
        "                hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                # STE: forward=hard, backward=soft\n",
        "                degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(\n",
        "                    logits.argmax(dim=-1), num_classes=self.max_degree\n",
        "                ).float()\n",
        "        else:\n",
        "            # Gate w/o STE: always soft (train/eval 동일)\n",
        "            degree_w = soft_probs\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block (Ablation switches ONLY; ★ Variant behavior matches \"예전버전\")\n",
        "#   - w/o Gate: fixed_degree (1..K) → build up to d, output Z[d-1]\n",
        "#   - Gate w/o STE: soft weighted sum of ALL degrees (train/eval 동일)\n",
        "#   - w/o Hadamard: prefix sum (Z[i]=Z[i-1]+Y[i])  (예전버전)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlockAblation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,      # 1..K if w/o Gate (fixed)\n",
        "                 use_ste=True,           # Gate w/o STE (soft routing)\n",
        "                 use_hadamard=True,      # w/o Hadamard (prefix-sum)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_gate = bool(use_gate)\n",
        "        self.fixed_degree = fixed_degree  # None or int in [1..K]\n",
        "        self.use_ste = bool(use_ste)\n",
        "        self.use_hadamard = bool(use_hadamard)\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _build_Y(self, x, max_deg):\n",
        "        return [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        \"\"\"\n",
        "        - use_hadamard=True : 예전버전(원본) Hadamard chain\n",
        "            Z0=Y0, Zi = pre(Z_{i-1}) * Yi\n",
        "        - use_hadamard=False: 예전버전 w/o Hadamard (prefix sum)\n",
        "            Z0=Y0, Zi = Z_{i-1} + Yi\n",
        "        \"\"\"\n",
        "        Y = self._build_Y(x, max_deg)\n",
        "\n",
        "        if self.use_hadamard:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(Z_ * Y[i])\n",
        "            return Z\n",
        "        else:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "            return Z\n",
        "\n",
        "    def _hard_select(self, Z_list, sel):\n",
        "        B = Z_list[0].shape[0]\n",
        "        Z_stack = torch.stack(Z_list, dim=0)  # (K,B,C,T)\n",
        "        return Z_stack[sel, torch.arange(B, device=Z_stack.device)]\n",
        "\n",
        "    def _soft_weighted_output(self, x, soft_probs):\n",
        "        \"\"\"\n",
        "        예전버전 Gate w/o STE:\n",
        "          - always compute ALL K degrees\n",
        "          - weighted sum with soft_probs\n",
        "          - hadamard / no_hadamard build rule은 동일하게 적용\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        Z = self._build_Z(x, max_deg=self.max_degree)      # list length K, each (B,C,T)\n",
        "        Z_stack = torch.stack(Z, dim=1)                    # (B,K,C,T)\n",
        "        w = soft_probs.view(B, self.max_degree, 1, 1)      # (B,K,1,1)\n",
        "        out = (Z_stack * w).sum(dim=1)                     # (B,C,T)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # ---- Case A) w/o Gate (fixed degree 1..K) ----\n",
        "        if (not self.use_gate) or (self.fixed_degree is not None):\n",
        "            d = int(self.fixed_degree) if self.fixed_degree is not None else self.max_degree\n",
        "            d = max(1, min(d, self.max_degree))\n",
        "\n",
        "            # build only up to d, output \"degree d\" path (예전버전)\n",
        "            Z = self._build_Z(x, max_deg=d)\n",
        "            out = Z[-1]\n",
        "\n",
        "            # stats payload (예전버전 스타일)\n",
        "            sel = torch.full((B,), d - 1, device=x.device, dtype=torch.long)\n",
        "            K = self.max_degree\n",
        "            sp = F.one_hot(sel, num_classes=K).float()\n",
        "            dw = sp\n",
        "            logits = sp\n",
        "\n",
        "            out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            if return_gate_info:\n",
        "                return out, {\n",
        "                    \"degree_selection\": dw,\n",
        "                    \"soft_probs\": sp,\n",
        "                    \"logits\": logits,\n",
        "                    \"compute_cost\": float(d),\n",
        "                }\n",
        "            return out\n",
        "\n",
        "        # ---- Case B) Gate ON ----\n",
        "        degree_w, logits, soft_probs = self.degree_gate(x, use_ste=self.use_ste)\n",
        "\n",
        "        if (not self.use_ste):\n",
        "            # 예전버전: Gate w/o STE는 train/eval 관계없이 ALWAYS soft weighted sum\n",
        "            out = self._soft_weighted_output(x, degree_w)  # degree_w == soft_probs\n",
        "            # (stats only) 대표 degree\n",
        "            selected = soft_probs.argmax(dim=-1)\n",
        "            # compute_cost (예전코드에선 argmax 기반이었지만, 여기서는 gi에만 들어가므로 그대로 둠)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "        else:\n",
        "            # 원본: hard select (STE는 train에서 degree_w에 반영됨)\n",
        "            selected = degree_w.argmax(dim=-1)\n",
        "            max_deg = max(1, min(int(selected.max().item()) + 1, self.max_degree))\n",
        "            Z = self._build_Z(x, max_deg=max_deg)\n",
        "            out = self._hard_select(Z, selected)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_w,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": compute_cost,\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model (Ablation switches ONLY; otherwise matches your logic)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR_Ablation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,     # for w/o Gate (1/2/3)\n",
        "                 use_ste=True,          # Gate w/o STE\n",
        "                 use_hadamard=True,     # w/o Hadamard\n",
        "                 use_ffn=True,          # w/o FFN\n",
        "                 use_residual=True,     # w/o Residual\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_ffn = bool(use_ffn)\n",
        "        self.use_residual = bool(use_residual)\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlockAblation(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                token_kernel=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "                use_gate=use_gate,\n",
        "                fixed_degree=fixed_degree,\n",
        "                use_ste=use_ste,\n",
        "                use_hadamard=use_hadamard,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms1[i], x + res)\n",
        "            else:\n",
        "                x = self._ln(self.norms1[i], x)\n",
        "\n",
        "            res2 = x\n",
        "            if self.use_ffn:\n",
        "                x = self.ffn[i](x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms2[i], x + res2)\n",
        "            else:\n",
        "                x = self._ln(self.norms2[i], x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_train_stats(train_loader, device=\"cpu\", eps=1e-6):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      mean: (C,1) tensor\n",
        "      std : (C,1) tensor\n",
        "    Note:\n",
        "      X shape from loader: (B,C,T)\n",
        "      We compute stats over (B,T) for each channel.\n",
        "    \"\"\"\n",
        "    sum_x = None\n",
        "    sum_x2 = None\n",
        "    n = 0\n",
        "\n",
        "    for X, _ in train_loader:\n",
        "        X = X.to(device)  # (B,C,T)\n",
        "        B, C, T = X.shape\n",
        "        if sum_x is None:\n",
        "            sum_x = torch.zeros(C, device=device)\n",
        "            sum_x2 = torch.zeros(C, device=device)\n",
        "\n",
        "        # sum over batch and time\n",
        "        sum_x  += X.sum(dim=(0, 2))                 # (C,)\n",
        "        sum_x2 += (X * X).sum(dim=(0, 2))           # (C,)\n",
        "        n += B * T\n",
        "\n",
        "    mean = (sum_x / n)                              # (C,)\n",
        "    var  = (sum_x2 / n) - mean * mean               # (C,)\n",
        "    std  = torch.sqrt(torch.clamp(var, min=eps))    # (C,)\n",
        "\n",
        "    # reshape for broadcasting: (1,C,1) or (C,1)\n",
        "    mean = mean.view(1, -1, 1)\n",
        "    std  = std.view(1, -1, 1)\n",
        "    return mean.detach().cpu(), std.detach().cpu()\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Table Metrics (keep as-is in your \"now code\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_gate_stats(model, loader, device, snr_db=None, max_degree=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      macro_f1, degree_entropy, norm_comp\n",
        "    Definitions:\n",
        "      - degree_entropy: mean over (layers, samples) of normalized entropy of soft_probs\n",
        "                        H(p)/log(K)  where K=max_degree\n",
        "      - norm_comp: mean expected degree / max_degree, averaged over layers\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    ent_sum = 0.0\n",
        "    ent_count = 0\n",
        "    comp_sum = 0.0\n",
        "    comp_count = 0\n",
        "\n",
        "    eps = 1e-12\n",
        "    logK = float(np.log(max_degree))\n",
        "\n",
        "    deg_vals = torch.arange(1, max_degree + 1, device=device).float()\n",
        "\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "        # gate stats\n",
        "        for gi in gate_info_list:\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            ent = -(sp * (sp + eps).log()).sum(dim=-1) / logK\n",
        "            ent_sum += ent.mean().item()\n",
        "            ent_count += 1\n",
        "\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "            comp_sum += (exp_deg / max_degree)\n",
        "            comp_count += 1\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    macro_f1 = float(f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    degree_entropy = float(ent_sum / max(ent_count, 1))\n",
        "    norm_comp = float(comp_sum / max(comp_count, 1))\n",
        "    return macro_f1, degree_entropy, norm_comp\n",
        "\n",
        "\n",
        "def format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp):\n",
        "    drop_pct = 100.0 * (clean_f1 - snr10_f1) / max(clean_f1, 1e-12)\n",
        "    return {\n",
        "        \"Variant\": name,\n",
        "        \"CleanF1\": clean_f1,\n",
        "        \"SNR10F1\": snr10_f1,\n",
        "        \"drop(%)\": drop_pct,\n",
        "        \"DegreeEntropy\": deg_ent,\n",
        "        \"NormComp\": norm_comp,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    header = [\"Variant\", \"Clean F1\", \"(SNR=10) F1\", \"drop(%)\", \"Degree Entropy\", \"NormComp\"]\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"UCI-HAR Ablation Table\")\n",
        "    print(\"=\"*110)\n",
        "    print(f\"{header[0]:<22s} | {header[1]:>8s} | {header[2]:>11s} | {header[3]:>7s} | {header[4]:>14s} | {header[5]:>8s}\")\n",
        "    print(\"-\"*110)\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']:<22s} | \"\n",
        "            f\"{r['CleanF1']:>8.4f} | \"\n",
        "            f\"{r['SNR10F1']:>11.4f} | \"\n",
        "            f\"{r['drop(%)']:>7.2f} | \"\n",
        "            f\"{r['DegreeEntropy']:>14.4f} | \"\n",
        "            f\"{r['NormComp']:>8.4f}\"\n",
        "        )\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    print(\"\\n[LaTeX rows]\")\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']} & {r['CleanF1']:.4f} & {r['SNR10F1']:.4f} & \"\n",
        "            f\"{r['drop(%)']:.2f} & {r['DegreeEntropy']:.4f} & {r['NormComp']:.4f} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Experiment Runner (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def build_variant(name, cfg):\n",
        "    model = PADReHAR_Ablation(\n",
        "        in_channels=cfg[\"in_channels\"],\n",
        "        seq_len=cfg[\"seq_len\"],\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_layers=cfg[\"num_layers\"],\n",
        "        max_degree=cfg[\"max_degree\"],\n",
        "        gate_hidden_dim=cfg[\"gate_hidden_dim\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        temperature_initial=cfg[\"temperature_initial\"],\n",
        "        temperature_min=cfg[\"temperature_min\"],\n",
        "        use_gate=cfg.get(\"use_gate\", True),\n",
        "        fixed_degree=cfg.get(\"fixed_degree\", None),\n",
        "        use_ste=cfg.get(\"use_ste\", True),\n",
        "        use_hadamard=cfg.get(\"use_hadamard\", True),\n",
        "        use_ffn=cfg.get(\"use_ffn\", True),\n",
        "        use_residual=cfg.get(\"use_residual\", True),\n",
        "    ).to(cfg[\"device\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg):\n",
        "    variants = []\n",
        "    variants.append((\"Full (Ours)\", dict()))\n",
        "    variants.append((\"w/o Gate-1\", dict(use_gate=False, fixed_degree=1)))\n",
        "    variants.append((\"w/o Gate-2\", dict(use_gate=False, fixed_degree=2)))\n",
        "    variants.append((\"w/o Gate-3\", dict(use_gate=False, fixed_degree=3)))\n",
        "    variants.append((\"Gate w/o STE\", dict(use_gate=True, use_ste=False)))\n",
        "    variants.append((\"w/o Hadamard\", dict(use_hadamard=False)))\n",
        "    variants.append((\"w/o FFN\", dict(use_ffn=False)))\n",
        "    variants.append((\"w/o Residual\", dict(use_residual=False)))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for name, delta in variants:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[Running Variant] {name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        cfg = copy.deepcopy(base_cfg)\n",
        "        cfg.update(delta)\n",
        "\n",
        "        set_seed(train_cfg[\"seed\"])\n",
        "        model = build_variant(name, cfg)\n",
        "        print(f\"Model params: {count_parameters(model):,}\")\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            cfg[\"device\"],\n",
        "            lr=train_cfg[\"lr\"],\n",
        "            weight_decay=train_cfg[\"wd\"],\n",
        "            epochs=train_cfg[\"epochs\"],\n",
        "            seed=train_cfg[\"seed\"],\n",
        "        )\n",
        "\n",
        "        clean_f1, deg_ent, norm_comp = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=None, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "        snr10_f1, _, _ = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=10.0, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "\n",
        "        rows.append(format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/UCI_HAR\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "\n",
        "    NUM_CLASSES = 6\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    train_dataset_raw = UCIHARDataset(DATA_PATH, split=\"train\", normalize=None)\n",
        "\n",
        "    stats_loader = DataLoader(\n",
        "        train_dataset_raw,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    mean, std = compute_train_stats(stats_loader, device=\"cpu\")\n",
        "    train_dataset = UCIHARDataset(DATA_PATH, split=\"train\", normalize=(mean, std))\n",
        "    test_dataset  = UCIHARDataset(DATA_PATH, split=\"test\",  normalize=(mean, std))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    base_cfg = dict(\n",
        "        device=DEVICE,\n",
        "        in_channels=9,\n",
        "        seq_len=128,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "        dropout=DROPOUT,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        use_gate=True,\n",
        "        fixed_degree=None,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    )\n",
        "\n",
        "    train_cfg = dict(\n",
        "        seed=SEED,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        wd=WD,\n",
        "    )\n",
        "\n",
        "    rows = run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg)\n",
        "    print_table(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOKMkJivm9m1",
        "outputId": "1c9f32ed-307c-45ef-b213-ec4f3690aa43"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Full (Ours)\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1073 | TestF1=0.9187 | BestF1=0.9325 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.0853 | TestF1=0.9088 | BestF1=0.9361 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0701 | TestF1=0.9241 | BestF1=0.9361 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0278 | TestF1=0.9338 | BestF1=0.9382 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0141 | TestF1=0.9471 | BestF1=0.9482 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0060 | TestF1=0.9500 | BestF1=0.9547 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0023 | TestF1=0.9436 | BestF1=0.9547 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0016 | TestF1=0.9469 | BestF1=0.9547 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0027 | TestF1=0.9524 | BestF1=0.9547 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0002 | TestF1=0.9486 | BestF1=0.9547 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9455 | BestF1=0.9547 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9454 | BestF1=0.9547 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0012 | TestF1=0.9440 | BestF1=0.9547 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9424 | BestF1=0.9547 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9437 | BestF1=0.9547 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9437 | BestF1=0.9547 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9435 | BestF1=0.9547 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9438 | BestF1=0.9547 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9438 | BestF1=0.9547 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9438 | BestF1=0.9547 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9547\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-1\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1164 | TestF1=0.9059 | BestF1=0.9258 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.0968 | TestF1=0.9334 | BestF1=0.9334 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0790 | TestF1=0.9469 | BestF1=0.9469 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0394 | TestF1=0.9501 | BestF1=0.9580 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0255 | TestF1=0.9513 | BestF1=0.9580 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0139 | TestF1=0.9531 | BestF1=0.9580 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0135 | TestF1=0.9579 | BestF1=0.9580 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0063 | TestF1=0.9442 | BestF1=0.9580 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0043 | TestF1=0.9611 | BestF1=0.9631 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0034 | TestF1=0.9597 | BestF1=0.9631 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0032 | TestF1=0.9468 | BestF1=0.9631 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0007 | TestF1=0.9483 | BestF1=0.9631 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0002 | TestF1=0.9483 | BestF1=0.9631 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9462 | BestF1=0.9631 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9459 | BestF1=0.9631 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9473 | BestF1=0.9631 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9473 | BestF1=0.9631 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9470 | BestF1=0.9631 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9463 | BestF1=0.9631 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9470 | BestF1=0.9631 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9631\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-2\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1044 | TestF1=0.9035 | BestF1=0.9305 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.0754 | TestF1=0.9185 | BestF1=0.9305 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0665 | TestF1=0.9231 | BestF1=0.9305 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0142 | TestF1=0.9393 | BestF1=0.9393 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0065 | TestF1=0.9520 | BestF1=0.9520 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0031 | TestF1=0.9461 | BestF1=0.9520 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0114 | TestF1=0.9463 | BestF1=0.9520 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0032 | TestF1=0.9395 | BestF1=0.9520 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0002 | TestF1=0.9376 | BestF1=0.9520 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0001 | TestF1=0.9404 | BestF1=0.9520 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9362 | BestF1=0.9520 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9369 | BestF1=0.9520 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0001 | TestF1=0.9377 | BestF1=0.9520 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9381 | BestF1=0.9520 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0000 | TestF1=0.9377 | BestF1=0.9520 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0000 | TestF1=0.9398 | BestF1=0.9520 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9398 | BestF1=0.9520 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9392 | BestF1=0.9520 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9392 | BestF1=0.9520 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9395 | BestF1=0.9520 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9520\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-3\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1099 | TestF1=0.9291 | BestF1=0.9291 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.0747 | TestF1=0.9388 | BestF1=0.9388 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0296 | TestF1=0.9603 | BestF1=0.9603 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0138 | TestF1=0.9668 | BestF1=0.9688 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0134 | TestF1=0.9549 | BestF1=0.9688 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0082 | TestF1=0.9548 | BestF1=0.9688 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0087 | TestF1=0.9605 | BestF1=0.9688 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0004 | TestF1=0.9506 | BestF1=0.9688 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0010 | TestF1=0.9661 | BestF1=0.9688 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0001 | TestF1=0.9620 | BestF1=0.9688 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9635 | BestF1=0.9688 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9645 | BestF1=0.9688 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0013 | TestF1=0.9594 | BestF1=0.9688 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9636 | BestF1=0.9688 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9625 | BestF1=0.9688 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0000 | TestF1=0.9617 | BestF1=0.9688 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9614 | BestF1=0.9688 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9621 | BestF1=0.9688 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9621 | BestF1=0.9688 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9617 | BestF1=0.9688 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9688\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Gate w/o STE\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1084 | TestF1=0.9130 | BestF1=0.9340 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.0849 | TestF1=0.9243 | BestF1=0.9400 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0702 | TestF1=0.9228 | BestF1=0.9400 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0456 | TestF1=0.9387 | BestF1=0.9400 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0205 | TestF1=0.9442 | BestF1=0.9491 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0061 | TestF1=0.9483 | BestF1=0.9491 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0081 | TestF1=0.9444 | BestF1=0.9523 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0003 | TestF1=0.9438 | BestF1=0.9523 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0039 | TestF1=0.9364 | BestF1=0.9523 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0003 | TestF1=0.9461 | BestF1=0.9523 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0002 | TestF1=0.9389 | BestF1=0.9523 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9403 | BestF1=0.9523 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0001 | TestF1=0.9393 | BestF1=0.9523 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9411 | BestF1=0.9523 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9353 | BestF1=0.9523 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9383 | BestF1=0.9523 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9347 | BestF1=0.9523 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9375 | BestF1=0.9523 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9363 | BestF1=0.9523 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9367 | BestF1=0.9523 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9523\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Hadamard\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1189 | TestF1=0.9114 | BestF1=0.9239 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.1094 | TestF1=0.9331 | BestF1=0.9365 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.1008 | TestF1=0.9266 | BestF1=0.9365 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0719 | TestF1=0.9299 | BestF1=0.9365 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0435 | TestF1=0.9460 | BestF1=0.9460 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0163 | TestF1=0.9403 | BestF1=0.9481 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0161 | TestF1=0.9452 | BestF1=0.9508 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0045 | TestF1=0.9468 | BestF1=0.9508 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0034 | TestF1=0.9373 | BestF1=0.9526 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0017 | TestF1=0.9430 | BestF1=0.9526 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0003 | TestF1=0.9486 | BestF1=0.9526 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0023 | TestF1=0.9486 | BestF1=0.9526 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0010 | TestF1=0.9464 | BestF1=0.9555 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0004 | TestF1=0.9500 | BestF1=0.9555 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0002 | TestF1=0.9422 | BestF1=0.9555 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9485 | BestF1=0.9555 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9471 | BestF1=0.9555 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9482 | BestF1=0.9555 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9485 | BestF1=0.9555 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9471 | BestF1=0.9555 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9555\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o FFN\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1169 | TestF1=0.9327 | BestF1=0.9361 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.0873 | TestF1=0.9389 | BestF1=0.9397 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0433 | TestF1=0.9528 | BestF1=0.9528 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0168 | TestF1=0.9450 | BestF1=0.9607 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0081 | TestF1=0.9552 | BestF1=0.9607 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0044 | TestF1=0.9446 | BestF1=0.9607 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0002 | TestF1=0.9465 | BestF1=0.9607 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0002 | TestF1=0.9487 | BestF1=0.9607 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0001 | TestF1=0.9465 | BestF1=0.9607 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0001 | TestF1=0.9475 | BestF1=0.9607 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9465 | BestF1=0.9607 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0023 | TestF1=0.9488 | BestF1=0.9607 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0010 | TestF1=0.9480 | BestF1=0.9607 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9490 | BestF1=0.9607 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9493 | BestF1=0.9607 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9489 | BestF1=0.9607 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9496 | BestF1=0.9607 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9493 | BestF1=0.9607 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9489 | BestF1=0.9607 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9489 | BestF1=0.9607 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9607\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Residual\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1345 | TestF1=0.9177 | BestF1=0.9299 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.1226 | TestF1=0.9320 | BestF1=0.9320 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0953 | TestF1=0.9207 | BestF1=0.9400 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0863 | TestF1=0.9237 | BestF1=0.9414 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0429 | TestF1=0.9441 | BestF1=0.9441 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0288 | TestF1=0.9550 | BestF1=0.9550 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0173 | TestF1=0.9572 | BestF1=0.9600 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0065 | TestF1=0.9474 | BestF1=0.9619 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0079 | TestF1=0.9454 | BestF1=0.9619 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0053 | TestF1=0.9512 | BestF1=0.9619 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0046 | TestF1=0.9464 | BestF1=0.9619 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0006 | TestF1=0.9500 | BestF1=0.9619 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0003 | TestF1=0.9452 | BestF1=0.9619 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9482 | BestF1=0.9619 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0012 | TestF1=0.9414 | BestF1=0.9619 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9437 | BestF1=0.9619 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0002 | TestF1=0.9409 | BestF1=0.9619 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9409 | BestF1=0.9619 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0002 | TestF1=0.9441 | BestF1=0.9619 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9431 | BestF1=0.9619 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9619\n",
            "\n",
            "==============================================================================================================\n",
            "UCI-HAR Ablation Table\n",
            "==============================================================================================================\n",
            "Variant                | Clean F1 | (SNR=10) F1 | drop(%) | Degree Entropy | NormComp\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Full (Ours)            |   0.9547 |      0.9446 |    1.06 |         0.9990 |   0.6660\n",
            "w/o Gate-1             |   0.9631 |      0.9392 |    2.48 |         0.0000 |   0.3333\n",
            "w/o Gate-2             |   0.9520 |      0.9373 |    1.54 |         0.0000 |   0.6667\n",
            "w/o Gate-3             |   0.9688 |      0.9390 |    3.08 |         0.0000 |   1.0000\n",
            "Gate w/o STE           |   0.9523 |      0.9283 |    2.52 |         0.9370 |   0.6705\n",
            "w/o Hadamard           |   0.9555 |      0.9342 |    2.22 |         0.9933 |   0.6621\n",
            "w/o FFN                |   0.9607 |      0.9417 |    1.98 |         0.9989 |   0.6664\n",
            "w/o Residual           |   0.9619 |      0.9408 |    2.19 |         0.9987 |   0.6667\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[LaTeX rows]\n",
            "Full (Ours) & 0.9547 & 0.9446 & 1.06 & 0.9990 & 0.6660 \\\\\n",
            "w/o Gate-1 & 0.9631 & 0.9392 & 2.48 & 0.0000 & 0.3333 \\\\\n",
            "w/o Gate-2 & 0.9520 & 0.9373 & 1.54 & 0.0000 & 0.6667 \\\\\n",
            "w/o Gate-3 & 0.9688 & 0.9390 & 3.08 & 0.0000 & 1.0000 \\\\\n",
            "Gate w/o STE & 0.9523 & 0.9283 & 2.52 & 0.9370 & 0.6705 \\\\\n",
            "w/o Hadamard & 0.9555 & 0.9342 & 2.22 & 0.9933 & 0.6621 \\\\\n",
            "w/o FFN & 0.9607 & 0.9417 & 1.98 & 0.9989 & 0.6664 \\\\\n",
            "w/o Residual & 0.9619 & 0.9408 & 2.19 & 0.9987 & 0.6667 \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PAMAP2"
      ],
      "metadata": {
        "id": "zTlovHkii3An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def create_pamap2_windows(df: pd.DataFrame, window_size: int, step_size: int):\n",
        "    feature_cols = [\n",
        "        # hand\n",
        "        \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "        \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "        \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "        # chest\n",
        "        \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "        \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "        \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "        # ankle\n",
        "        \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "        \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "        \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "    ]  # C = 27\n",
        "\n",
        "    ORDERED_IDS = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
        "    old2new = {\n",
        "        1: 0,   # Lying\n",
        "        2: 1,   # Sitting\n",
        "        3: 2,   # Standing\n",
        "        4: 3,   # Walking\n",
        "        5: 4,   # Running\n",
        "        6: 5,   # Cycling\n",
        "        7: 6,   # Nordic walking\n",
        "        12: 7,  # Ascending stairs\n",
        "        13: 8,  # Descending stairs\n",
        "        16: 9,  # Vacuum cleaning\n",
        "        17: 10, # Ironing\n",
        "        24: 11, # Rope jumping\n",
        "    }\n",
        "    label_names = [\n",
        "        \"Lying\", \"Sitting\", \"Standing\", \"Walking\",\n",
        "        \"Running\", \"Cycling\", \"Nordic walking\",\n",
        "        \"Ascending stairs\", \"Descending stairs\",\n",
        "        \"Vacuum cleaning\", \"Ironing\", \"Rope jumping\",\n",
        "    ]\n",
        "\n",
        "    X_list, y_list, subj_list = [], [], []\n",
        "\n",
        "    for subj_id, g in df.groupby(\"subject_id\"):\n",
        "        if \"timestamp\" in g.columns:\n",
        "            g = g.sort_values(\"timestamp\")\n",
        "        else:\n",
        "            g = g.sort_index()\n",
        "\n",
        "        data_arr  = g[feature_cols].to_numpy(dtype=np.float32)\n",
        "        label_arr = g[\"activityID\"].to_numpy(dtype=np.int64)\n",
        "        L = data_arr.shape[0]\n",
        "\n",
        "        start = 0\n",
        "        while start + window_size <= L:\n",
        "            end = start + window_size\n",
        "            last_label_orig = int(label_arr[end - 1])\n",
        "\n",
        "            if last_label_orig == 0:\n",
        "                start += step_size\n",
        "                continue\n",
        "            if last_label_orig not in old2new:\n",
        "                start += step_size\n",
        "                continue\n",
        "\n",
        "            window_ct = data_arr[start:end].T\n",
        "            X_list.append(window_ct)\n",
        "            y_list.append(old2new[last_label_orig])\n",
        "            subj_list.append(int(subj_id))\n",
        "            start += step_size\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        raise RuntimeError(\"No windows created. Check window_size/step_size and label filtering.\")\n",
        "\n",
        "    X = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y = np.asarray(y_list, dtype=np.int64)\n",
        "    subj_ids = np.asarray(subj_list, dtype=np.int64)\n",
        "    return X, y, subj_ids, label_names\n",
        "\n",
        "\n",
        "class PAMAP2Dataset(Dataset):\n",
        "    def __init__(self, data_dir, window_size, step_size):\n",
        "        super().__init__()\n",
        "\n",
        "        csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
        "        if len(csv_files) == 0:\n",
        "            raise RuntimeError(f\"No CSV files found under {data_dir}\")\n",
        "\n",
        "        dfs = []\n",
        "        for fpath in sorted(csv_files):\n",
        "            df_i = pd.read_csv(fpath)\n",
        "\n",
        "            if \"subject_id\" not in df_i.columns:\n",
        "                m = re.findall(r\"\\d+\", os.path.basename(fpath))\n",
        "                subj_guess = int(m[0]) if len(m) > 0 else 0\n",
        "                df_i[\"subject_id\"] = subj_guess\n",
        "\n",
        "            dfs.append(df_i)\n",
        "\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        df = df.dropna(subset=[\"activityID\"])\n",
        "        df[\"activityID\"] = df[\"activityID\"].astype(np.int64)\n",
        "        df[\"subject_id\"] = df[\"subject_id\"].astype(np.int64)\n",
        "        if \"timestamp\" in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "        feature_cols = [\n",
        "            # hand\n",
        "            \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "            \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "            \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "            # chest\n",
        "            \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "            \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "            \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "            # ankle\n",
        "            \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "            \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "            \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "        ]\n",
        "\n",
        "        def _fill_subject_group(g):\n",
        "            if \"timestamp\" in g.columns:\n",
        "                g = g.sort_values(\"timestamp\")\n",
        "            else:\n",
        "                g = g.sort_index()\n",
        "            g[feature_cols] = (\n",
        "                g[feature_cols]\n",
        "                .interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
        "                .ffill()\n",
        "                .bfill()\n",
        "            )\n",
        "            return g\n",
        "\n",
        "        df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n",
        "        df[feature_cols] = df[feature_cols].fillna(0.0)\n",
        "\n",
        "        X, y, subj_ids, label_names = create_pamap2_windows(df, window_size, step_size)\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y\n",
        "        self.subject_ids = subj_ids\n",
        "        self.label_names = label_names\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]\n",
        "        N, C, T = Xtr.shape\n",
        "\n",
        "        X2 = np.transpose(Xtr, (0, 2, 1)).reshape(-1, C)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = self.scaler\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = np.transpose(X, (0, 2, 1)).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        X_scaled = X2.reshape(N, T, C).transpose(0, 2, 1)\n",
        "\n",
        "        self.X = X_scaled.astype(np.float32)\n",
        "\n",
        "        print(\"Loaded PAMAP2 dataset\")\n",
        "        print(f\"X shape : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"Classes : {len(self.label_names)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long),\n",
        "            int(self.subject_ids[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions (SNR=10 uses this)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate  (★ Variant behavior matches \"예전버전\")\n",
        "#   - use_ste=True  : train=STE(hard fwd, soft bwd), eval=hard onehot\n",
        "#   - use_ste=False : always soft_probs (train/eval 동일)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits = self.gate(x)  # (B,K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx = logits.argmax(dim=-1)\n",
        "                hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                # STE: forward=hard, backward=soft\n",
        "                degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(\n",
        "                    logits.argmax(dim=-1), num_classes=self.max_degree\n",
        "                ).float()\n",
        "        else:\n",
        "            # Gate w/o STE: always soft (train/eval 동일)\n",
        "            degree_w = soft_probs\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block (Ablation switches ONLY; ★ Variant behavior matches \"예전버전\")\n",
        "#   - w/o Gate: fixed_degree (1..K) → build up to d, output Z[d-1]\n",
        "#   - Gate w/o STE: soft weighted sum of ALL degrees (train/eval 동일)\n",
        "#   - w/o Hadamard: prefix sum (Z[i]=Z[i-1]+Y[i])  (예전버전)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlockAblation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,      # 1..K if w/o Gate (fixed)\n",
        "                 use_ste=True,           # Gate w/o STE (soft routing)\n",
        "                 use_hadamard=True,      # w/o Hadamard (prefix-sum)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_gate = bool(use_gate)\n",
        "        self.fixed_degree = fixed_degree  # None or int in [1..K]\n",
        "        self.use_ste = bool(use_ste)\n",
        "        self.use_hadamard = bool(use_hadamard)\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _build_Y(self, x, max_deg):\n",
        "        return [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        \"\"\"\n",
        "        - use_hadamard=True : 예전버전(원본) Hadamard chain\n",
        "            Z0=Y0, Zi = pre(Z_{i-1}) * Yi\n",
        "        - use_hadamard=False: 예전버전 w/o Hadamard (prefix sum)\n",
        "            Z0=Y0, Zi = Z_{i-1} + Yi\n",
        "        \"\"\"\n",
        "        Y = self._build_Y(x, max_deg)\n",
        "\n",
        "        if self.use_hadamard:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(Z_ * Y[i])\n",
        "            return Z\n",
        "        else:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "            return Z\n",
        "\n",
        "    def _hard_select(self, Z_list, sel):\n",
        "        B = Z_list[0].shape[0]\n",
        "        Z_stack = torch.stack(Z_list, dim=0)  # (K,B,C,T)\n",
        "        return Z_stack[sel, torch.arange(B, device=Z_stack.device)]\n",
        "\n",
        "    def _soft_weighted_output(self, x, soft_probs):\n",
        "        \"\"\"\n",
        "        예전버전 Gate w/o STE:\n",
        "          - always compute ALL K degrees\n",
        "          - weighted sum with soft_probs\n",
        "          - hadamard / no_hadamard build rule은 동일하게 적용\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        Z = self._build_Z(x, max_deg=self.max_degree)      # list length K, each (B,C,T)\n",
        "        Z_stack = torch.stack(Z, dim=1)                    # (B,K,C,T)\n",
        "        w = soft_probs.view(B, self.max_degree, 1, 1)      # (B,K,1,1)\n",
        "        out = (Z_stack * w).sum(dim=1)                     # (B,C,T)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # ---- Case A) w/o Gate (fixed degree 1..K) ----\n",
        "        if (not self.use_gate) or (self.fixed_degree is not None):\n",
        "            d = int(self.fixed_degree) if self.fixed_degree is not None else self.max_degree\n",
        "            d = max(1, min(d, self.max_degree))\n",
        "\n",
        "            # build only up to d, output \"degree d\" path (예전버전)\n",
        "            Z = self._build_Z(x, max_deg=d)\n",
        "            out = Z[-1]\n",
        "\n",
        "            # stats payload (예전버전 스타일)\n",
        "            sel = torch.full((B,), d - 1, device=x.device, dtype=torch.long)\n",
        "            K = self.max_degree\n",
        "            sp = F.one_hot(sel, num_classes=K).float()\n",
        "            dw = sp\n",
        "            logits = sp\n",
        "\n",
        "            out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            if return_gate_info:\n",
        "                return out, {\n",
        "                    \"degree_selection\": dw,\n",
        "                    \"soft_probs\": sp,\n",
        "                    \"logits\": logits,\n",
        "                    \"compute_cost\": float(d),\n",
        "                }\n",
        "            return out\n",
        "\n",
        "        # ---- Case B) Gate ON ----\n",
        "        degree_w, logits, soft_probs = self.degree_gate(x, use_ste=self.use_ste)\n",
        "\n",
        "        if (not self.use_ste):\n",
        "            # 예전버전: Gate w/o STE는 train/eval 관계없이 ALWAYS soft weighted sum\n",
        "            out = self._soft_weighted_output(x, degree_w)  # degree_w == soft_probs\n",
        "            # (stats only) 대표 degree\n",
        "            selected = soft_probs.argmax(dim=-1)\n",
        "            # compute_cost (예전코드에선 argmax 기반이었지만, 여기서는 gi에만 들어가므로 그대로 둠)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "        else:\n",
        "            # 원본: hard select (STE는 train에서 degree_w에 반영됨)\n",
        "            selected = degree_w.argmax(dim=-1)\n",
        "            max_deg = max(1, min(int(selected.max().item()) + 1, self.max_degree))\n",
        "            Z = self._build_Z(x, max_deg=max_deg)\n",
        "            out = self._hard_select(Z, selected)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_w,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": compute_cost,\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model (Ablation switches ONLY; otherwise matches your logic)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR_Ablation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,     # for w/o Gate (1/2/3)\n",
        "                 use_ste=True,          # Gate w/o STE\n",
        "                 use_hadamard=True,     # w/o Hadamard\n",
        "                 use_ffn=True,          # w/o FFN\n",
        "                 use_residual=True,     # w/o Residual\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_ffn = bool(use_ffn)\n",
        "        self.use_residual = bool(use_residual)\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlockAblation(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                token_kernel=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "                use_gate=use_gate,\n",
        "                fixed_degree=fixed_degree,\n",
        "                use_ste=use_ste,\n",
        "                use_hadamard=use_hadamard,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms1[i], x + res)\n",
        "            else:\n",
        "                x = self._ln(self.norms1[i], x)\n",
        "\n",
        "            res2 = x\n",
        "            if self.use_ffn:\n",
        "                x = self.ffn[i](x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms2[i], x + res2)\n",
        "            else:\n",
        "                x = self._ln(self.norms2[i], x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Table Metrics (keep as-is in your \"now code\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_gate_stats(model, loader, device, snr_db=None, max_degree=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      macro_f1, degree_entropy, norm_comp\n",
        "    Definitions:\n",
        "      - degree_entropy: mean over (layers, samples) of normalized entropy of soft_probs\n",
        "                        H(p)/log(K)  where K=max_degree\n",
        "      - norm_comp: mean expected degree / max_degree, averaged over layers\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    ent_sum = 0.0\n",
        "    ent_count = 0\n",
        "    comp_sum = 0.0\n",
        "    comp_count = 0\n",
        "\n",
        "    eps = 1e-12\n",
        "    logK = float(np.log(max_degree))\n",
        "\n",
        "    deg_vals = torch.arange(1, max_degree + 1, device=device).float()\n",
        "\n",
        "    for X, y, _ in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "        # gate stats\n",
        "        for gi in gate_info_list:\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            ent = -(sp * (sp + eps).log()).sum(dim=-1) / logK\n",
        "            ent_sum += ent.mean().item()\n",
        "            ent_count += 1\n",
        "\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "            comp_sum += (exp_deg / max_degree)\n",
        "            comp_count += 1\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    macro_f1 = float(f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    degree_entropy = float(ent_sum / max(ent_count, 1))\n",
        "    norm_comp = float(comp_sum / max(comp_count, 1))\n",
        "    return macro_f1, degree_entropy, norm_comp\n",
        "\n",
        "\n",
        "def format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp):\n",
        "    drop_pct = 100.0 * (clean_f1 - snr10_f1) / max(clean_f1, 1e-12)\n",
        "    return {\n",
        "        \"Variant\": name,\n",
        "        \"CleanF1\": clean_f1,\n",
        "        \"SNR10F1\": snr10_f1,\n",
        "        \"drop(%)\": drop_pct,\n",
        "        \"DegreeEntropy\": deg_ent,\n",
        "        \"NormComp\": norm_comp,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    header = [\"Variant\", \"Clean F1\", \"(SNR=10) F1\", \"drop(%)\", \"Degree Entropy\", \"NormComp\"]\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"UCI-HAR Ablation Table\")\n",
        "    print(\"=\"*110)\n",
        "    print(f\"{header[0]:<22s} | {header[1]:>8s} | {header[2]:>11s} | {header[3]:>7s} | {header[4]:>14s} | {header[5]:>8s}\")\n",
        "    print(\"-\"*110)\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']:<22s} | \"\n",
        "            f\"{r['CleanF1']:>8.4f} | \"\n",
        "            f\"{r['SNR10F1']:>11.4f} | \"\n",
        "            f\"{r['drop(%)']:>7.2f} | \"\n",
        "            f\"{r['DegreeEntropy']:>14.4f} | \"\n",
        "            f\"{r['NormComp']:>8.4f}\"\n",
        "        )\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    print(\"\\n[LaTeX rows]\")\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']} & {r['CleanF1']:.4f} & {r['SNR10F1']:.4f} & \"\n",
        "            f\"{r['drop(%)']:.2f} & {r['DegreeEntropy']:.4f} & {r['NormComp']:.4f} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Experiment Runner (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def build_variant(name, cfg):\n",
        "    model = PADReHAR_Ablation(\n",
        "        in_channels=cfg[\"in_channels\"],\n",
        "        seq_len=cfg[\"seq_len\"],\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_layers=cfg[\"num_layers\"],\n",
        "        max_degree=cfg[\"max_degree\"],\n",
        "        gate_hidden_dim=cfg[\"gate_hidden_dim\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        temperature_initial=cfg[\"temperature_initial\"],\n",
        "        temperature_min=cfg[\"temperature_min\"],\n",
        "        use_gate=cfg.get(\"use_gate\", True),\n",
        "        fixed_degree=cfg.get(\"fixed_degree\", None),\n",
        "        use_ste=cfg.get(\"use_ste\", True),\n",
        "        use_hadamard=cfg.get(\"use_hadamard\", True),\n",
        "        use_ffn=cfg.get(\"use_ffn\", True),\n",
        "        use_residual=cfg.get(\"use_residual\", True),\n",
        "    ).to(cfg[\"device\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg):\n",
        "    variants = []\n",
        "    variants.append((\"Full (Ours)\", dict()))\n",
        "    variants.append((\"w/o Gate-1\", dict(use_gate=False, fixed_degree=1)))\n",
        "    variants.append((\"w/o Gate-2\", dict(use_gate=False, fixed_degree=2)))\n",
        "    variants.append((\"w/o Gate-3\", dict(use_gate=False, fixed_degree=3)))\n",
        "    variants.append((\"Gate w/o STE\", dict(use_gate=True, use_ste=False)))\n",
        "    variants.append((\"w/o Hadamard\", dict(use_hadamard=False)))\n",
        "    variants.append((\"w/o FFN\", dict(use_ffn=False)))\n",
        "    variants.append((\"w/o Residual\", dict(use_residual=False)))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for name, delta in variants:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[Running Variant] {name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        cfg = copy.deepcopy(base_cfg)\n",
        "        cfg.update(delta)\n",
        "\n",
        "        set_seed(train_cfg[\"seed\"])\n",
        "        model = build_variant(name, cfg)\n",
        "        print(f\"Model params: {count_parameters(model):,}\")\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            cfg[\"device\"],\n",
        "            lr=train_cfg[\"lr\"],\n",
        "            weight_decay=train_cfg[\"wd\"],\n",
        "            epochs=train_cfg[\"epochs\"],\n",
        "            seed=train_cfg[\"seed\"],\n",
        "        )\n",
        "\n",
        "        clean_f1, deg_ent, norm_comp = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=None, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "        snr10_f1, _, _ = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=10.0, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "\n",
        "        rows.append(format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH =  \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 50\n",
        "\n",
        "    NUM_CLASSES = 12\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE = 50\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = PAMAP2Dataset(\n",
        "        data_dir=DATA_PATH,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        step_size=STEP_SIZE\n",
        "    )\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    base_cfg = dict(\n",
        "        device=DEVICE,\n",
        "        in_channels=27,\n",
        "        seq_len=100,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "        dropout=DROPOUT,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        use_gate=True,\n",
        "        fixed_degree=None,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    )\n",
        "\n",
        "    train_cfg = dict(\n",
        "        seed=SEED,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        wd=WD,\n",
        "    )\n",
        "\n",
        "    rows = run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg)\n",
        "    print_table(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "X7Ct94csi4w9",
        "outputId": "66d03b1f-09d8-4e00-ca4e-a836b8666e17"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3604495045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m     full_dataset = PAMAP2Dataset(\n\u001b[0m\u001b[1;32m    833\u001b[0m         \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3604495045.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, window_size, step_size)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subject_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_fill_subject_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m                 if (\n\u001b[1;32m   1826\u001b[0m                     \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \"\"\"\n\u001b[0;32m-> 1885\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_groupwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnot_indexed_same\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m             \u001b[0mnot_indexed_same\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply_groupwise\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3604495045.py\u001b[0m in \u001b[0;36m_fill_subject_group\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m    158\u001b[0m             g[feature_cols] = (\n\u001b[1;32m    159\u001b[0m                 \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_direction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"both\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mbfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[1;32m   8497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8498\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_interp_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8499\u001b[0;31m             new_data = obj._mgr.interpolate(\n\u001b[0m\u001b[1;32m   8500\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8501\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, inplace, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         return self.apply_with_block(\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;34m\"interpolate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, method, index, inplace, limit, limit_direction, limit_area, downcast, using_cow, already_warned, **kwargs)\u001b[0m\n\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0;31m# Dispatch to the EA method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m         new_values = self.array_values.interpolate(\n\u001b[0m\u001b[1;32m   1798\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, method, axis, index, limit, limit_direction, limit_area, copy, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# TODO: assert we have floating dtype?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         missing.interpolate_2d_inplace(\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0mout_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/missing.py\u001b[0m in \u001b[0;36minterpolate_2d_inplace\u001b[0;34m(data, index, axis, method, limit, limit_direction, limit_area, fill_value, mask, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;31m# Sequence[Sequence[Sequence[_SupportsArray[dtype[<nothing>]]]]],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;31m# Sequence[Sequence[Sequence[Sequence[_SupportsArray[dtype[<nothing>]]]]]]]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_shape_base_impl.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuff_permute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/missing.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(yvalues)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# process 1-d slices in the axis direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         _interpolate_1d(\n\u001b[0m\u001b[1;32m    392\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0myvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/missing.py\u001b[0m in \u001b[0;36m_interpolate_1d\u001b[0;34m(indices, yvalues, method, limit, limit_direction, limit_area, fill_value, bounds_error, order, mask, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# np.interp requires sorted X values, #21037\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         yvalues[invalid] = np.interp(\n\u001b[1;32m    525\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order, stable)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \"\"\"\n\u001b[0;32m-> 1200\u001b[0;31m     return _wrapfunc(\n\u001b[0m\u001b[1;32m   1201\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def _load_single_mhealth_log(path: str, feature_cols: list[str]):\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        names=feature_cols + [\"label\"],\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def load_mhealth_dataframe(data_dir: str):\n",
        "    feature_cols = [\n",
        "        \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",      # 0,1,2\n",
        "        \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",      # 5,6,7\n",
        "        \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",   # 8,9,10\n",
        "        \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",            # 14,15,16\n",
        "        \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",         # 17,18,19\n",
        "    ]  # total 15 channels\n",
        "\n",
        "    log_files = glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\"))\n",
        "    if not log_files:\n",
        "        raise FileNotFoundError(f\"No mHealth_subject*.log files found in {data_dir}\")\n",
        "    print(f\"Found {len(log_files)} log files in {data_dir}\")\n",
        "\n",
        "    dfs = []\n",
        "    for fp in sorted(log_files):\n",
        "        dfs.append(_load_single_mhealth_log(fp, feature_cols))\n",
        "\n",
        "    full_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    full_df = full_df[full_df[\"label\"] != 0].copy()\n",
        "\n",
        "    full_df.loc[:, \"label\"] = full_df[\"label\"] - 1\n",
        "\n",
        "    return full_df, feature_cols\n",
        "\n",
        "\n",
        "def create_mhealth_windows(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: list[str],\n",
        "    window_size: int,\n",
        "    step_size: int,\n",
        "):\n",
        "    data_arr = df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    labels_arr = df[\"label\"].to_numpy(dtype=np.int64)\n",
        "    L = data_arr.shape[0]\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "    start = 0\n",
        "    while start + window_size <= L:\n",
        "        end = start + window_size\n",
        "        window_x = data_arr[start:end]\n",
        "        window_label = labels_arr[end - 1]\n",
        "        X_list.append(window_x.T)\n",
        "        y_list.append(int(window_label))\n",
        "        start += step_size\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No windows created. Check window_size / step_size / dataset length.\")\n",
        "\n",
        "    X_np = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y_np = np.array(y_list, dtype=np.int64)\n",
        "    return X_np, y_np\n",
        "\n",
        "\n",
        "class MHEALTHDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, window_size: int = 128, step_size: int = 64):\n",
        "        super().__init__()\n",
        "\n",
        "        full_df, feature_cols = load_mhealth_dataframe(data_dir)\n",
        "        X, y = create_mhealth_windows(full_df, feature_cols, window_size, step_size)\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.subjects = np.zeros(len(self.y), dtype=int)\n",
        "\n",
        "        self.label_names = [\n",
        "            \"Standing still\", \"Sitting and relaxing\", \"Lying down\",\n",
        "            \"Walking\", \"Climbing stairs\", \"Waist bends forward\",\n",
        "            \"Frontal elevation of arms\", \"Knees bending\", \"Cycling\",\n",
        "            \"Jogging\", \"Running\", \"Jump front & back\",\n",
        "        ]\n",
        "\n",
        "        print(\"Loaded MHEALTH dataset\")\n",
        "        print(f\"X shape : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"Classes : {len(self.label_names)}\")\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]\n",
        "        N, C, T = Xtr.shape\n",
        "        X2 = Xtr.transpose(0, 2, 1).reshape(-1, C)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = self.scaler\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = X.transpose(0, 2, 1).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        self.X = X2.reshape(N, T, C).transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx]).long(),\n",
        "            int(self.subjects[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions (SNR=10 uses this)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate  (★ Variant behavior matches \"예전버전\")\n",
        "#   - use_ste=True  : train=STE(hard fwd, soft bwd), eval=hard onehot\n",
        "#   - use_ste=False : always soft_probs (train/eval 동일)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits = self.gate(x)  # (B,K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx = logits.argmax(dim=-1)\n",
        "                hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                # STE: forward=hard, backward=soft\n",
        "                degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(\n",
        "                    logits.argmax(dim=-1), num_classes=self.max_degree\n",
        "                ).float()\n",
        "        else:\n",
        "            # Gate w/o STE: always soft (train/eval 동일)\n",
        "            degree_w = soft_probs\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block (Ablation switches ONLY; ★ Variant behavior matches \"예전버전\")\n",
        "#   - w/o Gate: fixed_degree (1..K) → build up to d, output Z[d-1]\n",
        "#   - Gate w/o STE: soft weighted sum of ALL degrees (train/eval 동일)\n",
        "#   - w/o Hadamard: prefix sum (Z[i]=Z[i-1]+Y[i])  (예전버전)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlockAblation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,      # 1..K if w/o Gate (fixed)\n",
        "                 use_ste=True,           # Gate w/o STE (soft routing)\n",
        "                 use_hadamard=True,      # w/o Hadamard (prefix-sum)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_gate = bool(use_gate)\n",
        "        self.fixed_degree = fixed_degree  # None or int in [1..K]\n",
        "        self.use_ste = bool(use_ste)\n",
        "        self.use_hadamard = bool(use_hadamard)\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _build_Y(self, x, max_deg):\n",
        "        return [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        \"\"\"\n",
        "        - use_hadamard=True : 예전버전(원본) Hadamard chain\n",
        "            Z0=Y0, Zi = pre(Z_{i-1}) * Yi\n",
        "        - use_hadamard=False: 예전버전 w/o Hadamard (prefix sum)\n",
        "            Z0=Y0, Zi = Z_{i-1} + Yi\n",
        "        \"\"\"\n",
        "        Y = self._build_Y(x, max_deg)\n",
        "\n",
        "        if self.use_hadamard:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(Z_ * Y[i])\n",
        "            return Z\n",
        "        else:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "            return Z\n",
        "\n",
        "    def _hard_select(self, Z_list, sel):\n",
        "        B = Z_list[0].shape[0]\n",
        "        Z_stack = torch.stack(Z_list, dim=0)  # (K,B,C,T)\n",
        "        return Z_stack[sel, torch.arange(B, device=Z_stack.device)]\n",
        "\n",
        "    def _soft_weighted_output(self, x, soft_probs):\n",
        "        \"\"\"\n",
        "        예전버전 Gate w/o STE:\n",
        "          - always compute ALL K degrees\n",
        "          - weighted sum with soft_probs\n",
        "          - hadamard / no_hadamard build rule은 동일하게 적용\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        Z = self._build_Z(x, max_deg=self.max_degree)      # list length K, each (B,C,T)\n",
        "        Z_stack = torch.stack(Z, dim=1)                    # (B,K,C,T)\n",
        "        w = soft_probs.view(B, self.max_degree, 1, 1)      # (B,K,1,1)\n",
        "        out = (Z_stack * w).sum(dim=1)                     # (B,C,T)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # ---- Case A) w/o Gate (fixed degree 1..K) ----\n",
        "        if (not self.use_gate) or (self.fixed_degree is not None):\n",
        "            d = int(self.fixed_degree) if self.fixed_degree is not None else self.max_degree\n",
        "            d = max(1, min(d, self.max_degree))\n",
        "\n",
        "            # build only up to d, output \"degree d\" path (예전버전)\n",
        "            Z = self._build_Z(x, max_deg=d)\n",
        "            out = Z[-1]\n",
        "\n",
        "            # stats payload (예전버전 스타일)\n",
        "            sel = torch.full((B,), d - 1, device=x.device, dtype=torch.long)\n",
        "            K = self.max_degree\n",
        "            sp = F.one_hot(sel, num_classes=K).float()\n",
        "            dw = sp\n",
        "            logits = sp\n",
        "\n",
        "            out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            if return_gate_info:\n",
        "                return out, {\n",
        "                    \"degree_selection\": dw,\n",
        "                    \"soft_probs\": sp,\n",
        "                    \"logits\": logits,\n",
        "                    \"compute_cost\": float(d),\n",
        "                }\n",
        "            return out\n",
        "\n",
        "        # ---- Case B) Gate ON ----\n",
        "        degree_w, logits, soft_probs = self.degree_gate(x, use_ste=self.use_ste)\n",
        "\n",
        "        if (not self.use_ste):\n",
        "            # 예전버전: Gate w/o STE는 train/eval 관계없이 ALWAYS soft weighted sum\n",
        "            out = self._soft_weighted_output(x, degree_w)  # degree_w == soft_probs\n",
        "            # (stats only) 대표 degree\n",
        "            selected = soft_probs.argmax(dim=-1)\n",
        "            # compute_cost (예전코드에선 argmax 기반이었지만, 여기서는 gi에만 들어가므로 그대로 둠)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "        else:\n",
        "            # 원본: hard select (STE는 train에서 degree_w에 반영됨)\n",
        "            selected = degree_w.argmax(dim=-1)\n",
        "            max_deg = max(1, min(int(selected.max().item()) + 1, self.max_degree))\n",
        "            Z = self._build_Z(x, max_deg=max_deg)\n",
        "            out = self._hard_select(Z, selected)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_w,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": compute_cost,\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model (Ablation switches ONLY; otherwise matches your logic)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR_Ablation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,     # for w/o Gate (1/2/3)\n",
        "                 use_ste=True,          # Gate w/o STE\n",
        "                 use_hadamard=True,     # w/o Hadamard\n",
        "                 use_ffn=True,          # w/o FFN\n",
        "                 use_residual=True,     # w/o Residual\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_ffn = bool(use_ffn)\n",
        "        self.use_residual = bool(use_residual)\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlockAblation(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                token_kernel=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "                use_gate=use_gate,\n",
        "                fixed_degree=fixed_degree,\n",
        "                use_ste=use_ste,\n",
        "                use_hadamard=use_hadamard,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms1[i], x + res)\n",
        "            else:\n",
        "                x = self._ln(self.norms1[i], x)\n",
        "\n",
        "            res2 = x\n",
        "            if self.use_ffn:\n",
        "                x = self.ffn[i](x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms2[i], x + res2)\n",
        "            else:\n",
        "                x = self._ln(self.norms2[i], x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Table Metrics (keep as-is in your \"now code\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_gate_stats(model, loader, device, snr_db=None, max_degree=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      macro_f1, degree_entropy, norm_comp\n",
        "    Definitions:\n",
        "      - degree_entropy: mean over (layers, samples) of normalized entropy of soft_probs\n",
        "                        H(p)/log(K)  where K=max_degree\n",
        "      - norm_comp: mean expected degree / max_degree, averaged over layers\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    ent_sum = 0.0\n",
        "    ent_count = 0\n",
        "    comp_sum = 0.0\n",
        "    comp_count = 0\n",
        "\n",
        "    eps = 1e-12\n",
        "    logK = float(np.log(max_degree))\n",
        "\n",
        "    deg_vals = torch.arange(1, max_degree + 1, device=device).float()\n",
        "\n",
        "    for X, y, _ in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "        # gate stats\n",
        "        for gi in gate_info_list:\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            ent = -(sp * (sp + eps).log()).sum(dim=-1) / logK\n",
        "            ent_sum += ent.mean().item()\n",
        "            ent_count += 1\n",
        "\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "            comp_sum += (exp_deg / max_degree)\n",
        "            comp_count += 1\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    macro_f1 = float(f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    degree_entropy = float(ent_sum / max(ent_count, 1))\n",
        "    norm_comp = float(comp_sum / max(comp_count, 1))\n",
        "    return macro_f1, degree_entropy, norm_comp\n",
        "\n",
        "\n",
        "def format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp):\n",
        "    drop_pct = 100.0 * (clean_f1 - snr10_f1) / max(clean_f1, 1e-12)\n",
        "    return {\n",
        "        \"Variant\": name,\n",
        "        \"CleanF1\": clean_f1,\n",
        "        \"SNR10F1\": snr10_f1,\n",
        "        \"drop(%)\": drop_pct,\n",
        "        \"DegreeEntropy\": deg_ent,\n",
        "        \"NormComp\": norm_comp,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    header = [\"Variant\", \"Clean F1\", \"(SNR=10) F1\", \"drop(%)\", \"Degree Entropy\", \"NormComp\"]\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"UCI-HAR Ablation Table\")\n",
        "    print(\"=\"*110)\n",
        "    print(f\"{header[0]:<22s} | {header[1]:>8s} | {header[2]:>11s} | {header[3]:>7s} | {header[4]:>14s} | {header[5]:>8s}\")\n",
        "    print(\"-\"*110)\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']:<22s} | \"\n",
        "            f\"{r['CleanF1']:>8.4f} | \"\n",
        "            f\"{r['SNR10F1']:>11.4f} | \"\n",
        "            f\"{r['drop(%)']:>7.2f} | \"\n",
        "            f\"{r['DegreeEntropy']:>14.4f} | \"\n",
        "            f\"{r['NormComp']:>8.4f}\"\n",
        "        )\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    print(\"\\n[LaTeX rows]\")\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']} & {r['CleanF1']:.4f} & {r['SNR10F1']:.4f} & \"\n",
        "            f\"{r['drop(%)']:.2f} & {r['DegreeEntropy']:.4f} & {r['NormComp']:.4f} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Experiment Runner (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def build_variant(name, cfg):\n",
        "    model = PADReHAR_Ablation(\n",
        "        in_channels=cfg[\"in_channels\"],\n",
        "        seq_len=cfg[\"seq_len\"],\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_layers=cfg[\"num_layers\"],\n",
        "        max_degree=cfg[\"max_degree\"],\n",
        "        gate_hidden_dim=cfg[\"gate_hidden_dim\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        temperature_initial=cfg[\"temperature_initial\"],\n",
        "        temperature_min=cfg[\"temperature_min\"],\n",
        "        use_gate=cfg.get(\"use_gate\", True),\n",
        "        fixed_degree=cfg.get(\"fixed_degree\", None),\n",
        "        use_ste=cfg.get(\"use_ste\", True),\n",
        "        use_hadamard=cfg.get(\"use_hadamard\", True),\n",
        "        use_ffn=cfg.get(\"use_ffn\", True),\n",
        "        use_residual=cfg.get(\"use_residual\", True),\n",
        "    ).to(cfg[\"device\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg):\n",
        "    variants = []\n",
        "    variants.append((\"Full (Ours)\", dict()))\n",
        "    variants.append((\"w/o Gate-1\", dict(use_gate=False, fixed_degree=1)))\n",
        "    variants.append((\"w/o Gate-2\", dict(use_gate=False, fixed_degree=2)))\n",
        "    variants.append((\"w/o Gate-3\", dict(use_gate=False, fixed_degree=3)))\n",
        "    variants.append((\"Gate w/o STE\", dict(use_gate=True, use_ste=False)))\n",
        "    variants.append((\"w/o Hadamard\", dict(use_hadamard=False)))\n",
        "    variants.append((\"w/o FFN\", dict(use_ffn=False)))\n",
        "    variants.append((\"w/o Residual\", dict(use_residual=False)))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for name, delta in variants:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[Running Variant] {name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        cfg = copy.deepcopy(base_cfg)\n",
        "        cfg.update(delta)\n",
        "\n",
        "        set_seed(train_cfg[\"seed\"])\n",
        "        model = build_variant(name, cfg)\n",
        "        print(f\"Model params: {count_parameters(model):,}\")\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            cfg[\"device\"],\n",
        "            lr=train_cfg[\"lr\"],\n",
        "            weight_decay=train_cfg[\"wd\"],\n",
        "            epochs=train_cfg[\"epochs\"],\n",
        "            seed=train_cfg[\"seed\"],\n",
        "        )\n",
        "\n",
        "        clean_f1, deg_ent, norm_comp = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=None, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "        snr10_f1, _, _ = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=10.0, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "\n",
        "        rows.append(format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH =  \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 25\n",
        "\n",
        "    NUM_CLASSES = 12\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE = 50\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = MHEALTHDataset(DATA_PATH, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    base_cfg = dict(\n",
        "        device=DEVICE,\n",
        "        in_channels=15,\n",
        "        seq_len=100,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "        dropout=DROPOUT,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        use_gate=True,\n",
        "        fixed_degree=None,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    )\n",
        "\n",
        "    train_cfg = dict(\n",
        "        seed=SEED,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        wd=WD,\n",
        "    )\n",
        "\n",
        "    rows = run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg)\n",
        "    print_table(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OL7esDTvfEo",
        "outputId": "c53638fe-16a1-4e40-ed7c-1169313f1042"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "Found 10 log files in /content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\n",
            "Loaded MHEALTH dataset\n",
            "X shape : (6862, 15, 100)  (N, C, T)\n",
            "y shape : (6862,)  (N,)\n",
            "Classes : 12\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Full (Ours)\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/25 | LR=0.0009 | Train Loss=0.0915 | TestF1=0.9812 | BestF1=0.9812 | Temp=4.699\n",
            "Epoch 10/25 | LR=0.0007 | Train Loss=0.0425 | TestF1=0.9853 | BestF1=0.9853 | Temp=3.611\n",
            "Epoch 15/25 | LR=0.0004 | Train Loss=0.0259 | TestF1=0.9853 | BestF1=0.9853 | Temp=2.168\n",
            "Epoch 20/25 | LR=0.0001 | Train Loss=0.0153 | TestF1=0.9839 | BestF1=0.9853 | Temp=0.965\n",
            "Epoch 25/25 | LR=0.0000 | Train Loss=0.0115 | TestF1=0.9839 | BestF1=0.9853 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9853\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-1\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/25 | LR=0.0009 | Train Loss=0.1706 | TestF1=0.9747 | BestF1=0.9747 | Temp=4.699\n",
            "Epoch 10/25 | LR=0.0007 | Train Loss=0.0833 | TestF1=0.9762 | BestF1=0.9811 | Temp=3.611\n",
            "Epoch 15/25 | LR=0.0004 | Train Loss=0.0551 | TestF1=0.9839 | BestF1=0.9847 | Temp=2.168\n",
            "Epoch 20/25 | LR=0.0001 | Train Loss=0.0385 | TestF1=0.9840 | BestF1=0.9847 | Temp=0.965\n",
            "Epoch 25/25 | LR=0.0000 | Train Loss=0.0374 | TestF1=0.9840 | BestF1=0.9847 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9847\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-2\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/25 | LR=0.0009 | Train Loss=0.0869 | TestF1=0.9860 | BestF1=0.9860 | Temp=4.699\n",
            "Epoch 10/25 | LR=0.0007 | Train Loss=0.0449 | TestF1=0.9839 | BestF1=0.9860 | Temp=3.611\n",
            "Epoch 15/25 | LR=0.0004 | Train Loss=0.0241 | TestF1=0.9867 | BestF1=0.9867 | Temp=2.168\n",
            "Epoch 20/25 | LR=0.0001 | Train Loss=0.0129 | TestF1=0.9860 | BestF1=0.9867 | Temp=0.965\n",
            "Epoch 25/25 | LR=0.0000 | Train Loss=0.0102 | TestF1=0.9867 | BestF1=0.9867 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9867\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-3\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/25 | LR=0.0009 | Train Loss=0.0833 | TestF1=0.9825 | BestF1=0.9825 | Temp=4.699\n",
            "Epoch 10/25 | LR=0.0007 | Train Loss=0.0428 | TestF1=0.9839 | BestF1=0.9853 | Temp=3.611\n",
            "Epoch 15/25 | LR=0.0004 | Train Loss=0.0210 | TestF1=0.9846 | BestF1=0.9853 | Temp=2.168\n",
            "Epoch 20/25 | LR=0.0001 | Train Loss=0.0120 | TestF1=0.9839 | BestF1=0.9853 | Temp=0.965\n",
            "Epoch 25/25 | LR=0.0000 | Train Loss=0.0096 | TestF1=0.9846 | BestF1=0.9853 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9853\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Gate w/o STE\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/25 | LR=0.0009 | Train Loss=0.1003 | TestF1=0.9846 | BestF1=0.9846 | Temp=4.699\n",
            "Epoch 10/25 | LR=0.0007 | Train Loss=0.0566 | TestF1=0.9838 | BestF1=0.9846 | Temp=3.611\n",
            "Epoch 15/25 | LR=0.0004 | Train Loss=0.0271 | TestF1=0.9860 | BestF1=0.9867 | Temp=2.168\n",
            "Epoch 20/25 | LR=0.0001 | Train Loss=0.0158 | TestF1=0.9847 | BestF1=0.9867 | Temp=0.965\n",
            "Epoch 25/25 | LR=0.0000 | Train Loss=0.0153 | TestF1=0.9860 | BestF1=0.9867 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9867\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Hadamard\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/25 | LR=0.0009 | Train Loss=0.1478 | TestF1=0.9785 | BestF1=0.9785 | Temp=4.699\n",
            "Epoch 10/25 | LR=0.0007 | Train Loss=0.0787 | TestF1=0.9713 | BestF1=0.9796 | Temp=3.611\n",
            "Epoch 15/25 | LR=0.0004 | Train Loss=0.0522 | TestF1=0.9839 | BestF1=0.9839 | Temp=2.168\n",
            "Epoch 20/25 | LR=0.0001 | Train Loss=0.0383 | TestF1=0.9817 | BestF1=0.9839 | Temp=0.965\n",
            "Epoch 25/25 | LR=0.0000 | Train Loss=0.0352 | TestF1=0.9817 | BestF1=0.9839 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9839\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o FFN\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/25 | LR=0.0009 | Train Loss=0.0936 | TestF1=0.9826 | BestF1=0.9826 | Temp=4.699\n",
            "Epoch 10/25 | LR=0.0007 | Train Loss=0.0501 | TestF1=0.9805 | BestF1=0.9826 | Temp=3.611\n",
            "Epoch 15/25 | LR=0.0004 | Train Loss=0.0281 | TestF1=0.9833 | BestF1=0.9840 | Temp=2.168\n",
            "Epoch 20/25 | LR=0.0001 | Train Loss=0.0179 | TestF1=0.9846 | BestF1=0.9847 | Temp=0.965\n",
            "Epoch 25/25 | LR=0.0000 | Train Loss=0.0122 | TestF1=0.9846 | BestF1=0.9847 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9847\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Residual\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/25 | LR=0.0009 | Train Loss=0.1352 | TestF1=0.9663 | BestF1=0.9675 | Temp=4.699\n",
            "Epoch 10/25 | LR=0.0007 | Train Loss=0.0574 | TestF1=0.9764 | BestF1=0.9818 | Temp=3.611\n",
            "Epoch 15/25 | LR=0.0004 | Train Loss=0.0317 | TestF1=0.9845 | BestF1=0.9860 | Temp=2.168\n",
            "Epoch 20/25 | LR=0.0001 | Train Loss=0.0167 | TestF1=0.9838 | BestF1=0.9867 | Temp=0.965\n",
            "Epoch 25/25 | LR=0.0000 | Train Loss=0.0139 | TestF1=0.9852 | BestF1=0.9867 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9867\n",
            "\n",
            "==============================================================================================================\n",
            "UCI-HAR Ablation Table\n",
            "==============================================================================================================\n",
            "Variant                | Clean F1 | (SNR=10) F1 | drop(%) | Degree Entropy | NormComp\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Full (Ours)            |   0.9853 |      0.9812 |    0.42 |         0.9985 |   0.6666\n",
            "w/o Gate-1             |   0.9847 |      0.9810 |    0.38 |         0.0000 |   0.3333\n",
            "w/o Gate-2             |   0.9867 |      0.9853 |    0.14 |         0.0000 |   0.6667\n",
            "w/o Gate-3             |   0.9853 |      0.9853 |   -0.00 |         0.0000 |   1.0000\n",
            "Gate w/o STE           |   0.9867 |      0.9867 |   -0.00 |         0.9555 |   0.7038\n",
            "w/o Hadamard           |   0.9839 |      0.9839 |    0.00 |         0.9958 |   0.6661\n",
            "w/o FFN                |   0.9847 |      0.9819 |    0.28 |         0.9904 |   0.6670\n",
            "w/o Residual           |   0.9867 |      0.9874 |   -0.07 |         0.9901 |   0.6644\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[LaTeX rows]\n",
            "Full (Ours) & 0.9853 & 0.9812 & 0.42 & 0.9985 & 0.6666 \\\\\n",
            "w/o Gate-1 & 0.9847 & 0.9810 & 0.38 & 0.0000 & 0.3333 \\\\\n",
            "w/o Gate-2 & 0.9867 & 0.9853 & 0.14 & 0.0000 & 0.6667 \\\\\n",
            "w/o Gate-3 & 0.9853 & 0.9853 & -0.00 & 0.0000 & 1.0000 \\\\\n",
            "Gate w/o STE & 0.9867 & 0.9867 & -0.00 & 0.9555 & 0.7038 \\\\\n",
            "w/o Hadamard & 0.9839 & 0.9839 & 0.00 & 0.9958 & 0.6661 \\\\\n",
            "w/o FFN & 0.9847 & 0.9819 & 0.28 & 0.9904 & 0.6670 \\\\\n",
            "w/o Residual & 0.9867 & 0.9874 & -0.07 & 0.9901 & 0.6644 \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WISDM"
      ],
      "metadata": {
        "id": "1VA2QRlFwBtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def _load_single_mhealth_log(path: str, feature_cols: list[str]):\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        names=feature_cols + [\"label\"],\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def load_mhealth_dataframe(data_dir: str):\n",
        "    feature_cols = [\n",
        "        \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",      # 0,1,2\n",
        "        \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",      # 5,6,7\n",
        "        \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",   # 8,9,10\n",
        "        \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",            # 14,15,16\n",
        "        \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",         # 17,18,19\n",
        "    ]  # total 15 channels\n",
        "\n",
        "    log_files = glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\"))\n",
        "    if not log_files:\n",
        "        raise FileNotFoundError(f\"No mHealth_subject*.log files found in {data_dir}\")\n",
        "    print(f\"Found {len(log_files)} log files in {data_dir}\")\n",
        "\n",
        "    dfs = []\n",
        "    for fp in sorted(log_files):\n",
        "        dfs.append(_load_single_mhealth_log(fp, feature_cols))\n",
        "\n",
        "    full_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    full_df = full_df[full_df[\"label\"] != 0].copy()\n",
        "\n",
        "    full_df.loc[:, \"label\"] = full_df[\"label\"] - 1\n",
        "\n",
        "    return full_df, feature_cols\n",
        "\n",
        "\n",
        "def create_mhealth_windows(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: list[str],\n",
        "    window_size: int,\n",
        "    step_size: int,\n",
        "):\n",
        "    data_arr = df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    labels_arr = df[\"label\"].to_numpy(dtype=np.int64)\n",
        "    L = data_arr.shape[0]\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "    start = 0\n",
        "    while start + window_size <= L:\n",
        "        end = start + window_size\n",
        "        window_x = data_arr[start:end]\n",
        "        window_label = labels_arr[end - 1]\n",
        "        X_list.append(window_x.T)\n",
        "        y_list.append(int(window_label))\n",
        "        start += step_size\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No windows created. Check window_size / step_size / dataset length.\")\n",
        "\n",
        "    X_np = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y_np = np.array(y_list, dtype=np.int64)\n",
        "    return X_np, y_np\n",
        "\n",
        "\n",
        "class MHEALTHDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, window_size: int = 128, step_size: int = 64):\n",
        "        super().__init__()\n",
        "\n",
        "        full_df, feature_cols = load_mhealth_dataframe(data_dir)\n",
        "        X, y = create_mhealth_windows(full_df, feature_cols, window_size, step_size)\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.subjects = np.zeros(len(self.y), dtype=int)\n",
        "\n",
        "        self.label_names = [\n",
        "            \"Standing still\", \"Sitting and relaxing\", \"Lying down\",\n",
        "            \"Walking\", \"Climbing stairs\", \"Waist bends forward\",\n",
        "            \"Frontal elevation of arms\", \"Knees bending\", \"Cycling\",\n",
        "            \"Jogging\", \"Running\", \"Jump front & back\",\n",
        "        ]\n",
        "\n",
        "        print(\"Loaded MHEALTH dataset\")\n",
        "        print(f\"X shape : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"Classes : {len(self.label_names)}\")\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]\n",
        "        N, C, T = Xtr.shape\n",
        "        X2 = Xtr.transpose(0, 2, 1).reshape(-1, C)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = self.scaler\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = X.transpose(0, 2, 1).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        self.X = X2.reshape(N, T, C).transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx]).long(),\n",
        "            int(self.subjects[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions (SNR=10 uses this)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate  (★ Variant behavior matches \"예전버전\")\n",
        "#   - use_ste=True  : train=STE(hard fwd, soft bwd), eval=hard onehot\n",
        "#   - use_ste=False : always soft_probs (train/eval 동일)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits = self.gate(x)  # (B,K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx = logits.argmax(dim=-1)\n",
        "                hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                # STE: forward=hard, backward=soft\n",
        "                degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(\n",
        "                    logits.argmax(dim=-1), num_classes=self.max_degree\n",
        "                ).float()\n",
        "        else:\n",
        "            # Gate w/o STE: always soft (train/eval 동일)\n",
        "            degree_w = soft_probs\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block (Ablation switches ONLY; ★ Variant behavior matches \"예전버전\")\n",
        "#   - w/o Gate: fixed_degree (1..K) → build up to d, output Z[d-1]\n",
        "#   - Gate w/o STE: soft weighted sum of ALL degrees (train/eval 동일)\n",
        "#   - w/o Hadamard: prefix sum (Z[i]=Z[i-1]+Y[i])  (예전버전)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlockAblation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,      # 1..K if w/o Gate (fixed)\n",
        "                 use_ste=True,           # Gate w/o STE (soft routing)\n",
        "                 use_hadamard=True,      # w/o Hadamard (prefix-sum)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_gate = bool(use_gate)\n",
        "        self.fixed_degree = fixed_degree  # None or int in [1..K]\n",
        "        self.use_ste = bool(use_ste)\n",
        "        self.use_hadamard = bool(use_hadamard)\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _build_Y(self, x, max_deg):\n",
        "        return [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        \"\"\"\n",
        "        - use_hadamard=True : 예전버전(원본) Hadamard chain\n",
        "            Z0=Y0, Zi = pre(Z_{i-1}) * Yi\n",
        "        - use_hadamard=False: 예전버전 w/o Hadamard (prefix sum)\n",
        "            Z0=Y0, Zi = Z_{i-1} + Yi\n",
        "        \"\"\"\n",
        "        Y = self._build_Y(x, max_deg)\n",
        "\n",
        "        if self.use_hadamard:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(Z_ * Y[i])\n",
        "            return Z\n",
        "        else:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "            return Z\n",
        "\n",
        "    def _hard_select(self, Z_list, sel):\n",
        "        B = Z_list[0].shape[0]\n",
        "        Z_stack = torch.stack(Z_list, dim=0)  # (K,B,C,T)\n",
        "        return Z_stack[sel, torch.arange(B, device=Z_stack.device)]\n",
        "\n",
        "    def _soft_weighted_output(self, x, soft_probs):\n",
        "        \"\"\"\n",
        "        예전버전 Gate w/o STE:\n",
        "          - always compute ALL K degrees\n",
        "          - weighted sum with soft_probs\n",
        "          - hadamard / no_hadamard build rule은 동일하게 적용\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        Z = self._build_Z(x, max_deg=self.max_degree)      # list length K, each (B,C,T)\n",
        "        Z_stack = torch.stack(Z, dim=1)                    # (B,K,C,T)\n",
        "        w = soft_probs.view(B, self.max_degree, 1, 1)      # (B,K,1,1)\n",
        "        out = (Z_stack * w).sum(dim=1)                     # (B,C,T)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # ---- Case A) w/o Gate (fixed degree 1..K) ----\n",
        "        if (not self.use_gate) or (self.fixed_degree is not None):\n",
        "            d = int(self.fixed_degree) if self.fixed_degree is not None else self.max_degree\n",
        "            d = max(1, min(d, self.max_degree))\n",
        "\n",
        "            # build only up to d, output \"degree d\" path (예전버전)\n",
        "            Z = self._build_Z(x, max_deg=d)\n",
        "            out = Z[-1]\n",
        "\n",
        "            # stats payload (예전버전 스타일)\n",
        "            sel = torch.full((B,), d - 1, device=x.device, dtype=torch.long)\n",
        "            K = self.max_degree\n",
        "            sp = F.one_hot(sel, num_classes=K).float()\n",
        "            dw = sp\n",
        "            logits = sp\n",
        "\n",
        "            out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            if return_gate_info:\n",
        "                return out, {\n",
        "                    \"degree_selection\": dw,\n",
        "                    \"soft_probs\": sp,\n",
        "                    \"logits\": logits,\n",
        "                    \"compute_cost\": float(d),\n",
        "                }\n",
        "            return out\n",
        "\n",
        "        # ---- Case B) Gate ON ----\n",
        "        degree_w, logits, soft_probs = self.degree_gate(x, use_ste=self.use_ste)\n",
        "\n",
        "        if (not self.use_ste):\n",
        "            # 예전버전: Gate w/o STE는 train/eval 관계없이 ALWAYS soft weighted sum\n",
        "            out = self._soft_weighted_output(x, degree_w)  # degree_w == soft_probs\n",
        "            # (stats only) 대표 degree\n",
        "            selected = soft_probs.argmax(dim=-1)\n",
        "            # compute_cost (예전코드에선 argmax 기반이었지만, 여기서는 gi에만 들어가므로 그대로 둠)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "        else:\n",
        "            # 원본: hard select (STE는 train에서 degree_w에 반영됨)\n",
        "            selected = degree_w.argmax(dim=-1)\n",
        "            max_deg = max(1, min(int(selected.max().item()) + 1, self.max_degree))\n",
        "            Z = self._build_Z(x, max_deg=max_deg)\n",
        "            out = self._hard_select(Z, selected)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_w,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": compute_cost,\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model (Ablation switches ONLY; otherwise matches your logic)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR_Ablation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,     # for w/o Gate (1/2/3)\n",
        "                 use_ste=True,          # Gate w/o STE\n",
        "                 use_hadamard=True,     # w/o Hadamard\n",
        "                 use_ffn=True,          # w/o FFN\n",
        "                 use_residual=True,     # w/o Residual\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_ffn = bool(use_ffn)\n",
        "        self.use_residual = bool(use_residual)\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlockAblation(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                token_kernel=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "                use_gate=use_gate,\n",
        "                fixed_degree=fixed_degree,\n",
        "                use_ste=use_ste,\n",
        "                use_hadamard=use_hadamard,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms1[i], x + res)\n",
        "            else:\n",
        "                x = self._ln(self.norms1[i], x)\n",
        "\n",
        "            res2 = x\n",
        "            if self.use_ffn:\n",
        "                x = self.ffn[i](x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms2[i], x + res2)\n",
        "            else:\n",
        "                x = self._ln(self.norms2[i], x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Table Metrics (keep as-is in your \"now code\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_gate_stats(model, loader, device, snr_db=None, max_degree=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      macro_f1, degree_entropy, norm_comp\n",
        "    Definitions:\n",
        "      - degree_entropy: mean over (layers, samples) of normalized entropy of soft_probs\n",
        "                        H(p)/log(K)  where K=max_degree\n",
        "      - norm_comp: mean expected degree / max_degree, averaged over layers\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    ent_sum = 0.0\n",
        "    ent_count = 0\n",
        "    comp_sum = 0.0\n",
        "    comp_count = 0\n",
        "\n",
        "    eps = 1e-12\n",
        "    logK = float(np.log(max_degree))\n",
        "\n",
        "    deg_vals = torch.arange(1, max_degree + 1, device=device).float()\n",
        "\n",
        "    for X, y, _ in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "        # gate stats\n",
        "        for gi in gate_info_list:\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            ent = -(sp * (sp + eps).log()).sum(dim=-1) / logK\n",
        "            ent_sum += ent.mean().item()\n",
        "            ent_count += 1\n",
        "\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "            comp_sum += (exp_deg / max_degree)\n",
        "            comp_count += 1\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    macro_f1 = float(f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    degree_entropy = float(ent_sum / max(ent_count, 1))\n",
        "    norm_comp = float(comp_sum / max(comp_count, 1))\n",
        "    return macro_f1, degree_entropy, norm_comp\n",
        "\n",
        "\n",
        "def format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp):\n",
        "    drop_pct = 100.0 * (clean_f1 - snr10_f1) / max(clean_f1, 1e-12)\n",
        "    return {\n",
        "        \"Variant\": name,\n",
        "        \"CleanF1\": clean_f1,\n",
        "        \"SNR10F1\": snr10_f1,\n",
        "        \"drop(%)\": drop_pct,\n",
        "        \"DegreeEntropy\": deg_ent,\n",
        "        \"NormComp\": norm_comp,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    header = [\"Variant\", \"Clean F1\", \"(SNR=10) F1\", \"drop(%)\", \"Degree Entropy\", \"NormComp\"]\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"UCI-HAR Ablation Table\")\n",
        "    print(\"=\"*110)\n",
        "    print(f\"{header[0]:<22s} | {header[1]:>8s} | {header[2]:>11s} | {header[3]:>7s} | {header[4]:>14s} | {header[5]:>8s}\")\n",
        "    print(\"-\"*110)\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']:<22s} | \"\n",
        "            f\"{r['CleanF1']:>8.4f} | \"\n",
        "            f\"{r['SNR10F1']:>11.4f} | \"\n",
        "            f\"{r['drop(%)']:>7.2f} | \"\n",
        "            f\"{r['DegreeEntropy']:>14.4f} | \"\n",
        "            f\"{r['NormComp']:>8.4f}\"\n",
        "        )\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    print(\"\\n[LaTeX rows]\")\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']} & {r['CleanF1']:.4f} & {r['SNR10F1']:.4f} & \"\n",
        "            f\"{r['drop(%)']:.2f} & {r['DegreeEntropy']:.4f} & {r['NormComp']:.4f} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Experiment Runner (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def build_variant(name, cfg):\n",
        "    model = PADReHAR_Ablation(\n",
        "        in_channels=cfg[\"in_channels\"],\n",
        "        seq_len=cfg[\"seq_len\"],\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_layers=cfg[\"num_layers\"],\n",
        "        max_degree=cfg[\"max_degree\"],\n",
        "        gate_hidden_dim=cfg[\"gate_hidden_dim\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        temperature_initial=cfg[\"temperature_initial\"],\n",
        "        temperature_min=cfg[\"temperature_min\"],\n",
        "        use_gate=cfg.get(\"use_gate\", True),\n",
        "        fixed_degree=cfg.get(\"fixed_degree\", None),\n",
        "        use_ste=cfg.get(\"use_ste\", True),\n",
        "        use_hadamard=cfg.get(\"use_hadamard\", True),\n",
        "        use_ffn=cfg.get(\"use_ffn\", True),\n",
        "        use_residual=cfg.get(\"use_residual\", True),\n",
        "    ).to(cfg[\"device\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg):\n",
        "    variants = []\n",
        "    variants.append((\"Full (Ours)\", dict()))\n",
        "    variants.append((\"w/o Gate-1\", dict(use_gate=False, fixed_degree=1)))\n",
        "    variants.append((\"w/o Gate-2\", dict(use_gate=False, fixed_degree=2)))\n",
        "    variants.append((\"w/o Gate-3\", dict(use_gate=False, fixed_degree=3)))\n",
        "    variants.append((\"Gate w/o STE\", dict(use_gate=True, use_ste=False)))\n",
        "    variants.append((\"w/o Hadamard\", dict(use_hadamard=False)))\n",
        "    variants.append((\"w/o FFN\", dict(use_ffn=False)))\n",
        "    variants.append((\"w/o Residual\", dict(use_residual=False)))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for name, delta in variants:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[Running Variant] {name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        cfg = copy.deepcopy(base_cfg)\n",
        "        cfg.update(delta)\n",
        "\n",
        "        set_seed(train_cfg[\"seed\"])\n",
        "        model = build_variant(name, cfg)\n",
        "        print(f\"Model params: {count_parameters(model):,}\")\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            cfg[\"device\"],\n",
        "            lr=train_cfg[\"lr\"],\n",
        "            weight_decay=train_cfg[\"wd\"],\n",
        "            epochs=train_cfg[\"epochs\"],\n",
        "            seed=train_cfg[\"seed\"],\n",
        "        )\n",
        "\n",
        "        clean_f1, deg_ent, norm_comp = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=None, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "        snr10_f1, _, _ = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=10.0, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "\n",
        "        rows.append(format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH =  \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 5\n",
        "\n",
        "    NUM_CLASSES = 12\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE = 50\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = MHEALTHDataset(DATA_PATH, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    base_cfg = dict(\n",
        "        device=DEVICE,\n",
        "        in_channels=15,\n",
        "        seq_len=100,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "        dropout=DROPOUT,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        use_gate=True,\n",
        "        fixed_degree=None,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    )\n",
        "\n",
        "    train_cfg = dict(\n",
        "        seed=SEED,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        wd=WD,\n",
        "    )\n",
        "\n",
        "    rows = run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg)\n",
        "    print_table(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_0rZ4xMwC1R",
        "outputId": "bfdcf281-3a48-482d-caf3-2002566f17c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "Found 10 log files in /content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\n",
            "Loaded MHEALTH dataset\n",
            "X shape : (6862, 15, 100)  (N, C, T)\n",
            "y shape : (6862,)  (N,)\n",
            "Classes : 12\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Full (Ours)\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/5 | LR=0.0000 | Train Loss=0.1030 | TestF1=0.9762 | BestF1=0.9783 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9783\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-1\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/5 | LR=0.0000 | Train Loss=0.2083 | TestF1=0.9627 | BestF1=0.9627 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9627\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-2\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/5 | LR=0.0000 | Train Loss=0.1000 | TestF1=0.9797 | BestF1=0.9797 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9797\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-3\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/5 | LR=0.0000 | Train Loss=0.0921 | TestF1=0.9805 | BestF1=0.9805 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9805\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Gate w/o STE\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/5 | LR=0.0000 | Train Loss=0.1310 | TestF1=0.9783 | BestF1=0.9783 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9783\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Hadamard\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/5 | LR=0.0000 | Train Loss=0.1844 | TestF1=0.9680 | BestF1=0.9680 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9680\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o FFN\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/5 | LR=0.0000 | Train Loss=0.1230 | TestF1=0.9727 | BestF1=0.9727 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9727\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Residual\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/5 | LR=0.0000 | Train Loss=0.1311 | TestF1=0.9625 | BestF1=0.9671 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9671\n",
            "\n",
            "==============================================================================================================\n",
            "UCI-HAR Ablation Table\n",
            "==============================================================================================================\n",
            "Variant                | Clean F1 | (SNR=10) F1 | drop(%) | Degree Entropy | NormComp\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Full (Ours)            |   0.9783 |      0.9763 |    0.21 |         0.9862 |   0.6649\n",
            "w/o Gate-1             |   0.9627 |      0.9647 |   -0.22 |         0.0000 |   0.3333\n",
            "w/o Gate-2             |   0.9797 |      0.9797 |   -0.00 |         0.0000 |   0.6667\n",
            "w/o Gate-3             |   0.9805 |      0.9805 |   -0.00 |         0.0000 |   1.0000\n",
            "Gate w/o STE           |   0.9783 |      0.9777 |    0.06 |         0.8045 |   0.7329\n",
            "w/o Hadamard           |   0.9680 |      0.9643 |    0.38 |         0.9184 |   0.6688\n",
            "w/o FFN                |   0.9727 |      0.9715 |    0.13 |         0.9299 |   0.6634\n",
            "w/o Residual           |   0.9671 |      0.9671 |    0.00 |         0.9978 |   0.6662\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[LaTeX rows]\n",
            "Full (Ours) & 0.9783 & 0.9763 & 0.21 & 0.9862 & 0.6649 \\\\\n",
            "w/o Gate-1 & 0.9627 & 0.9647 & -0.22 & 0.0000 & 0.3333 \\\\\n",
            "w/o Gate-2 & 0.9797 & 0.9797 & -0.00 & 0.0000 & 0.6667 \\\\\n",
            "w/o Gate-3 & 0.9805 & 0.9805 & -0.00 & 0.0000 & 1.0000 \\\\\n",
            "Gate w/o STE & 0.9783 & 0.9777 & 0.06 & 0.8045 & 0.7329 \\\\\n",
            "w/o Hadamard & 0.9680 & 0.9643 & 0.38 & 0.9184 & 0.6688 \\\\\n",
            "w/o FFN & 0.9727 & 0.9715 & 0.13 & 0.9299 & 0.6634 \\\\\n",
            "w/o Residual & 0.9671 & 0.9671 & 0.00 & 0.9978 & 0.6662 \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WISDM"
      ],
      "metadata": {
        "id": "Ve_FfVAlwYC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class WISDMDataset(Dataset):\n",
        "    def __init__(self, file_path: str, window_size: int = 80, step_size: int = 40):\n",
        "        super().__init__()\n",
        "        self.file_path = file_path\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            raise FileNotFoundError(f\"WISDM txt file not found: {file_path}\")\n",
        "\n",
        "        df = self._load_file(file_path)\n",
        "        self.X, self.y, self.subjects = self._create_windows(df)\n",
        "        self.unique_subjects = sorted(np.unique(self.subjects))\n",
        "\n",
        "        self.n_classes = int(len(np.unique(self.y)))\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Loaded WISDM dataset (single txt)\")\n",
        "        print(f\"  X shape       : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"  y shape       : {self.y.shape}  (N,)\")\n",
        "        print(f\"  subjects shape: {self.subjects.shape} (N,)\")\n",
        "        print(f\"  num classes   : {self.n_classes}\")\n",
        "        print(f\"  unique subjects: {self.unique_subjects[:10]} ... (total {len(self.unique_subjects)})\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def _load_file(self, file_path: str) -> pd.DataFrame:\n",
        "        WISDM_LABEL_MAP = {\n",
        "            \"walking\": 0,\n",
        "            \"jogging\": 1,\n",
        "            \"sitting\": 2,\n",
        "            \"standing\": 3,\n",
        "            \"upstairs\": 4,\n",
        "            \"downstairs\": 5,\n",
        "        }\n",
        "\n",
        "        with open(file_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        rows = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            line = line.replace(\";\", \"\")\n",
        "            parts = line.split(\",\")\n",
        "\n",
        "            if len(parts) != 6:\n",
        "                continue\n",
        "\n",
        "            subj, act, ts, x, y, z = parts\n",
        "            if x.strip() == \"\" or y.strip() == \"\" or z.strip() == \"\":\n",
        "                continue\n",
        "\n",
        "            act_norm = act.strip().lower()\n",
        "            if act_norm not in WISDM_LABEL_MAP:\n",
        "                continue\n",
        "\n",
        "            rows.append([subj, act_norm, ts, x, y, z])\n",
        "\n",
        "        if not rows:\n",
        "            raise ValueError(f\"No valid rows parsed from file: {file_path}\")\n",
        "\n",
        "        df = pd.DataFrame(rows, columns=[\"subject\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"])\n",
        "        df = df.replace([\"\", \"NaN\", \"nan\"], np.nan).dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        df[\"subject\"] = pd.to_numeric(df[\"subject\"], errors=\"coerce\")\n",
        "        df[\"x\"] = pd.to_numeric(df[\"x\"], errors=\"coerce\")\n",
        "        df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\")\n",
        "        df[\"z\"] = pd.to_numeric(df[\"z\"], errors=\"coerce\")\n",
        "        df = df.dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "        if df.empty:\n",
        "            raise ValueError(\"After cleaning, WISDM DataFrame is empty. Check file format.\")\n",
        "\n",
        "        df[\"subject\"] = df[\"subject\"].astype(int)\n",
        "        df[\"activity_id\"] = df[\"activity\"].map(WISDM_LABEL_MAP).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_windows(self, df: pd.DataFrame):\n",
        "        X_list, y_list, s_list = [], [], []\n",
        "\n",
        "        for subj_id in sorted(df[\"subject\"].unique()):\n",
        "            df_sub = df[df[\"subject\"] == subj_id]\n",
        "            data = df_sub[[\"x\", \"y\", \"z\"]].to_numpy(dtype=np.float32)\n",
        "            labels = df_sub[\"activity_id\"].to_numpy(dtype=np.int64)\n",
        "            L = len(df_sub)\n",
        "\n",
        "            start = 0\n",
        "            while start + self.window_size <= L:\n",
        "                end = start + self.window_size\n",
        "                window_x = data[start:end]\n",
        "                window_y = labels[end - 1]\n",
        "\n",
        "                X_list.append(window_x.T)\n",
        "                y_list.append(window_y)\n",
        "                s_list.append(subj_id)\n",
        "\n",
        "                start += self.step_size\n",
        "\n",
        "        if len(X_list) == 0:\n",
        "            raise ValueError(\"[WISDMDataset] No windows created. Try smaller window_size or check data.\")\n",
        "\n",
        "        X = np.stack(X_list, axis=0).astype(np.float32)\n",
        "        y = np.array(y_list, dtype=np.int64)\n",
        "        s = np.array(s_list, dtype=np.int64)\n",
        "        return X, y, s\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]  # (N,C,T)\n",
        "        N, C, T = Xtr.shape\n",
        "        X2 = np.transpose(Xtr, (0, 2, 1)).reshape(-1, C)  # (N*T, C)\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = getattr(self, \"scaler\", None)\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = np.transpose(X, (0, 2, 1)).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        self.X = X2.reshape(N, T, C).transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.FloatTensor(self.X[idx]),\n",
        "            torch.LongTensor([self.y[idx]])[0],\n",
        "            int(self.subjects[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions (SNR=10 uses this)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate  (★ Variant behavior matches \"예전버전\")\n",
        "#   - use_ste=True  : train=STE(hard fwd, soft bwd), eval=hard onehot\n",
        "#   - use_ste=False : always soft_probs (train/eval 동일)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits = self.gate(x)  # (B,K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx = logits.argmax(dim=-1)\n",
        "                hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                # STE: forward=hard, backward=soft\n",
        "                degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(\n",
        "                    logits.argmax(dim=-1), num_classes=self.max_degree\n",
        "                ).float()\n",
        "        else:\n",
        "            # Gate w/o STE: always soft (train/eval 동일)\n",
        "            degree_w = soft_probs\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block (Ablation switches ONLY; ★ Variant behavior matches \"예전버전\")\n",
        "#   - w/o Gate: fixed_degree (1..K) → build up to d, output Z[d-1]\n",
        "#   - Gate w/o STE: soft weighted sum of ALL degrees (train/eval 동일)\n",
        "#   - w/o Hadamard: prefix sum (Z[i]=Z[i-1]+Y[i])  (예전버전)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlockAblation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,      # 1..K if w/o Gate (fixed)\n",
        "                 use_ste=True,           # Gate w/o STE (soft routing)\n",
        "                 use_hadamard=True,      # w/o Hadamard (prefix-sum)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_gate = bool(use_gate)\n",
        "        self.fixed_degree = fixed_degree  # None or int in [1..K]\n",
        "        self.use_ste = bool(use_ste)\n",
        "        self.use_hadamard = bool(use_hadamard)\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _build_Y(self, x, max_deg):\n",
        "        return [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        \"\"\"\n",
        "        - use_hadamard=True : 예전버전(원본) Hadamard chain\n",
        "            Z0=Y0, Zi = pre(Z_{i-1}) * Yi\n",
        "        - use_hadamard=False: 예전버전 w/o Hadamard (prefix sum)\n",
        "            Z0=Y0, Zi = Z_{i-1} + Yi\n",
        "        \"\"\"\n",
        "        Y = self._build_Y(x, max_deg)\n",
        "\n",
        "        if self.use_hadamard:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(Z_ * Y[i])\n",
        "            return Z\n",
        "        else:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "            return Z\n",
        "\n",
        "    def _hard_select(self, Z_list, sel):\n",
        "        B = Z_list[0].shape[0]\n",
        "        Z_stack = torch.stack(Z_list, dim=0)  # (K,B,C,T)\n",
        "        return Z_stack[sel, torch.arange(B, device=Z_stack.device)]\n",
        "\n",
        "    def _soft_weighted_output(self, x, soft_probs):\n",
        "        \"\"\"\n",
        "        예전버전 Gate w/o STE:\n",
        "          - always compute ALL K degrees\n",
        "          - weighted sum with soft_probs\n",
        "          - hadamard / no_hadamard build rule은 동일하게 적용\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        Z = self._build_Z(x, max_deg=self.max_degree)      # list length K, each (B,C,T)\n",
        "        Z_stack = torch.stack(Z, dim=1)                    # (B,K,C,T)\n",
        "        w = soft_probs.view(B, self.max_degree, 1, 1)      # (B,K,1,1)\n",
        "        out = (Z_stack * w).sum(dim=1)                     # (B,C,T)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # ---- Case A) w/o Gate (fixed degree 1..K) ----\n",
        "        if (not self.use_gate) or (self.fixed_degree is not None):\n",
        "            d = int(self.fixed_degree) if self.fixed_degree is not None else self.max_degree\n",
        "            d = max(1, min(d, self.max_degree))\n",
        "\n",
        "            # build only up to d, output \"degree d\" path (예전버전)\n",
        "            Z = self._build_Z(x, max_deg=d)\n",
        "            out = Z[-1]\n",
        "\n",
        "            # stats payload (예전버전 스타일)\n",
        "            sel = torch.full((B,), d - 1, device=x.device, dtype=torch.long)\n",
        "            K = self.max_degree\n",
        "            sp = F.one_hot(sel, num_classes=K).float()\n",
        "            dw = sp\n",
        "            logits = sp\n",
        "\n",
        "            out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            if return_gate_info:\n",
        "                return out, {\n",
        "                    \"degree_selection\": dw,\n",
        "                    \"soft_probs\": sp,\n",
        "                    \"logits\": logits,\n",
        "                    \"compute_cost\": float(d),\n",
        "                }\n",
        "            return out\n",
        "\n",
        "        # ---- Case B) Gate ON ----\n",
        "        degree_w, logits, soft_probs = self.degree_gate(x, use_ste=self.use_ste)\n",
        "\n",
        "        if (not self.use_ste):\n",
        "            # 예전버전: Gate w/o STE는 train/eval 관계없이 ALWAYS soft weighted sum\n",
        "            out = self._soft_weighted_output(x, degree_w)  # degree_w == soft_probs\n",
        "            # (stats only) 대표 degree\n",
        "            selected = soft_probs.argmax(dim=-1)\n",
        "            # compute_cost (예전코드에선 argmax 기반이었지만, 여기서는 gi에만 들어가므로 그대로 둠)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "        else:\n",
        "            # 원본: hard select (STE는 train에서 degree_w에 반영됨)\n",
        "            selected = degree_w.argmax(dim=-1)\n",
        "            max_deg = max(1, min(int(selected.max().item()) + 1, self.max_degree))\n",
        "            Z = self._build_Z(x, max_deg=max_deg)\n",
        "            out = self._hard_select(Z, selected)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_w,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": compute_cost,\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model (Ablation switches ONLY; otherwise matches your logic)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR_Ablation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,     # for w/o Gate (1/2/3)\n",
        "                 use_ste=True,          # Gate w/o STE\n",
        "                 use_hadamard=True,     # w/o Hadamard\n",
        "                 use_ffn=True,          # w/o FFN\n",
        "                 use_residual=True,     # w/o Residual\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_ffn = bool(use_ffn)\n",
        "        self.use_residual = bool(use_residual)\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlockAblation(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                token_kernel=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "                use_gate=use_gate,\n",
        "                fixed_degree=fixed_degree,\n",
        "                use_ste=use_ste,\n",
        "                use_hadamard=use_hadamard,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms1[i], x + res)\n",
        "            else:\n",
        "                x = self._ln(self.norms1[i], x)\n",
        "\n",
        "            res2 = x\n",
        "            if self.use_ffn:\n",
        "                x = self.ffn[i](x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms2[i], x + res2)\n",
        "            else:\n",
        "                x = self._ln(self.norms2[i], x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Table Metrics (keep as-is in your \"now code\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_gate_stats(model, loader, device, snr_db=None, max_degree=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      macro_f1, degree_entropy, norm_comp\n",
        "    Definitions:\n",
        "      - degree_entropy: mean over (layers, samples) of normalized entropy of soft_probs\n",
        "                        H(p)/log(K)  where K=max_degree\n",
        "      - norm_comp: mean expected degree / max_degree, averaged over layers\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    ent_sum = 0.0\n",
        "    ent_count = 0\n",
        "    comp_sum = 0.0\n",
        "    comp_count = 0\n",
        "\n",
        "    eps = 1e-12\n",
        "    logK = float(np.log(max_degree))\n",
        "\n",
        "    deg_vals = torch.arange(1, max_degree + 1, device=device).float()\n",
        "\n",
        "    for X, y, _ in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "        # gate stats\n",
        "        for gi in gate_info_list:\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            ent = -(sp * (sp + eps).log()).sum(dim=-1) / logK\n",
        "            ent_sum += ent.mean().item()\n",
        "            ent_count += 1\n",
        "\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "            comp_sum += (exp_deg / max_degree)\n",
        "            comp_count += 1\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    macro_f1 = float(f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    degree_entropy = float(ent_sum / max(ent_count, 1))\n",
        "    norm_comp = float(comp_sum / max(comp_count, 1))\n",
        "    return macro_f1, degree_entropy, norm_comp\n",
        "\n",
        "\n",
        "def format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp):\n",
        "    drop_pct = 100.0 * (clean_f1 - snr10_f1) / max(clean_f1, 1e-12)\n",
        "    return {\n",
        "        \"Variant\": name,\n",
        "        \"CleanF1\": clean_f1,\n",
        "        \"SNR10F1\": snr10_f1,\n",
        "        \"drop(%)\": drop_pct,\n",
        "        \"DegreeEntropy\": deg_ent,\n",
        "        \"NormComp\": norm_comp,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    header = [\"Variant\", \"Clean F1\", \"(SNR=10) F1\", \"drop(%)\", \"Degree Entropy\", \"NormComp\"]\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"UCI-HAR Ablation Table\")\n",
        "    print(\"=\"*110)\n",
        "    print(f\"{header[0]:<22s} | {header[1]:>8s} | {header[2]:>11s} | {header[3]:>7s} | {header[4]:>14s} | {header[5]:>8s}\")\n",
        "    print(\"-\"*110)\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']:<22s} | \"\n",
        "            f\"{r['CleanF1']:>8.4f} | \"\n",
        "            f\"{r['SNR10F1']:>11.4f} | \"\n",
        "            f\"{r['drop(%)']:>7.2f} | \"\n",
        "            f\"{r['DegreeEntropy']:>14.4f} | \"\n",
        "            f\"{r['NormComp']:>8.4f}\"\n",
        "        )\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    print(\"\\n[LaTeX rows]\")\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']} & {r['CleanF1']:.4f} & {r['SNR10F1']:.4f} & \"\n",
        "            f\"{r['drop(%)']:.2f} & {r['DegreeEntropy']:.4f} & {r['NormComp']:.4f} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Experiment Runner (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def build_variant(name, cfg):\n",
        "    model = PADReHAR_Ablation(\n",
        "        in_channels=cfg[\"in_channels\"],\n",
        "        seq_len=cfg[\"seq_len\"],\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_layers=cfg[\"num_layers\"],\n",
        "        max_degree=cfg[\"max_degree\"],\n",
        "        gate_hidden_dim=cfg[\"gate_hidden_dim\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        temperature_initial=cfg[\"temperature_initial\"],\n",
        "        temperature_min=cfg[\"temperature_min\"],\n",
        "        use_gate=cfg.get(\"use_gate\", True),\n",
        "        fixed_degree=cfg.get(\"fixed_degree\", None),\n",
        "        use_ste=cfg.get(\"use_ste\", True),\n",
        "        use_hadamard=cfg.get(\"use_hadamard\", True),\n",
        "        use_ffn=cfg.get(\"use_ffn\", True),\n",
        "        use_residual=cfg.get(\"use_residual\", True),\n",
        "    ).to(cfg[\"device\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg):\n",
        "    variants = []\n",
        "    variants.append((\"Full (Ours)\", dict()))\n",
        "    variants.append((\"w/o Gate-1\", dict(use_gate=False, fixed_degree=1)))\n",
        "    variants.append((\"w/o Gate-2\", dict(use_gate=False, fixed_degree=2)))\n",
        "    variants.append((\"w/o Gate-3\", dict(use_gate=False, fixed_degree=3)))\n",
        "    variants.append((\"Gate w/o STE\", dict(use_gate=True, use_ste=False)))\n",
        "    variants.append((\"w/o Hadamard\", dict(use_hadamard=False)))\n",
        "    variants.append((\"w/o FFN\", dict(use_ffn=False)))\n",
        "    variants.append((\"w/o Residual\", dict(use_residual=False)))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for name, delta in variants:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[Running Variant] {name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        cfg = copy.deepcopy(base_cfg)\n",
        "        cfg.update(delta)\n",
        "\n",
        "        set_seed(train_cfg[\"seed\"])\n",
        "        model = build_variant(name, cfg)\n",
        "        print(f\"Model params: {count_parameters(model):,}\")\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            cfg[\"device\"],\n",
        "            lr=train_cfg[\"lr\"],\n",
        "            weight_decay=train_cfg[\"wd\"],\n",
        "            epochs=train_cfg[\"epochs\"],\n",
        "            seed=train_cfg[\"seed\"],\n",
        "        )\n",
        "\n",
        "        clean_f1, deg_ent, norm_comp = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=None, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "        snr10_f1, _, _ = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=10.0, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "\n",
        "        rows.append(format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH =  \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/WISDM_ar_v1.1_raw.txt\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 20\n",
        "\n",
        "    NUM_CLASSES = 6\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 80\n",
        "    STEP_SIZE = 40\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = WISDMDataset(DATA_PATH, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    base_cfg = dict(\n",
        "        device=DEVICE,\n",
        "        in_channels=3,\n",
        "        seq_len=80,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "        dropout=DROPOUT,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        use_gate=True,\n",
        "        fixed_degree=None,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    )\n",
        "\n",
        "    train_cfg = dict(\n",
        "        seed=SEED,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        wd=WD,\n",
        "    )\n",
        "\n",
        "    rows = run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg)\n",
        "    print_table(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "494zVv0Ywam_",
        "outputId": "2407450d-fb6f-4ae3-bfe3-5725ba58b677"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "================================================================================\n",
            "Loaded WISDM dataset (single txt)\n",
            "  X shape       : (27108, 3, 80)  (N, C, T)\n",
            "  y shape       : (27108,)  (N,)\n",
            "  subjects shape: (27108,) (N,)\n",
            "  num classes   : 6\n",
            "  unique subjects: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)] ... (total 36)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Full (Ours)\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0009 | Train Loss=0.0658 | TestF1=0.9611 | BestF1=0.9641 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0005 | Train Loss=0.0277 | TestF1=0.9729 | BestF1=0.9729 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0002 | Train Loss=0.0077 | TestF1=0.9771 | BestF1=0.9771 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0044 | TestF1=0.9776 | BestF1=0.9793 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9793\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-1\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0009 | Train Loss=0.1321 | TestF1=0.9340 | BestF1=0.9367 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0005 | Train Loss=0.0825 | TestF1=0.9632 | BestF1=0.9632 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0002 | Train Loss=0.0585 | TestF1=0.9664 | BestF1=0.9689 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0469 | TestF1=0.9706 | BestF1=0.9709 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9709\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-2\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0009 | Train Loss=0.0668 | TestF1=0.9602 | BestF1=0.9654 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0005 | Train Loss=0.0266 | TestF1=0.9788 | BestF1=0.9788 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0002 | Train Loss=0.0083 | TestF1=0.9782 | BestF1=0.9788 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0044 | TestF1=0.9802 | BestF1=0.9811 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9811\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-3\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0009 | Train Loss=0.0593 | TestF1=0.9659 | BestF1=0.9659 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0005 | Train Loss=0.0152 | TestF1=0.9743 | BestF1=0.9743 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0002 | Train Loss=0.0041 | TestF1=0.9781 | BestF1=0.9784 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0023 | TestF1=0.9790 | BestF1=0.9801 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9801\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Gate w/o STE\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0009 | Train Loss=0.0615 | TestF1=0.9713 | BestF1=0.9713 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0005 | Train Loss=0.0219 | TestF1=0.9768 | BestF1=0.9768 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0002 | Train Loss=0.0070 | TestF1=0.9769 | BestF1=0.9777 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0043 | TestF1=0.9770 | BestF1=0.9777 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9777\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Hadamard\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0009 | Train Loss=0.1230 | TestF1=0.9455 | BestF1=0.9469 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0005 | Train Loss=0.0803 | TestF1=0.9626 | BestF1=0.9626 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0002 | Train Loss=0.0531 | TestF1=0.9677 | BestF1=0.9677 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0434 | TestF1=0.9707 | BestF1=0.9707 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9707\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o FFN\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0009 | Train Loss=0.0630 | TestF1=0.9645 | BestF1=0.9658 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0005 | Train Loss=0.0196 | TestF1=0.9681 | BestF1=0.9755 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0002 | Train Loss=0.0060 | TestF1=0.9746 | BestF1=0.9755 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0040 | TestF1=0.9755 | BestF1=0.9764 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9764\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Residual\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0009 | Train Loss=0.1130 | TestF1=0.9452 | BestF1=0.9452 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0005 | Train Loss=0.0641 | TestF1=0.9641 | BestF1=0.9641 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0002 | Train Loss=0.0368 | TestF1=0.9701 | BestF1=0.9701 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0271 | TestF1=0.9701 | BestF1=0.9715 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9715\n",
            "\n",
            "==============================================================================================================\n",
            "UCI-HAR Ablation Table\n",
            "==============================================================================================================\n",
            "Variant                | Clean F1 | (SNR=10) F1 | drop(%) | Degree Entropy | NormComp\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Full (Ours)            |   0.9793 |      0.9161 |    6.46 |         0.9495 |   0.6571\n",
            "w/o Gate-1             |   0.9709 |      0.9381 |    3.38 |         0.0000 |   0.3333\n",
            "w/o Gate-2             |   0.9811 |      0.7954 |   18.92 |         0.0000 |   0.6667\n",
            "w/o Gate-3             |   0.9801 |      0.8603 |   12.22 |         0.0000 |   1.0000\n",
            "Gate w/o STE           |   0.9777 |      0.9593 |    1.89 |         0.8166 |   0.7498\n",
            "w/o Hadamard           |   0.9707 |      0.9437 |    2.78 |         0.9169 |   0.6595\n",
            "w/o FFN                |   0.9764 |      0.8984 |    8.00 |         0.9688 |   0.6548\n",
            "w/o Residual           |   0.9715 |      0.9599 |    1.19 |         0.9197 |   0.6581\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[LaTeX rows]\n",
            "Full (Ours) & 0.9793 & 0.9161 & 6.46 & 0.9495 & 0.6571 \\\\\n",
            "w/o Gate-1 & 0.9709 & 0.9381 & 3.38 & 0.0000 & 0.3333 \\\\\n",
            "w/o Gate-2 & 0.9811 & 0.7954 & 18.92 & 0.0000 & 0.6667 \\\\\n",
            "w/o Gate-3 & 0.9801 & 0.8603 & 12.22 & 0.0000 & 1.0000 \\\\\n",
            "Gate w/o STE & 0.9777 & 0.9593 & 1.89 & 0.8166 & 0.7498 \\\\\n",
            "w/o Hadamard & 0.9707 & 0.9437 & 2.78 & 0.9169 & 0.6595 \\\\\n",
            "w/o FFN & 0.9764 & 0.8984 & 8.00 & 0.9688 & 0.6548 \\\\\n",
            "w/o Residual & 0.9715 & 0.9599 & 1.19 & 0.9197 & 0.6581 \\\\\n"
          ]
        }
      ]
    }
  ]
}