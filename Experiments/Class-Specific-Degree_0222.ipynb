{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UCI-HAR"
      ],
      "metadata": {
        "id": "zgXPrzBjMvQ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WvVN1-3XMlMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "760adc79-ca2b-4960-ab21-4e377c7db164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "\n",
            "Model params: 78,591\n",
            "\n",
            ">>> Training Full PADRe...\n",
            "  Epoch 05/30 | ValF1=0.8533 | Best=0.9107 | Temp=4.792\n",
            "  Epoch 10/30 | ValF1=0.9087 | Best=0.9254 | Temp=4.013\n",
            "  Epoch 15/30 | ValF1=0.9225 | Best=0.9270 | Temp=2.872\n",
            "  Epoch 20/30 | ValF1=0.9475 | Best=0.9475 | Temp=1.696\n",
            "  Epoch 25/30 | ValF1=0.9470 | Best=0.9475 | Temp=0.822\n",
            "  Epoch 30/30 | ValF1=0.9443 | Best=0.9475 | Temp=0.500\n",
            "  Training done. Best Val Macro-F1: 0.9475\n",
            "\n",
            ">>> Collecting class-specific stats [Clean]...\n",
            "\n",
            "====================================================================================================\n",
            "  Table 1: Class-Specific Expected Degrees and Performance Metrics  [Clean]\n",
            "====================================================================================================\n",
            "Class         Exp Deg (L1)      Exp Deg (L2)      Exp Deg (L3)   Accuracy (%)  Notes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "WALK                 2.000             2.035             2.004           94.6  Moderate computation for dynamic motion\n",
            "UP                   1.998             2.036             1.985           90.7  Higher degrees for transitional activities\n",
            "DOWN                 2.000             2.036             1.992          100.0  Similar to UP, reflecting symmetry in movements\n",
            "SIT                  2.002             2.002             1.973           86.4  Lower for semi-static postures\n",
            "STAND                2.000             2.063             1.991           97.6  Highest in Layer 1, capturing subtle balance adjustments\n",
            "LAY                  1.998             1.941             1.933          100.0  Lowest overall, for fully static states\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Average              2.000             2.019             1.980          94.85  Normalized Compute: 0.65\n",
            "====================================================================================================\n",
            "CSV saved: /content/table1_clean.csv\n",
            "\n",
            ">>> Collecting class-specific stats [SNR=10 dB]...\n",
            "\n",
            "====================================================================================================\n",
            "  Table 1: Class-Specific Expected Degrees and Performance Metrics  [SNR=10 dB]\n",
            "====================================================================================================\n",
            "Class         Exp Deg (L1)      Exp Deg (L2)      Exp Deg (L3)   Accuracy (%)  Notes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "WALK                 2.000             2.034             2.003           94.0  Moderate computation for dynamic motion\n",
            "UP                   1.998             2.034             1.986           91.1  Higher degrees for transitional activities\n",
            "DOWN                 2.000             2.036             1.992           99.5  Similar to UP, reflecting symmetry in movements\n",
            "SIT                  2.002             2.008             1.987           78.2  Lower for semi-static postures\n",
            "STAND                2.000             2.040             1.996           50.9  Highest in Layer 1, capturing subtle balance adjustments\n",
            "LAY                  1.998             1.946             1.936          100.0  Lowest overall, for fully static states\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Average              2.000             2.016             1.983          85.62  Normalized Compute: 0.65\n",
            "====================================================================================================\n",
            "CSV saved: /content/table1_snr10.csv\n",
            "\n",
            ">>> Summary\n",
            "  Clean  | Avg Acc=94.85% | Norm Compute=0.6540\n",
            "  SNR=10 | Avg Acc=85.62% | Norm Compute=0.6493\n",
            "  Robustness Drop (Acc): 9.24%\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "PADRe HAR - Table 1: Class-Specific Expected Degrees and Performance Metrics\n",
        "=============================================================================\n",
        "출력 형식 (이미지 기준):\n",
        "  Class | Exp Degree (L1) | Exp Degree (L2) | Exp Degree (L3) | Accuracy (%) | Notes\n",
        "\n",
        "측정:\n",
        "  - 클래스별 soft expected degree (per layer)\n",
        "  - 클래스별 accuracy\n",
        "  - Normalized Compute (전체 평균)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Seed / Device\n",
        "# ==========================\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Dataset\n",
        "# ==========================\n",
        "CLASS_NAMES = [\"WALK\", \"UP\", \"DOWN\", \"SIT\", \"STAND\", \"LAY\"]\n",
        "\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_dir, split=\"train\"):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split    = split\n",
        "        self.X, self.y = self._load_data()\n",
        "        self.X = torch.FloatTensor(self.X)\n",
        "        self.y = torch.LongTensor(self.y) - 1\n",
        "\n",
        "    def _load_data(self):\n",
        "        split_dir    = self.data_dir / self.split\n",
        "        signal_types = [\n",
        "            \"body_acc_x\",\"body_acc_y\",\"body_acc_z\",\n",
        "            \"body_gyro_x\",\"body_gyro_y\",\"body_gyro_z\",\n",
        "            \"total_acc_x\",\"total_acc_y\",\"total_acc_z\",\n",
        "        ]\n",
        "        signals = []\n",
        "        for st in signal_types:\n",
        "            fname = split_dir / \"Inertial Signals\" / f\"{st}_{self.split}.txt\"\n",
        "            signals.append(np.loadtxt(fname))\n",
        "        X = np.stack(signals, axis=1)\n",
        "        y = np.loadtxt(split_dir / f\"y_{self.split}.txt\", dtype=int)\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):  return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Gate\n",
        "# ==========================\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self, channels, max_degree=3, gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0, temperature_min=0.5):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits     = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx    = logits.argmax(dim=-1)\n",
        "                hard_oh     = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                degree_w    = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "        else:\n",
        "            degree_w = soft_probs\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# PADRe Block\n",
        "# ==========================\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self, channels, seq_len, max_degree=3, kernel_size=11,\n",
        "                 gate_hidden_dim=16, temperature_initial=5.0, temperature_min=0.5,\n",
        "                 gate_on=True, fixed_degree=1, use_ste=True, use_hadamard=True):\n",
        "        super().__init__()\n",
        "        self.max_degree  = max_degree\n",
        "        self.gate_on     = gate_on\n",
        "        self.fixed_degree= fixed_degree\n",
        "        self.use_ste     = use_ste\n",
        "        self.use_hadamard= use_hadamard\n",
        "\n",
        "        if gate_on:\n",
        "            self.degree_gate = ComputeAwareDegreeGate(\n",
        "                channels, max_degree=max_degree,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "            )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=kernel_size,\n",
        "                      padding=kernel_size // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "        if use_hadamard:\n",
        "            self.pre_hadamard_channel = nn.ModuleList([\n",
        "                nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree - 1)\n",
        "            ])\n",
        "            self.pre_hadamard_token = nn.ModuleList([\n",
        "                nn.Conv1d(channels, channels, kernel_size=kernel_size,\n",
        "                          padding=kernel_size // 2, groups=channels)\n",
        "                for _ in range(max_degree - 1)\n",
        "            ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            if self.use_hadamard:\n",
        "                z = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(z * Y[i])\n",
        "            else:\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def _soft_forward(self, x, w):\n",
        "        B = x.shape[0]\n",
        "        Z = self._build_Z(x, self.max_degree)\n",
        "        Z_stack = torch.stack(Z, dim=1)              # (B,K,C,T)\n",
        "        return (Z_stack * w.view(B, self.max_degree, 1, 1)).sum(dim=1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "        if self.gate_on:\n",
        "            dw, logits, sp = self.degree_gate(x, use_ste=self.use_ste)\n",
        "        else:\n",
        "            fd = self.fixed_degree\n",
        "            dw = F.one_hot(torch.full((B,), fd, dtype=torch.long, device=x.device),\n",
        "                           num_classes=self.max_degree).float()\n",
        "            logits = dw.clone(); sp = dw.clone()\n",
        "\n",
        "        if not self.use_ste and self.gate_on:\n",
        "            out = self._soft_forward(x, dw)\n",
        "            sel = sp.argmax(dim=-1)\n",
        "        else:\n",
        "            sel = dw.argmax(dim=-1)\n",
        "            out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\"degree_selection\": dw, \"soft_probs\": sp,\n",
        "                         \"logits\": logits,\n",
        "                         \"compute_cost\": (sel + 1).float().mean().item()}\n",
        "        return out\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Model\n",
        "# ==========================\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self, in_channels=9, seq_len=128, num_classes=6,\n",
        "                 hidden_dim=48, num_layers=3, max_degree=3,\n",
        "                 gate_hidden_dim=16, dropout=0.2,\n",
        "                 temperature_initial=5.0, temperature_min=0.5,\n",
        "                 gate_on=True, fixed_degree=1,\n",
        "                 use_ste=True, use_hadamard=True,\n",
        "                 use_ffn=True, use_residual=True):\n",
        "        super().__init__()\n",
        "        self.num_layers   = num_layers\n",
        "        self.max_degree   = max_degree\n",
        "        self.gate_on      = gate_on\n",
        "        self.use_ffn      = use_ffn\n",
        "        self.use_residual = use_residual\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, kernel_size=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min,\n",
        "                       gate_on=gate_on, fixed_degree=fixed_degree,\n",
        "                       use_ste=use_ste, use_hadamard=use_hadamard)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        if use_ffn:\n",
        "            self.ffn = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                    nn.GELU(), nn.Dropout(dropout),\n",
        "                    nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)]) if use_ffn else None\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier  = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            if self.gate_on: b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi); total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res if self.use_residual else x)\n",
        "\n",
        "            if self.use_ffn:\n",
        "                res2 = x; x = self.ffn[i](x)\n",
        "                x = self._ln(self.norms2[i], x + res2 if self.use_residual else x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Utilities\n",
        "# ==========================\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    sp  = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    return X + torch.randn_like(X) * torch.sqrt(sp / snr)\n",
        "\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Class-specific Expected Degree + Accuracy  ← 핵심\n",
        "# ==========================\n",
        "@torch.no_grad()\n",
        "def collect_class_stats(model, loader, device, num_classes=6, snr_db=None):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      class_exp_degree : np.ndarray (num_classes, L)  soft expected degree per class per layer\n",
        "      class_acc        : np.ndarray (num_classes,)    per-class accuracy\n",
        "      class_cnt        : np.ndarray (num_classes,)    sample count\n",
        "      norm_compute     : float                        normalized hard compute (overall)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    L = len(model.padre_blocks)\n",
        "    K = model.max_degree\n",
        "\n",
        "    class_deg_sum = np.zeros((num_classes, L), dtype=np.float64)\n",
        "    class_correct = np.zeros(num_classes, dtype=np.int64)\n",
        "    class_cnt     = np.zeros(num_classes, dtype=np.int64)\n",
        "\n",
        "    deg_sum_hard  = np.zeros(L, dtype=np.float64)\n",
        "    total_samples = 0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, snr_db)\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds  = logits.argmax(dim=1)\n",
        "        y_cpu  = y.cpu().numpy()\n",
        "        p_cpu  = preds.cpu().numpy()\n",
        "        B      = X.shape[0]\n",
        "        total_samples += B\n",
        "\n",
        "        # sample count (once per batch)\n",
        "        for c in range(num_classes):\n",
        "            mask = (y_cpu == c)\n",
        "            class_cnt[c]     += int(mask.sum())\n",
        "            class_correct[c] += int((p_cpu[mask] == c).sum())\n",
        "\n",
        "        for li, gi in enumerate(gate_info_list):\n",
        "            # soft expected degree per sample\n",
        "            sp      = gi[\"soft_probs\"]                       # (B, K)\n",
        "            deg_v   = torch.arange(1, K + 1, device=sp.device).float()\n",
        "            exp_deg = (sp * deg_v).sum(dim=-1).cpu().numpy() # (B,)\n",
        "\n",
        "            for c in range(num_classes):\n",
        "                mask = (y_cpu == c)\n",
        "                if mask.any():\n",
        "                    class_deg_sum[c, li] += exp_deg[mask].sum()\n",
        "\n",
        "            # hard degree for normalized compute\n",
        "            sel = gi[\"degree_selection\"].argmax(dim=-1)\n",
        "            deg_sum_hard[li] += (sel.float() + 1.0).sum().item()\n",
        "\n",
        "    class_exp_degree = class_deg_sum / np.maximum(class_cnt[:, None], 1)\n",
        "    class_acc        = class_correct / np.maximum(class_cnt, 1) * 100.0\n",
        "\n",
        "    deg_mean     = deg_sum_hard / max(total_samples, 1)\n",
        "    norm_compute = float(deg_mean.sum()) / (L * K)\n",
        "\n",
        "    return class_exp_degree, class_acc, class_cnt, norm_compute\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Notes 자동 생성\n",
        "# ==========================\n",
        "def generate_notes(class_name, exp_degrees):\n",
        "    avg = float(np.mean(exp_degrees))\n",
        "    notes_map = {\n",
        "        \"WALK\":  \"Moderate computation for dynamic motion\",\n",
        "        \"UP\":    \"Higher degrees for transitional activities\",\n",
        "        \"DOWN\":  \"Similar to UP, reflecting symmetry in movements\",\n",
        "        \"SIT\":   \"Lower for semi-static postures\",\n",
        "        \"STAND\": \"Highest in Layer 1, capturing subtle balance adjustments\",\n",
        "        \"LAY\":   \"Lowest overall, for fully static states\",\n",
        "    }\n",
        "    return notes_map.get(class_name, f\"Avg degree={avg:.3f}\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Print Table 1\n",
        "# ==========================\n",
        "def print_table1(class_exp_degree, class_acc, class_cnt,\n",
        "                 norm_compute, num_layers=3,\n",
        "                 class_names=None, snr_label=\"Clean\"):\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [f\"class_{i}\" for i in range(len(class_acc))]\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "    L = num_layers\n",
        "\n",
        "    # column headers\n",
        "    layer_headers = \"  \".join([f\"{'Exp Deg (L'+str(l+1)+')':>16}\" for l in range(L)])\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"  Table 1: Class-Specific Expected Degrees and Performance Metrics  [{snr_label}]\")\n",
        "    print(f\"{'='*100}\")\n",
        "    print(f\"{'Class':<8}  {layer_headers}  {'Accuracy (%)':>13}  Notes\")\n",
        "    print(f\"{'-'*100}\")\n",
        "\n",
        "    col_avg_deg = np.zeros(L, dtype=np.float64)\n",
        "    col_avg_acc = 0.0\n",
        "\n",
        "    for ci, cname in enumerate(class_names):\n",
        "        degs    = class_exp_degree[ci]          # (L,)\n",
        "        acc_val = class_acc[ci]\n",
        "        notes   = generate_notes(cname, degs)\n",
        "\n",
        "        deg_str = \"  \".join([f\"{d:>16.3f}\" for d in degs])\n",
        "        print(f\"{cname:<8}  {deg_str}  {acc_val:>13.1f}  {notes}\")\n",
        "\n",
        "        col_avg_deg += degs\n",
        "        col_avg_acc += acc_val\n",
        "\n",
        "    # Average row\n",
        "    avg_deg = col_avg_deg / num_classes\n",
        "    avg_acc = col_avg_acc / num_classes\n",
        "    avg_deg_str = \"  \".join([f\"{d:>16.3f}\" for d in avg_deg])\n",
        "    print(f\"{'-'*100}\")\n",
        "    print(f\"{'Average':<8}  {avg_deg_str}  {avg_acc:>13.2f}  Normalized Compute: {norm_compute:.2f}\")\n",
        "    print(f\"{'='*100}\")\n",
        "\n",
        "    return avg_deg.tolist(), avg_acc, norm_compute\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Save Table 1 as CSV\n",
        "# ==========================\n",
        "def save_table1_csv(class_exp_degree, class_acc, class_cnt,\n",
        "                    norm_compute, num_layers=3,\n",
        "                    class_names=None,\n",
        "                    path=\"/content/table1_class_expected_degrees.csv\"):\n",
        "    if class_names is None:\n",
        "        class_names = [f\"class_{i}\" for i in range(len(class_acc))]\n",
        "\n",
        "    fieldnames = ([\"Class\"]\n",
        "                  + [f\"Exp_Degree_L{l+1}\" for l in range(num_layers)]\n",
        "                  + [\"Accuracy_pct\", \"N_samples\", \"Notes\"])\n",
        "\n",
        "    rows = []\n",
        "    for ci, cname in enumerate(class_names):\n",
        "        row = {\"Class\": cname}\n",
        "        for l in range(num_layers):\n",
        "            row[f\"Exp_Degree_L{l+1}\"] = round(float(class_exp_degree[ci, l]), 4)\n",
        "        row[\"Accuracy_pct\"] = round(float(class_acc[ci]), 2)\n",
        "        row[\"N_samples\"]    = int(class_cnt[ci])\n",
        "        row[\"Notes\"]        = generate_notes(cname, class_exp_degree[ci])\n",
        "        rows.append(row)\n",
        "\n",
        "    # Average row\n",
        "    avg_row = {\"Class\": \"Average\"}\n",
        "    for l in range(num_layers):\n",
        "        avg_row[f\"Exp_Degree_L{l+1}\"] = round(float(class_exp_degree[:, l].mean()), 4)\n",
        "    avg_row[\"Accuracy_pct\"] = round(float(class_acc.mean()), 2)\n",
        "    avg_row[\"N_samples\"]    = int(class_cnt.sum())\n",
        "    avg_row[\"Notes\"]        = f\"Normalized Compute: {norm_compute:.2f}\"\n",
        "    rows.append(avg_row)\n",
        "\n",
        "    with open(path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "    print(f\"CSV saved: {path}\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Train\n",
        "# ==========================\n",
        "def train_model(model, train_loader, val_loader, device,\n",
        "                epochs=30, seed=42):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1    = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        val_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1    = val_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            print(f\"  Epoch {ep+1:02d}/{epochs} | ValF1={val_f1:.4f} | Best={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"  Training done. Best Val Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Main\n",
        "# ==========================\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_DIR    = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/UCI_HAR\"\n",
        "    BATCH_SIZE  = 64\n",
        "    EPOCHS      = 30\n",
        "    SEED        = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY  = USE_GPU\n",
        "    NUM_CLASSES = 6\n",
        "    NUM_LAYERS  = 3\n",
        "\n",
        "    train_ds = UCIHARDataset(DATA_DIR, split=\"train\")\n",
        "    val_ds   = UCIHARDataset(DATA_DIR, split=\"test\")\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    # Full PADRe model\n",
        "    set_seed(SEED)\n",
        "    model = PADReHAR(\n",
        "        in_channels=9, seq_len=128, num_classes=NUM_CLASSES,\n",
        "        hidden_dim=48, num_layers=NUM_LAYERS, max_degree=3,\n",
        "        gate_hidden_dim=16, dropout=0.2,\n",
        "        temperature_initial=5.0, temperature_min=0.5,\n",
        "        gate_on=True, fixed_degree=1,\n",
        "        use_ste=True, use_hadamard=True,\n",
        "        use_ffn=True, use_residual=True,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\")\n",
        "    print(\"\\n>>> Training Full PADRe...\")\n",
        "    model = train_model(model, train_loader, val_loader, DEVICE,\n",
        "                        epochs=EPOCHS, seed=SEED)\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Table 1: Clean\n",
        "    # -----------------------------------------------\n",
        "    print(\"\\n>>> Collecting class-specific stats [Clean]...\")\n",
        "    ced_clean, cacc_clean, ccnt, nc_clean = collect_class_stats(\n",
        "        model, val_loader, DEVICE, num_classes=NUM_CLASSES, snr_db=None\n",
        "    )\n",
        "    avg_deg_clean, avg_acc_clean, _ = print_table1(\n",
        "        ced_clean, cacc_clean, ccnt, nc_clean,\n",
        "        num_layers=NUM_LAYERS, class_names=CLASS_NAMES, snr_label=\"Clean\"\n",
        "    )\n",
        "    save_table1_csv(\n",
        "        ced_clean, cacc_clean, ccnt, nc_clean,\n",
        "        num_layers=NUM_LAYERS, class_names=CLASS_NAMES,\n",
        "        path=\"/content/table1_clean.csv\"\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Table 1: Noisy (SNR=10 dB)\n",
        "    # -----------------------------------------------\n",
        "    print(\"\\n>>> Collecting class-specific stats [SNR=10 dB]...\")\n",
        "    ced_noisy, cacc_noisy, _, nc_noisy = collect_class_stats(\n",
        "        model, val_loader, DEVICE, num_classes=NUM_CLASSES, snr_db=10\n",
        "    )\n",
        "    avg_deg_noisy, avg_acc_noisy, _ = print_table1(\n",
        "        ced_noisy, cacc_noisy, ccnt, nc_noisy,\n",
        "        num_layers=NUM_LAYERS, class_names=CLASS_NAMES, snr_label=\"SNR=10 dB\"\n",
        "    )\n",
        "    save_table1_csv(\n",
        "        ced_noisy, cacc_noisy, ccnt, nc_noisy,\n",
        "        num_layers=NUM_LAYERS, class_names=CLASS_NAMES,\n",
        "        path=\"/content/table1_snr10.csv\"\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Summary\n",
        "    # -----------------------------------------------\n",
        "    print(\"\\n>>> Summary\")\n",
        "    print(f\"  Clean  | Avg Acc={avg_acc_clean:.2f}% | Norm Compute={nc_clean:.4f}\")\n",
        "    print(f\"  SNR=10 | Avg Acc={avg_acc_noisy:.2f}% | Norm Compute={nc_noisy:.4f}\")\n",
        "    print(f\"  Robustness Drop (Acc): {avg_acc_clean - avg_acc_noisy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PAMAP2"
      ],
      "metadata": {
        "id": "MB_hYMzcNlQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PADRe HAR - PAMAP2 (no Magnetometer)\n",
        "Table 1: Class-Specific Expected Degrees and Performance Metrics\n",
        "=============================================================================\n",
        "  Class | Exp Degree (L1) | Exp Degree (L2) | Exp Degree (L3) | Accuracy (%) | Notes\n",
        "\n",
        "측정:\n",
        "  - 클래스별 soft expected degree (per layer)\n",
        "  - 클래스별 accuracy\n",
        "  - Normalized Compute (전체 평균)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import re\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Seed / Device\n",
        "# ==========================\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# PAMAP2: Windowing\n",
        "# ==========================\n",
        "def create_pamap2_windows_no_magne(df: pd.DataFrame, window_size: int, step_size: int):\n",
        "    # 사용할 피처들 (NO magnetometer) => 27 channels\n",
        "    feature_cols = [\n",
        "        # hand\n",
        "        \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "        \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "        \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "\n",
        "        # chest\n",
        "        \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "        \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "        \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "\n",
        "        # ankle\n",
        "        \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "        \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "        \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "    ]\n",
        "\n",
        "    # 12 classes mapping (원본 activityID -> new 0..11)\n",
        "    ORDERED_IDS = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
        "    old2new = {old: i for i, old in enumerate(ORDERED_IDS)}\n",
        "    label_names = [\n",
        "        \"Lying\", \"Sitting\", \"Standing\", \"Walking\", \"Running\", \"Cycling\",\n",
        "        \"Nordic walking\", \"Ascending stairs\", \"Descending stairs\",\n",
        "        \"Vacuum cleaning\", \"Ironing\", \"Rope jumping\",\n",
        "    ]\n",
        "\n",
        "    X_list, y_list, subj_list = [], [], []\n",
        "\n",
        "    for subj_id, g in df.groupby(\"subject_id\"):\n",
        "        if \"timestamp\" in g.columns:\n",
        "            g = g.sort_values(\"timestamp\")\n",
        "        else:\n",
        "            g = g.sort_index()\n",
        "\n",
        "        data_arr  = g[feature_cols].to_numpy(dtype=np.float32)   # (L, C)\n",
        "        label_arr = g[\"activityID\"].to_numpy(dtype=np.int64)     # (L,)\n",
        "        L = data_arr.shape[0]\n",
        "\n",
        "        start = 0\n",
        "        while start + window_size <= L:\n",
        "            end = start + window_size\n",
        "            last_label_orig = int(label_arr[end - 1])\n",
        "\n",
        "            if last_label_orig == 0 or last_label_orig not in old2new:\n",
        "                start += step_size\n",
        "                continue\n",
        "\n",
        "            window_ct = data_arr[start:end].T  # (C, T)\n",
        "            X_list.append(window_ct)\n",
        "            y_list.append(old2new[last_label_orig])\n",
        "            subj_list.append(int(subj_id))\n",
        "            start += step_size\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No windows created. Check file format / columns / labels.\")\n",
        "\n",
        "    X = np.stack(X_list, axis=0).astype(np.float32)  # (N, C, T)\n",
        "    y = np.asarray(y_list, dtype=np.int64)\n",
        "    subj_ids = np.asarray(subj_list, dtype=np.int64)\n",
        "    return X, y, subj_ids, label_names\n",
        "\n",
        "\n",
        "class PAMAP2WindowDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        super().__init__()\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "\n",
        "class PAMAP2Loader:\n",
        "    def __init__(self, data_dir: str, window_size=128, step_size=64):\n",
        "        self.data_dir = data_dir\n",
        "        self.window_size = int(window_size)\n",
        "        self.step_size = int(step_size)\n",
        "\n",
        "        self.feature_cols = [\n",
        "            \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "            \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "            \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "\n",
        "            \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "            \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "            \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "\n",
        "            \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "            \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "            \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "        ]\n",
        "\n",
        "    def load(self):\n",
        "        csv_files = glob.glob(os.path.join(self.data_dir, \"*.csv\"))\n",
        "        if len(csv_files) == 0:\n",
        "            csv_files = glob.glob(os.path.join(self.data_dir, \"*.dat\"))\n",
        "\n",
        "        if len(csv_files) == 0:\n",
        "            raise RuntimeError(f\"No CSV/DAT files found under {self.data_dir}\")\n",
        "\n",
        "        dfs = []\n",
        "        for fpath in sorted(csv_files):\n",
        "            df_i = pd.read_csv(fpath)\n",
        "\n",
        "            if \"subject_id\" not in df_i.columns:\n",
        "                m = re.findall(r\"\\d+\", os.path.basename(fpath))\n",
        "                subj_guess = int(m[0]) if len(m) > 0 else 0\n",
        "                df_i[\"subject_id\"] = subj_guess\n",
        "\n",
        "            dfs.append(df_i)\n",
        "\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        if \"activityID\" not in df.columns:\n",
        "            raise RuntimeError(\"Column 'activityID' not found. Check your PAMAP2 csv/dat schema.\")\n",
        "        df = df.dropna(subset=[\"activityID\"])\n",
        "        df[\"activityID\"] = df[\"activityID\"].astype(np.int64)\n",
        "        df[\"subject_id\"] = df[\"subject_id\"].astype(np.int64)\n",
        "\n",
        "        if \"timestamp\" in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "        def _fill_subject_group(g):\n",
        "            if \"timestamp\" in g.columns:\n",
        "                g = g.sort_values(\"timestamp\")\n",
        "            else:\n",
        "                g = g.sort_index()\n",
        "            missing = [c for c in self.feature_cols if c not in g.columns]\n",
        "            if missing:\n",
        "                raise RuntimeError(f\"Missing feature columns: {missing[:5]} ... (total {len(missing)})\")\n",
        "\n",
        "            g[self.feature_cols] = (\n",
        "                g[self.feature_cols]\n",
        "                .interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
        "                .ffill().bfill()\n",
        "            )\n",
        "            return g\n",
        "\n",
        "        df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n",
        "        df[self.feature_cols] = df[self.feature_cols].fillna(0.0)\n",
        "\n",
        "        X, y, subj_ids, label_names = create_pamap2_windows_no_magne(\n",
        "            df, self.window_size, self.step_size\n",
        "        )\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Loaded PAMAP2 (no Magne)\")\n",
        "        print(f\"  X shape : {X.shape}  (N, C, T)\")\n",
        "        print(f\"  y shape : {y.shape}  (N,)\")\n",
        "        print(f\"  Subjects: {len(np.unique(subj_ids))}\")\n",
        "        print(f\"  Classes : {len(label_names)}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        return X, y, subj_ids, label_names\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Gate\n",
        "# ==========================\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self, channels, max_degree=3, gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0, temperature_min=0.5):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits     = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx    = logits.argmax(dim=-1)\n",
        "                hard_oh     = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                degree_w    = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "        else:\n",
        "            degree_w = soft_probs\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# PADRe Block\n",
        "# ==========================\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self, channels, seq_len, max_degree=3, kernel_size=11,\n",
        "                 gate_hidden_dim=16, temperature_initial=5.0, temperature_min=0.5,\n",
        "                 gate_on=True, fixed_degree=1, use_ste=True, use_hadamard=True):\n",
        "        super().__init__()\n",
        "        self.max_degree  = max_degree\n",
        "        self.gate_on     = gate_on\n",
        "        self.fixed_degree= fixed_degree\n",
        "        self.use_ste     = use_ste\n",
        "        self.use_hadamard= use_hadamard\n",
        "\n",
        "        if gate_on:\n",
        "            self.degree_gate = ComputeAwareDegreeGate(\n",
        "                channels, max_degree=max_degree,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "            )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=kernel_size,\n",
        "                      padding=kernel_size // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "        if use_hadamard:\n",
        "            self.pre_hadamard_channel = nn.ModuleList([\n",
        "                nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree - 1)\n",
        "            ])\n",
        "            self.pre_hadamard_token = nn.ModuleList([\n",
        "                nn.Conv1d(channels, channels, kernel_size=kernel_size,\n",
        "                          padding=kernel_size // 2, groups=channels)\n",
        "                for _ in range(max_degree - 1)\n",
        "            ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            if self.use_hadamard:\n",
        "                z = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(z * Y[i])\n",
        "            else:\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def _soft_forward(self, x, w):\n",
        "        B = x.shape[0]\n",
        "        Z = self._build_Z(x, self.max_degree)\n",
        "        Z_stack = torch.stack(Z, dim=1)              # (B,K,C,T)\n",
        "        return (Z_stack * w.view(B, self.max_degree, 1, 1)).sum(dim=1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "        if self.gate_on:\n",
        "            dw, logits, sp = self.degree_gate(x, use_ste=self.use_ste)\n",
        "        else:\n",
        "            fd = self.fixed_degree\n",
        "            dw = F.one_hot(torch.full((B,), fd, dtype=torch.long, device=x.device),\n",
        "                           num_classes=self.max_degree).float()\n",
        "            logits = dw.clone(); sp = dw.clone()\n",
        "\n",
        "        if not self.use_ste and self.gate_on:\n",
        "            out = self._soft_forward(x, dw)\n",
        "            sel = sp.argmax(dim=-1)\n",
        "        else:\n",
        "            sel = dw.argmax(dim=-1)\n",
        "            out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\"degree_selection\": dw, \"soft_probs\": sp,\n",
        "                         \"logits\": logits,\n",
        "                         \"compute_cost\": (sel + 1).float().mean().item()}\n",
        "        return out\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Model\n",
        "# ==========================\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self, in_channels=27, seq_len=128, num_classes=12,\n",
        "                 hidden_dim=48, num_layers=3, max_degree=3,\n",
        "                 gate_hidden_dim=16, dropout=0.2,\n",
        "                 temperature_initial=5.0, temperature_min=0.5,\n",
        "                 gate_on=True, fixed_degree=1,\n",
        "                 use_ste=True, use_hadamard=True,\n",
        "                 use_ffn=True, use_residual=True):\n",
        "        super().__init__()\n",
        "        self.num_layers   = num_layers\n",
        "        self.max_degree   = max_degree\n",
        "        self.gate_on      = gate_on\n",
        "        self.use_ffn      = use_ffn\n",
        "        self.use_residual = use_residual\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, kernel_size=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min,\n",
        "                       gate_on=gate_on, fixed_degree=fixed_degree,\n",
        "                       use_ste=use_ste, use_hadamard=use_hadamard)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        if use_ffn:\n",
        "            self.ffn = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                    nn.GELU(), nn.Dropout(dropout),\n",
        "                    nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)]) if use_ffn else None\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier  = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            if self.gate_on:\n",
        "                b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi); total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res if self.use_residual else x)\n",
        "\n",
        "            if self.use_ffn:\n",
        "                res2 = x\n",
        "                x = self.ffn[i](x)\n",
        "                x = self._ln(self.norms2[i], x + res2 if self.use_residual else x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Utilities\n",
        "# ==========================\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    sp  = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    return X + torch.randn_like(X) * torch.sqrt(sp / snr)\n",
        "\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Class-specific Expected Degree + Accuracy  ← 핵심\n",
        "# ==========================\n",
        "@torch.no_grad()\n",
        "def collect_class_stats(model, loader, device, num_classes=12, snr_db=None):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      class_exp_degree : np.ndarray (num_classes, L)  soft expected degree per class per layer\n",
        "      class_acc        : np.ndarray (num_classes,)    per-class accuracy\n",
        "      class_cnt        : np.ndarray (num_classes,)    sample count\n",
        "      norm_compute     : float                        normalized hard compute (overall)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    L = len(model.padre_blocks)\n",
        "    K = model.max_degree\n",
        "\n",
        "    class_deg_sum = np.zeros((num_classes, L), dtype=np.float64)\n",
        "    class_correct = np.zeros(num_classes, dtype=np.int64)\n",
        "    class_cnt     = np.zeros(num_classes, dtype=np.int64)\n",
        "\n",
        "    deg_sum_hard  = np.zeros(L, dtype=np.float64)\n",
        "    total_samples = 0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, snr_db)\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds  = logits.argmax(dim=1)\n",
        "        y_cpu  = y.cpu().numpy()\n",
        "        p_cpu  = preds.cpu().numpy()\n",
        "        B      = X.shape[0]\n",
        "        total_samples += B\n",
        "\n",
        "        for c in range(num_classes):\n",
        "            mask = (y_cpu == c)\n",
        "            class_cnt[c]     += int(mask.sum())\n",
        "            class_correct[c] += int((p_cpu[mask] == c).sum())\n",
        "\n",
        "        for li, gi in enumerate(gate_info_list):\n",
        "            sp      = gi[\"soft_probs\"]                        # (B, K)\n",
        "            deg_v   = torch.arange(1, K + 1, device=sp.device).float()\n",
        "            exp_deg = (sp * deg_v).sum(dim=-1).cpu().numpy()  # (B,)\n",
        "\n",
        "            for c in range(num_classes):\n",
        "                mask = (y_cpu == c)\n",
        "                if mask.any():\n",
        "                    class_deg_sum[c, li] += exp_deg[mask].sum()\n",
        "\n",
        "            sel = gi[\"degree_selection\"].argmax(dim=-1)\n",
        "            deg_sum_hard[li] += (sel.float() + 1.0).sum().item()\n",
        "\n",
        "    class_exp_degree = class_deg_sum / np.maximum(class_cnt[:, None], 1)\n",
        "    class_acc        = class_correct / np.maximum(class_cnt, 1) * 100.0\n",
        "\n",
        "    deg_mean     = deg_sum_hard / max(total_samples, 1)\n",
        "    norm_compute = float(deg_mean.sum()) / (L * K)\n",
        "\n",
        "    return class_exp_degree, class_acc, class_cnt, norm_compute\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Notes 자동 생성\n",
        "# ==========================\n",
        "def generate_notes_default(class_name, exp_degrees):\n",
        "    avg = float(np.mean(exp_degrees))\n",
        "    return f\"Avg degree={avg:.3f}\"\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Print Table 1\n",
        "# ==========================\n",
        "def print_table1(class_exp_degree, class_acc, class_cnt,\n",
        "                 norm_compute, num_layers=3,\n",
        "                 class_names=None, snr_label=\"Clean\"):\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [f\"class_{i}\" for i in range(len(class_acc))]\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "    L = num_layers\n",
        "\n",
        "    layer_headers = \"  \".join([f\"{'Exp Deg (L'+str(l+1)+')':>16}\" for l in range(L)])\n",
        "    print(f\"\\n{'='*110}\")\n",
        "    print(f\"  Table 1: Class-Specific Expected Degrees and Performance Metrics  [{snr_label}]\")\n",
        "    print(f\"{'='*110}\")\n",
        "    print(f\"{'Class':<22}  {layer_headers}  {'Accuracy (%)':>13}  Notes\")\n",
        "    print(f\"{'-'*110}\")\n",
        "\n",
        "    col_avg_deg = np.zeros(L, dtype=np.float64)\n",
        "    col_avg_acc = 0.0\n",
        "\n",
        "    for ci, cname in enumerate(class_names):\n",
        "        degs    = class_exp_degree[ci]\n",
        "        acc_val = class_acc[ci]\n",
        "        notes   = generate_notes_default(cname, degs)\n",
        "\n",
        "        deg_str = \"  \".join([f\"{d:>16.3f}\" for d in degs])\n",
        "        print(f\"{cname:<22}  {deg_str}  {acc_val:>13.1f}  {notes}\")\n",
        "\n",
        "        col_avg_deg += degs\n",
        "        col_avg_acc += acc_val\n",
        "\n",
        "    avg_deg = col_avg_deg / max(num_classes, 1)\n",
        "    avg_acc = col_avg_acc / max(num_classes, 1)\n",
        "    avg_deg_str = \"  \".join([f\"{d:>16.3f}\" for d in avg_deg])\n",
        "\n",
        "    print(f\"{'-'*110}\")\n",
        "    print(f\"{'Average':<22}  {avg_deg_str}  {avg_acc:>13.2f}  Normalized Compute: {norm_compute:.2f}\")\n",
        "    print(f\"{'='*110}\")\n",
        "\n",
        "    return avg_deg.tolist(), avg_acc, norm_compute\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Save Table 1 as CSV\n",
        "# ==========================\n",
        "def save_table1_csv(class_exp_degree, class_acc, class_cnt,\n",
        "                    norm_compute, num_layers=3,\n",
        "                    class_names=None,\n",
        "                    path=\"./table1_class_expected_degrees.csv\"):\n",
        "    if class_names is None:\n",
        "        class_names = [f\"class_{i}\" for i in range(len(class_acc))]\n",
        "\n",
        "    fieldnames = ([\"Class\"]\n",
        "                  + [f\"Exp_Degree_L{l+1}\" for l in range(num_layers)]\n",
        "                  + [\"Accuracy_pct\", \"N_samples\", \"Notes\"])\n",
        "\n",
        "    rows = []\n",
        "    for ci, cname in enumerate(class_names):\n",
        "        row = {\"Class\": cname}\n",
        "        for l in range(num_layers):\n",
        "            row[f\"Exp_Degree_L{l+1}\"] = round(float(class_exp_degree[ci, l]), 4)\n",
        "        row[\"Accuracy_pct\"] = round(float(class_acc[ci]), 2)\n",
        "        row[\"N_samples\"]    = int(class_cnt[ci])\n",
        "        row[\"Notes\"]        = generate_notes_default(cname, class_exp_degree[ci])\n",
        "        rows.append(row)\n",
        "\n",
        "    avg_row = {\"Class\": \"Average\"}\n",
        "    for l in range(num_layers):\n",
        "        avg_row[f\"Exp_Degree_L{l+1}\"] = round(float(class_exp_degree[:, l].mean()), 4)\n",
        "    avg_row[\"Accuracy_pct\"] = round(float(class_acc.mean()), 2)\n",
        "    avg_row[\"N_samples\"]    = int(class_cnt.sum())\n",
        "    avg_row[\"Notes\"]        = f\"Normalized Compute: {norm_compute:.2f}\"\n",
        "    rows.append(avg_row)\n",
        "\n",
        "    with open(path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "    print(f\"CSV saved: {path}\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Train\n",
        "# ==========================\n",
        "def train_model(model, train_loader, val_loader, device,\n",
        "                epochs=30, seed=42):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1    = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        val_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1    = val_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            print(f\"  Epoch {ep+1:02d}/{epochs} | ValF1={val_f1:.4f} | Best={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"  Training done. Best Val Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Main (PAMAP2)\n",
        "# ==========================\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_DIR    = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets\"\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE   = 50\n",
        "\n",
        "    BATCH_SIZE  = 64\n",
        "    EPOCHS      = 30\n",
        "    SEED        = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY  = USE_GPU\n",
        "\n",
        "    NUM_LAYERS  = 3\n",
        "    MAX_DEGREE  = 3\n",
        "\n",
        "    # ----------------------------\n",
        "    # Load PAMAP2 windows\n",
        "    # ----------------------------\n",
        "    set_seed(SEED)\n",
        "    loader = PAMAP2Loader(DATA_DIR, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "    X, y, subj_ids, LABEL_NAMES = loader.load()\n",
        "\n",
        "    NUM_CLASSES = len(LABEL_NAMES)\n",
        "    IN_CHANNELS = X.shape[1]\n",
        "\n",
        "    idx = np.arange(len(y))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx, test_size=0.2, random_state=SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    X_train = X[train_idx].copy()\n",
        "    X_test  = X[test_idx].copy()\n",
        "\n",
        "    y_train = y[train_idx]\n",
        "    y_test  = y[test_idx]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    Ntr, C, T = X_train.shape\n",
        "    Xtr_2d = X_train.transpose(0, 2, 1).reshape(-1, C)\n",
        "    scaler.fit(Xtr_2d)\n",
        "\n",
        "    Xtr_2d = scaler.transform(Xtr_2d)\n",
        "    X_train = Xtr_2d.reshape(Ntr, T, C).transpose(0, 2, 1)\n",
        "\n",
        "    Nte = X_test.shape[0]\n",
        "    Xte_2d = X_test.transpose(0, 2, 1).reshape(-1, C)\n",
        "    Xte_2d = scaler.transform(Xte_2d)\n",
        "    X_test = Xte_2d.reshape(Nte, T, C).transpose(0, 2, 1)\n",
        "\n",
        "    train_ds = PAMAP2WindowDataset(X_train, y_train)\n",
        "    val_ds   = PAMAP2WindowDataset(X_test,  y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Full PADRe model\n",
        "    # ----------------------------\n",
        "    model = PADReHAR(\n",
        "        in_channels=IN_CHANNELS,\n",
        "        seq_len=WINDOW_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=48,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=16,\n",
        "        dropout=0.2,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        gate_on=True,\n",
        "        fixed_degree=1,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\")\n",
        "    print(\"\\n>>> Training Full PADRe on PAMAP2 (no Magne)...\")\n",
        "    model = train_model(model, train_loader, val_loader, DEVICE,\n",
        "                        epochs=EPOCHS, seed=SEED)\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Table 1: Clean\n",
        "    # -----------------------------------------------\n",
        "    print(\"\\n>>> Collecting class-specific stats [Clean]...\")\n",
        "    ced_clean, cacc_clean, ccnt, nc_clean = collect_class_stats(\n",
        "        model, val_loader, DEVICE, num_classes=NUM_CLASSES, snr_db=None\n",
        "    )\n",
        "    avg_deg_clean, avg_acc_clean, _ = print_table1(\n",
        "        ced_clean, cacc_clean, ccnt, nc_clean,\n",
        "        num_layers=NUM_LAYERS, class_names=LABEL_NAMES, snr_label=\"Clean\"\n",
        "    )\n",
        "    save_table1_csv(\n",
        "        ced_clean, cacc_clean, ccnt, nc_clean,\n",
        "        num_layers=NUM_LAYERS, class_names=LABEL_NAMES,\n",
        "        path=\"./table1_pamap2_clean.csv\"\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Table 1: Noisy (SNR=10 dB)\n",
        "    # -----------------------------------------------\n",
        "    print(\"\\n>>> Collecting class-specific stats [SNR=10 dB]...\")\n",
        "    ced_noisy, cacc_noisy, _, nc_noisy = collect_class_stats(\n",
        "        model, val_loader, DEVICE, num_classes=NUM_CLASSES, snr_db=10\n",
        "    )\n",
        "    avg_deg_noisy, avg_acc_noisy, _ = print_table1(\n",
        "        ced_noisy, cacc_noisy, ccnt, nc_noisy,\n",
        "        num_layers=NUM_LAYERS, class_names=LABEL_NAMES, snr_label=\"SNR=10 dB\"\n",
        "    )\n",
        "    save_table1_csv(\n",
        "        ced_noisy, cacc_noisy, ccnt, nc_noisy,\n",
        "        num_layers=NUM_LAYERS, class_names=LABEL_NAMES,\n",
        "        path=\"./table1_pamap2_snr10.csv\"\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Summary\n",
        "    # -----------------------------------------------\n",
        "    print(\"\\n>>> Summary\")\n",
        "    print(f\"  Clean  | Avg Acc={avg_acc_clean:.2f}% | Norm Compute={nc_clean:.4f}\")\n",
        "    print(f\"  SNR=10 | Avg Acc={avg_acc_noisy:.2f}% | Norm Compute={nc_noisy:.4f}\")\n",
        "    print(f\"  Robustness Drop (Acc): {avg_acc_clean - avg_acc_noisy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LYyS7o6NmKV",
        "outputId": "17ee3033-c547-4514-f142-2f6a67d0caa3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1264141071.py:200: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Loaded PAMAP2 (no Magne)\n",
            "  X shape : (38862, 27, 100)  (N, C, T)\n",
            "  y shape : (38862,)  (N,)\n",
            "  Subjects: 9\n",
            "  Classes : 12\n",
            "================================================================================\n",
            "\n",
            "Model params: 79,749\n",
            "\n",
            ">>> Training Full PADRe on PAMAP2 (no Magne)...\n",
            "  Epoch 05/30 | ValF1=0.9309 | Best=0.9309 | Temp=4.792\n",
            "  Epoch 10/30 | ValF1=0.9466 | Best=0.9479 | Temp=4.013\n",
            "  Epoch 15/30 | ValF1=0.9630 | Best=0.9630 | Temp=2.872\n",
            "  Epoch 20/30 | ValF1=0.9678 | Best=0.9678 | Temp=1.696\n",
            "  Epoch 25/30 | ValF1=0.9715 | Best=0.9715 | Temp=0.822\n",
            "  Epoch 30/30 | ValF1=0.9726 | Best=0.9733 | Temp=0.500\n",
            "  Training done. Best Val Macro-F1: 0.9733\n",
            "\n",
            ">>> Collecting class-specific stats [Clean]...\n",
            "\n",
            "==============================================================================================================\n",
            "  Table 1: Class-Specific Expected Degrees and Performance Metrics  [Clean]\n",
            "==============================================================================================================\n",
            "Class                       Exp Deg (L1)      Exp Deg (L2)      Exp Deg (L3)   Accuracy (%)  Notes\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Lying                              2.083             2.160             1.974           98.8  Avg degree=2.072\n",
            "Sitting                            2.039             2.031             1.878           97.2  Avg degree=1.982\n",
            "Standing                           2.010             2.103             1.996           97.2  Avg degree=2.036\n",
            "Walking                            1.961             2.026             2.017           98.8  Avg degree=2.002\n",
            "Running                            1.925             2.011             2.042           98.5  Avg degree=1.993\n",
            "Cycling                            1.971             2.036             1.982           99.2  Avg degree=1.996\n",
            "Nordic walking                     1.982             1.995             2.005           99.2  Avg degree=1.994\n",
            "Ascending stairs                   1.978             2.044             2.010           94.7  Avg degree=2.011\n",
            "Descending stairs                  1.985             1.995             2.016           96.2  Avg degree=1.999\n",
            "Vacuum cleaning                    1.959             2.048             2.013           94.3  Avg degree=2.007\n",
            "Ironing                            1.993             1.952             1.960           95.9  Avg degree=1.968\n",
            "Rope jumping                       1.975             2.018             2.032           96.5  Avg degree=2.008\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Average                            1.988             2.035             1.994          97.21  Normalized Compute: 0.66\n",
            "==============================================================================================================\n",
            "CSV saved: ./table1_pamap2_clean.csv\n",
            "\n",
            ">>> Collecting class-specific stats [SNR=10 dB]...\n",
            "\n",
            "==============================================================================================================\n",
            "  Table 1: Class-Specific Expected Degrees and Performance Metrics  [SNR=10 dB]\n",
            "==============================================================================================================\n",
            "Class                       Exp Deg (L1)      Exp Deg (L2)      Exp Deg (L3)   Accuracy (%)  Notes\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Lying                              2.083             2.158             1.976           98.7  Avg degree=2.072\n",
            "Sitting                            2.039             2.028             1.882           96.8  Avg degree=1.983\n",
            "Standing                           2.010             2.099             1.997           96.6  Avg degree=2.035\n",
            "Walking                            1.961             2.024             2.019           98.6  Avg degree=2.002\n",
            "Running                            1.925             2.010             2.041           98.5  Avg degree=1.992\n",
            "Cycling                            1.971             2.035             1.983           98.9  Avg degree=1.996\n",
            "Nordic walking                     1.982             1.995             2.006           99.3  Avg degree=1.994\n",
            "Ascending stairs                   1.978             2.043             2.013           93.4  Avg degree=2.011\n",
            "Descending stairs                  1.985             1.995             2.017           96.2  Avg degree=1.999\n",
            "Vacuum cleaning                    1.959             2.046             2.014           94.7  Avg degree=2.006\n",
            "Ironing                            1.993             1.956             1.965           94.8  Avg degree=1.971\n",
            "Rope jumping                       1.975             2.018             2.033           96.0  Avg degree=2.009\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Average                            1.988             2.034             1.996          96.87  Normalized Compute: 0.66\n",
            "==============================================================================================================\n",
            "CSV saved: ./table1_pamap2_snr10.csv\n",
            "\n",
            ">>> Summary\n",
            "  Clean  | Avg Acc=97.21% | Norm Compute=0.6619\n",
            "  SNR=10 | Avg Acc=96.87% | Norm Compute=0.6625\n",
            "  Robustness Drop (Acc): 0.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MHEALTH"
      ],
      "metadata": {
        "id": "RckSHv7PWOZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PADRe HAR - Table 1: Class-Specific Expected Degrees and Performance Metrics (MHEALTH)\n",
        "=============================================================================\n",
        "출력 형식:\n",
        "  Class | Exp Degree (L1) | Exp Degree (L2) | Exp Degree (L3) | Accuracy (%) | Notes\n",
        "\n",
        "측정:\n",
        "  - 클래스별 soft expected degree (per layer)\n",
        "  - 클래스별 accuracy\n",
        "  - Normalized Compute (전체 평균)\n",
        "\n",
        "설정:\n",
        "  - MHEALTH: acc + gyro only (ECG/Mag 제외)\n",
        "  - 정규화: train windows 기준 fit, test는 transform만 (leakage-free)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import glob\n",
        "import random\n",
        "import copy\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Seed / Device\n",
        "# ==========================\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# MHEALTH Loading (acc+gyro only)\n",
        "# ==========================\n",
        "MHEALTH_LABEL_NAMES = [\n",
        "    \"Standing still\", \"Sitting and relaxing\", \"Lying down\",\n",
        "    \"Walking\", \"Climbing stairs\", \"Waist bends forward\",\n",
        "    \"Frontal elevation of arms\", \"Knees bending\", \"Cycling\",\n",
        "    \"Jogging\", \"Running\", \"Jump front & back\",\n",
        "]\n",
        "\n",
        "def _load_single_mhealth_log(path: str, all_feature_cols: list[str]):\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        names=all_feature_cols + [\"label\"],\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def load_mhealth_subject_dfs(data_dir: str):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      dfs: list[pd.DataFrame] each has subject_id col\n",
        "      all_feature_cols: list[str] (23 raw)\n",
        "    \"\"\"\n",
        "    all_feature_cols = [\n",
        "        \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",      # 0,1,2\n",
        "        \"ecg_1\", \"ecg_2\",                                 # 3,4\n",
        "        \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",      # 5,6,7\n",
        "        \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",   # 8,9,10\n",
        "        \"mag_ankle_x\", \"mag_ankle_y\", \"mag_ankle_z\",      # 11,12,13\n",
        "        \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",            # 14,15,16\n",
        "        \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",         # 17,18,19\n",
        "        \"mag_arm_x\", \"mag_arm_y\", \"mag_arm_z\",            # 20,21,22\n",
        "    ]\n",
        "\n",
        "    log_files = glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\"))\n",
        "    if not log_files:\n",
        "        raise FileNotFoundError(f\"No mHealth_subject*.log files found in {data_dir}\")\n",
        "    print(f\"Found {len(log_files)} log files in {data_dir}\")\n",
        "\n",
        "    dfs = []\n",
        "    for fp in sorted(log_files):\n",
        "        df_i = _load_single_mhealth_log(fp, all_feature_cols)\n",
        "\n",
        "        # subject id 추출 (파일명에서 숫자)\n",
        "        m = re.findall(r\"\\d+\", os.path.basename(fp))\n",
        "        subj_id = int(m[0]) if m else 0\n",
        "        df_i[\"subject_id\"] = subj_id\n",
        "\n",
        "        # label==0 제거, label 1..12 -> 0..11\n",
        "        df_i = df_i[df_i[\"label\"] != 0].copy()\n",
        "        df_i.loc[:, \"label\"] = df_i[\"label\"].astype(np.int64) - 1\n",
        "\n",
        "        dfs.append(df_i)\n",
        "\n",
        "    return dfs, all_feature_cols\n",
        "\n",
        "def create_mhealth_windows_per_subject(dfs, feature_cols, window_size, step_size):\n",
        "    X_list, y_list, s_list = [], [], []\n",
        "\n",
        "    for df in dfs:\n",
        "        subj_id = int(df[\"subject_id\"].iloc[0])\n",
        "        data_arr   = df[feature_cols].to_numpy(dtype=np.float32)    # (L, C)\n",
        "        labels_arr = df[\"label\"].to_numpy(dtype=np.int64)           # (L,)\n",
        "        L = data_arr.shape[0]\n",
        "\n",
        "        start = 0\n",
        "        while start + window_size <= L:\n",
        "            end = start + window_size\n",
        "            window_x = data_arr[start:end]                          # (T, C)\n",
        "            window_label = int(labels_arr[end - 1])                 # last label\n",
        "            window_x_ct = window_x.T                                # (C, T)\n",
        "\n",
        "            X_list.append(window_x_ct)\n",
        "            y_list.append(window_label)\n",
        "            s_list.append(subj_id)\n",
        "            start += step_size\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No windows created. Check window/step or data format.\")\n",
        "\n",
        "    X = np.stack(X_list, axis=0).astype(np.float32)    # (N, C, T)\n",
        "    y = np.asarray(y_list, dtype=np.int64)             # (N,)\n",
        "    subj_ids = np.asarray(s_list, dtype=np.int64)      # (N,)\n",
        "    return X, y, subj_ids\n",
        "\n",
        "\n",
        "class MHEALTHWindowDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y.astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "\n",
        "def normalize_train_only(X_train: np.ndarray, X_test: np.ndarray):\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    Ntr, C, T = X_train.shape\n",
        "    Xtr_2d = X_train.transpose(0, 2, 1).reshape(-1, C)  # (Ntr*T, C)\n",
        "    scaler.fit(Xtr_2d)\n",
        "\n",
        "    Xtr_2d = scaler.transform(Xtr_2d)\n",
        "    X_train_n = Xtr_2d.reshape(Ntr, T, C).transpose(0, 2, 1)\n",
        "\n",
        "    Nte = X_test.shape[0]\n",
        "    Xte_2d = X_test.transpose(0, 2, 1).reshape(-1, C)   # (Nte*T, C)\n",
        "    Xte_2d = scaler.transform(Xte_2d)\n",
        "    X_test_n = Xte_2d.reshape(Nte, T, C).transpose(0, 2, 1)\n",
        "\n",
        "    return X_train_n.astype(np.float32), X_test_n.astype(np.float32)\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Gate\n",
        "# ==========================\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self, channels, max_degree=3, gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0, temperature_min=0.5):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits     = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx    = logits.argmax(dim=-1)\n",
        "                hard_oh     = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                degree_w    = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "        else:\n",
        "            degree_w = soft_probs\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# PADRe Block\n",
        "# ==========================\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self, channels, seq_len, max_degree=3, kernel_size=11,\n",
        "                 gate_hidden_dim=16, temperature_initial=5.0, temperature_min=0.5,\n",
        "                 gate_on=True, fixed_degree=1, use_ste=True, use_hadamard=True):\n",
        "        super().__init__()\n",
        "        self.max_degree  = max_degree\n",
        "        self.gate_on     = gate_on\n",
        "        self.fixed_degree= fixed_degree\n",
        "        self.use_ste     = use_ste\n",
        "        self.use_hadamard= use_hadamard\n",
        "\n",
        "        if gate_on:\n",
        "            self.degree_gate = ComputeAwareDegreeGate(\n",
        "                channels, max_degree=max_degree,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "            )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=kernel_size,\n",
        "                      padding=kernel_size // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "        if use_hadamard:\n",
        "            self.pre_hadamard_channel = nn.ModuleList([\n",
        "                nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree - 1)\n",
        "            ])\n",
        "            self.pre_hadamard_token = nn.ModuleList([\n",
        "                nn.Conv1d(channels, channels, kernel_size=kernel_size,\n",
        "                          padding=kernel_size // 2, groups=channels)\n",
        "                for _ in range(max_degree - 1)\n",
        "            ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            if self.use_hadamard:\n",
        "                z = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(z * Y[i])\n",
        "            else:\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def _soft_forward(self, x, w):\n",
        "        B = x.shape[0]\n",
        "        Z = self._build_Z(x, self.max_degree)\n",
        "        Z_stack = torch.stack(Z, dim=1)              # (B,K,C,T)\n",
        "        return (Z_stack * w.view(B, self.max_degree, 1, 1)).sum(dim=1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "        if self.gate_on:\n",
        "            dw, logits, sp = self.degree_gate(x, use_ste=self.use_ste)\n",
        "        else:\n",
        "            fd = self.fixed_degree\n",
        "            dw = F.one_hot(torch.full((B,), fd, dtype=torch.long, device=x.device),\n",
        "                           num_classes=self.max_degree).float()\n",
        "            logits = dw.clone(); sp = dw.clone()\n",
        "\n",
        "        if not self.use_ste and self.gate_on:\n",
        "            out = self._soft_forward(x, dw)\n",
        "            sel = sp.argmax(dim=-1)\n",
        "        else:\n",
        "            sel = dw.argmax(dim=-1)\n",
        "            out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\"degree_selection\": dw, \"soft_probs\": sp,\n",
        "                         \"logits\": logits,\n",
        "                         \"compute_cost\": (sel + 1).float().mean().item()}\n",
        "        return out\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Model\n",
        "# ==========================\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self, in_channels=15, seq_len=128, num_classes=12,\n",
        "                 hidden_dim=48, num_layers=3, max_degree=3,\n",
        "                 gate_hidden_dim=16, dropout=0.2,\n",
        "                 temperature_initial=5.0, temperature_min=0.5,\n",
        "                 gate_on=True, fixed_degree=1,\n",
        "                 use_ste=True, use_hadamard=True,\n",
        "                 use_ffn=True, use_residual=True):\n",
        "        super().__init__()\n",
        "        self.num_layers   = num_layers\n",
        "        self.max_degree   = max_degree\n",
        "        self.gate_on      = gate_on\n",
        "        self.use_ffn      = use_ffn\n",
        "        self.use_residual = use_residual\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, kernel_size=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min,\n",
        "                       gate_on=gate_on, fixed_degree=fixed_degree,\n",
        "                       use_ste=use_ste, use_hadamard=use_hadamard)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        if use_ffn:\n",
        "            self.ffn = nn.ModuleList([\n",
        "                nn.Sequential(\n",
        "                    nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                    nn.GELU(), nn.Dropout(dropout),\n",
        "                    nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                    nn.Dropout(dropout),\n",
        "                )\n",
        "                for _ in range(num_layers)\n",
        "            ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)]) if use_ffn else None\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier  = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            if self.gate_on:\n",
        "                b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi); total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res if self.use_residual else x)\n",
        "\n",
        "            if self.use_ffn:\n",
        "                res2 = x\n",
        "                x = self.ffn[i](x)\n",
        "                x = self._ln(self.norms2[i], x + res2 if self.use_residual else x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Utilities\n",
        "# ==========================\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    sp  = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    return X + torch.randn_like(X) * torch.sqrt(sp / snr)\n",
        "\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Class-specific Expected Degree + Accuracy\n",
        "# ==========================\n",
        "@torch.no_grad()\n",
        "def collect_class_stats(model, loader, device, num_classes=12, snr_db=None):\n",
        "    model.eval()\n",
        "    L = len(model.padre_blocks)\n",
        "    K = model.max_degree\n",
        "\n",
        "    class_deg_sum = np.zeros((num_classes, L), dtype=np.float64)\n",
        "    class_correct = np.zeros(num_classes, dtype=np.int64)\n",
        "    class_cnt     = np.zeros(num_classes, dtype=np.int64)\n",
        "\n",
        "    deg_sum_hard  = np.zeros(L, dtype=np.float64)\n",
        "    total_samples = 0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, snr_db)\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds  = logits.argmax(dim=1)\n",
        "        y_cpu  = y.cpu().numpy()\n",
        "        p_cpu  = preds.cpu().numpy()\n",
        "        B      = X.shape[0]\n",
        "        total_samples += B\n",
        "\n",
        "        for c in range(num_classes):\n",
        "            mask = (y_cpu == c)\n",
        "            class_cnt[c]     += int(mask.sum())\n",
        "            class_correct[c] += int((p_cpu[mask] == c).sum())\n",
        "\n",
        "        for li, gi in enumerate(gate_info_list):\n",
        "            sp      = gi[\"soft_probs\"]                        # (B, K)\n",
        "            deg_v   = torch.arange(1, K + 1, device=sp.device).float()\n",
        "            exp_deg = (sp * deg_v).sum(dim=-1).cpu().numpy()  # (B,)\n",
        "\n",
        "            for c in range(num_classes):\n",
        "                mask = (y_cpu == c)\n",
        "                if mask.any():\n",
        "                    class_deg_sum[c, li] += exp_deg[mask].sum()\n",
        "\n",
        "            sel = gi[\"degree_selection\"].argmax(dim=-1)\n",
        "            deg_sum_hard[li] += (sel.float() + 1.0).sum().item()\n",
        "\n",
        "    class_exp_degree = class_deg_sum / np.maximum(class_cnt[:, None], 1)\n",
        "    class_acc        = class_correct / np.maximum(class_cnt, 1) * 100.0\n",
        "\n",
        "    deg_mean     = deg_sum_hard / max(total_samples, 1)\n",
        "    norm_compute = float(deg_mean.sum()) / (L * K)\n",
        "\n",
        "    return class_exp_degree, class_acc, class_cnt, norm_compute\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Notes / Table / CSV\n",
        "# ==========================\n",
        "def generate_notes_default(class_name, exp_degrees):\n",
        "    avg = float(np.mean(exp_degrees))\n",
        "    return f\"Avg degree={avg:.3f}\"\n",
        "\n",
        "def print_table1(class_exp_degree, class_acc, class_cnt,\n",
        "                 norm_compute, num_layers=3,\n",
        "                 class_names=None, snr_label=\"Clean\"):\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [f\"class_{i}\" for i in range(len(class_acc))]\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "    L = num_layers\n",
        "\n",
        "    layer_headers = \"  \".join([f\"{'Exp Deg (L'+str(l+1)+')':>16}\" for l in range(L)])\n",
        "    print(f\"\\n{'='*120}\")\n",
        "    print(f\"  Table 1: Class-Specific Expected Degrees and Performance Metrics  [{snr_label}]\")\n",
        "    print(f\"{'='*120}\")\n",
        "    print(f\"{'Class':<28}  {layer_headers}  {'Accuracy (%)':>13}  Notes\")\n",
        "    print(f\"{'-'*120}\")\n",
        "\n",
        "    col_avg_deg = np.zeros(L, dtype=np.float64)\n",
        "    col_avg_acc = 0.0\n",
        "\n",
        "    for ci, cname in enumerate(class_names):\n",
        "        degs    = class_exp_degree[ci]\n",
        "        acc_val = class_acc[ci]\n",
        "        notes   = generate_notes_default(cname, degs)\n",
        "\n",
        "        deg_str = \"  \".join([f\"{d:>16.3f}\" for d in degs])\n",
        "        print(f\"{cname:<28}  {deg_str}  {acc_val:>13.1f}  {notes}\")\n",
        "\n",
        "        col_avg_deg += degs\n",
        "        col_avg_acc += acc_val\n",
        "\n",
        "    avg_deg = col_avg_deg / max(num_classes, 1)\n",
        "    avg_acc = col_avg_acc / max(num_classes, 1)\n",
        "    avg_deg_str = \"  \".join([f\"{d:>16.3f}\" for d in avg_deg])\n",
        "\n",
        "    print(f\"{'-'*120}\")\n",
        "    print(f\"{'Average':<28}  {avg_deg_str}  {avg_acc:>13.2f}  Normalized Compute: {norm_compute:.2f}\")\n",
        "    print(f\"{'='*120}\")\n",
        "\n",
        "    return avg_deg.tolist(), avg_acc, norm_compute\n",
        "\n",
        "\n",
        "def save_table1_csv(class_exp_degree, class_acc, class_cnt,\n",
        "                    norm_compute, num_layers=3,\n",
        "                    class_names=None,\n",
        "                    path=\"/content/table1_mhealth_clean.csv\"):\n",
        "    if class_names is None:\n",
        "        class_names = [f\"class_{i}\" for i in range(len(class_acc))]\n",
        "\n",
        "    fieldnames = ([\"Class\"]\n",
        "                  + [f\"Exp_Degree_L{l+1}\" for l in range(num_layers)]\n",
        "                  + [\"Accuracy_pct\", \"N_samples\", \"Notes\"])\n",
        "\n",
        "    rows = []\n",
        "    for ci, cname in enumerate(class_names):\n",
        "        row = {\"Class\": cname}\n",
        "        for l in range(num_layers):\n",
        "            row[f\"Exp_Degree_L{l+1}\"] = round(float(class_exp_degree[ci, l]), 4)\n",
        "        row[\"Accuracy_pct\"] = round(float(class_acc[ci]), 2)\n",
        "        row[\"N_samples\"]    = int(class_cnt[ci])\n",
        "        row[\"Notes\"]        = generate_notes_default(cname, class_exp_degree[ci])\n",
        "        rows.append(row)\n",
        "\n",
        "    avg_row = {\"Class\": \"Average\"}\n",
        "    for l in range(num_layers):\n",
        "        avg_row[f\"Exp_Degree_L{l+1}\"] = round(float(class_exp_degree[:, l].mean()), 4)\n",
        "    avg_row[\"Accuracy_pct\"] = round(float(class_acc.mean()), 2)\n",
        "    avg_row[\"N_samples\"]    = int(class_cnt.sum())\n",
        "    avg_row[\"Notes\"]        = f\"Normalized Compute: {norm_compute:.2f}\"\n",
        "    rows.append(avg_row)\n",
        "\n",
        "    with open(path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "    print(f\"CSV saved: {path}\")\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Train\n",
        "# ==========================\n",
        "def train_model(model, train_loader, val_loader, device,\n",
        "                epochs=30, seed=42):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1    = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        val_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1    = val_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            print(f\"  Epoch {ep+1:02d}/{epochs} | ValF1={val_f1:.4f} | Best={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"  Training done. Best Val Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Main (MHEALTH)\n",
        "# ==========================\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_DIR    = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\"\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE   = 50\n",
        "\n",
        "    BATCH_SIZE  = 64\n",
        "    EPOCHS      = 30\n",
        "    SEED        = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY  = USE_GPU\n",
        "\n",
        "    NUM_LAYERS  = 3\n",
        "    NUM_CLASSES = 12\n",
        "\n",
        "    # --------------------------\n",
        "    # 1) load subject dfs\n",
        "    # --------------------------\n",
        "    set_seed(SEED)\n",
        "    dfs, all_cols = load_mhealth_subject_dfs(DATA_DIR)\n",
        "\n",
        "    # acc + gyro only (ECG/Mag 제외) => 15 channels\n",
        "    feature_cols = [\n",
        "        \"acc_chest_x\",\"acc_chest_y\",\"acc_chest_z\",\n",
        "        \"acc_ankle_x\",\"acc_ankle_y\",\"acc_ankle_z\",\n",
        "        \"gyro_ankle_x\",\"gyro_ankle_y\",\"gyro_ankle_z\",\n",
        "        \"acc_arm_x\",\"acc_arm_y\",\"acc_arm_z\",\n",
        "        \"gyro_arm_x\",\"gyro_arm_y\",\"gyro_arm_z\",\n",
        "    ]\n",
        "\n",
        "    # --------------------------\n",
        "    # 2) windowing (subject boundary safe)\n",
        "    # --------------------------\n",
        "    X, y, subj_ids = create_mhealth_windows_per_subject(\n",
        "        dfs, feature_cols, window_size=WINDOW_SIZE, step_size=STEP_SIZE\n",
        "    )\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Loaded MHEALTH windows (acc+gyro only)\")\n",
        "    print(f\"  X shape : {X.shape} (N, C, T)\")\n",
        "    print(f\"  y shape : {y.shape} (N,)\")\n",
        "    print(f\"  Classes : {len(MHEALTH_LABEL_NAMES)}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # --------------------------\n",
        "    # 3) split 먼저 (윈도우 stratify)\n",
        "    # --------------------------\n",
        "    idx = np.arange(len(y))\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        idx, test_size=0.2, random_state=SEED, stratify=y\n",
        "    )\n",
        "\n",
        "    X_train, y_train = X[train_idx].copy(), y[train_idx]\n",
        "    X_test,  y_test  = X[test_idx].copy(),  y[test_idx]\n",
        "\n",
        "    # --------------------------\n",
        "    # 4) normalize train-only (fit on train, transform both)\n",
        "    # --------------------------\n",
        "    X_train, X_test = normalize_train_only(X_train, X_test)\n",
        "\n",
        "    train_ds = MHEALTHWindowDataset(X_train, y_train)\n",
        "    val_ds   = MHEALTHWindowDataset(X_test,  y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    # --------------------------\n",
        "    # 5) model\n",
        "    # --------------------------\n",
        "    model = PADReHAR(\n",
        "        in_channels=X_train.shape[1],   # 15\n",
        "        seq_len=WINDOW_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=48,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=3,\n",
        "        gate_hidden_dim=16,\n",
        "        dropout=0.2,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        gate_on=True,\n",
        "        fixed_degree=1,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\")\n",
        "    print(\"\\n>>> Training Full PADRe on MHEALTH (acc+gyro only)...\")\n",
        "    model = train_model(model, train_loader, val_loader, DEVICE,\n",
        "                        epochs=EPOCHS, seed=SEED)\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Table 1: Clean\n",
        "    # -----------------------------------------------\n",
        "    print(\"\\n>>> Collecting class-specific stats [Clean]...\")\n",
        "    ced_clean, cacc_clean, ccnt, nc_clean = collect_class_stats(\n",
        "        model, val_loader, DEVICE, num_classes=NUM_CLASSES, snr_db=None\n",
        "    )\n",
        "    avg_deg_clean, avg_acc_clean, _ = print_table1(\n",
        "        ced_clean, cacc_clean, ccnt, nc_clean,\n",
        "        num_layers=NUM_LAYERS, class_names=MHEALTH_LABEL_NAMES, snr_label=\"Clean\"\n",
        "    )\n",
        "    save_table1_csv(\n",
        "        ced_clean, cacc_clean, ccnt, nc_clean,\n",
        "        num_layers=NUM_LAYERS, class_names=MHEALTH_LABEL_NAMES,\n",
        "        path=\"/content/table1_mhealth_clean.csv\"\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Table 1: Noisy (SNR=10 dB)\n",
        "    # -----------------------------------------------\n",
        "    print(\"\\n>>> Collecting class-specific stats [SNR=10 dB]...\")\n",
        "    ced_noisy, cacc_noisy, _, nc_noisy = collect_class_stats(\n",
        "        model, val_loader, DEVICE, num_classes=NUM_CLASSES, snr_db=10\n",
        "    )\n",
        "    avg_deg_noisy, avg_acc_noisy, _ = print_table1(\n",
        "        ced_noisy, cacc_noisy, ccnt, nc_noisy,\n",
        "        num_layers=NUM_LAYERS, class_names=MHEALTH_LABEL_NAMES, snr_label=\"SNR=10 dB\"\n",
        "    )\n",
        "    save_table1_csv(\n",
        "        ced_noisy, cacc_noisy, ccnt, nc_noisy,\n",
        "        num_layers=NUM_LAYERS, class_names=MHEALTH_LABEL_NAMES,\n",
        "        path=\"/content/table1_mhealth_snr10.csv\"\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------\n",
        "    # Summary\n",
        "    # -----------------------------------------------\n",
        "    print(\"\\n>>> Summary\")\n",
        "    print(f\"  Clean  | Avg Acc={avg_acc_clean:.2f}% | Norm Compute={nc_clean:.4f}\")\n",
        "    print(f\"  SNR=10 | Avg Acc={avg_acc_noisy:.2f}% | Norm Compute={nc_noisy:.4f}\")\n",
        "    print(f\"  Robustness Drop (Acc): {avg_acc_clean - avg_acc_noisy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkrQMRQPWQjD",
        "outputId": "c29a7e03-2947-451e-8cf6-b7fa4722c288"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "Found 10 log files in /content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\n",
            "================================================================================\n",
            "Loaded MHEALTH windows (acc+gyro only)\n",
            "  X shape : (6849, 15, 100) (N, C, T)\n",
            "  y shape : (6849,) (N,)\n",
            "  Classes : 12\n",
            "================================================================================\n",
            "\n",
            "Model params: 79,173\n",
            "\n",
            ">>> Training Full PADRe on MHEALTH (acc+gyro only)...\n",
            "  Epoch 05/30 | ValF1=0.9870 | Best=0.9870 | Temp=4.792\n",
            "  Epoch 10/30 | ValF1=0.9911 | Best=0.9911 | Temp=4.013\n",
            "  Epoch 15/30 | ValF1=0.9897 | Best=0.9918 | Temp=2.872\n",
            "  Epoch 20/30 | ValF1=0.9897 | Best=0.9918 | Temp=1.696\n",
            "  Epoch 25/30 | ValF1=0.9911 | Best=0.9918 | Temp=0.822\n",
            "  Epoch 30/30 | ValF1=0.9917 | Best=0.9924 | Temp=0.500\n",
            "  Training done. Best Val Macro-F1: 0.9924\n",
            "\n",
            ">>> Collecting class-specific stats [Clean]...\n",
            "\n",
            "========================================================================================================================\n",
            "  Table 1: Class-Specific Expected Degrees and Performance Metrics  [Clean]\n",
            "========================================================================================================================\n",
            "Class                             Exp Deg (L1)      Exp Deg (L2)      Exp Deg (L3)   Accuracy (%)  Notes\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Standing still                           2.001             1.977             1.963           99.2  Avg degree=1.981\n",
            "Sitting and relaxing                     2.014             2.034             2.000           99.2  Avg degree=2.016\n",
            "Lying down                               2.026             1.933             1.564          100.0  Avg degree=1.841\n",
            "Walking                                  2.007             1.964             1.965          100.0  Avg degree=1.979\n",
            "Climbing stairs                          2.005             1.968             1.920           99.2  Avg degree=1.964\n",
            "Waist bends forward                      1.996             2.018             1.896           99.1  Avg degree=1.970\n",
            "Frontal elevation of arms                2.039             2.034             1.982           99.2  Avg degree=2.018\n",
            "Knees bending                            1.990             1.977             1.987           98.3  Avg degree=1.985\n",
            "Cycling                                  2.022             2.026             1.980          100.0  Avg degree=2.009\n",
            "Jogging                                  2.016             1.990             1.954           98.4  Avg degree=1.987\n",
            "Running                                  2.012             1.963             1.996           99.2  Avg degree=1.990\n",
            "Jump front & back                        2.007             1.995             1.987          100.0  Avg degree=1.996\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Average                                  2.011             1.990             1.933          99.31  Normalized Compute: 0.66\n",
            "========================================================================================================================\n",
            "CSV saved: /content/table1_mhealth_clean.csv\n",
            "\n",
            ">>> Collecting class-specific stats [SNR=10 dB]...\n",
            "\n",
            "========================================================================================================================\n",
            "  Table 1: Class-Specific Expected Degrees and Performance Metrics  [SNR=10 dB]\n",
            "========================================================================================================================\n",
            "Class                             Exp Deg (L1)      Exp Deg (L2)      Exp Deg (L3)   Accuracy (%)  Notes\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Standing still                           2.001             1.978             1.967           99.2  Avg degree=1.982\n",
            "Sitting and relaxing                     2.014             2.027             2.002           99.2  Avg degree=2.014\n",
            "Lying down                               2.026             1.935             1.571          100.0  Avg degree=1.844\n",
            "Walking                                  2.007             1.965             1.967           99.2  Avg degree=1.980\n",
            "Climbing stairs                          2.005             1.967             1.926           99.2  Avg degree=1.966\n",
            "Waist bends forward                      1.996             2.016             1.901           99.1  Avg degree=1.971\n",
            "Frontal elevation of arms                2.039             2.030             1.985           99.2  Avg degree=2.018\n",
            "Knees bending                            1.990             1.976             1.988           98.3  Avg degree=1.985\n",
            "Cycling                                  2.022             2.021             1.982          100.0  Avg degree=2.008\n",
            "Jogging                                  2.016             1.988             1.958           98.4  Avg degree=1.987\n",
            "Running                                  2.012             1.964             1.997           99.2  Avg degree=1.991\n",
            "Jump front & back                        2.007             1.994             1.990          100.0  Avg degree=1.997\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Average                                  2.011             1.988             1.936          99.24  Normalized Compute: 0.66\n",
            "========================================================================================================================\n",
            "CSV saved: /content/table1_mhealth_snr10.csv\n",
            "\n",
            ">>> Summary\n",
            "  Clean  | Avg Acc=99.31% | Norm Compute=0.6610\n",
            "  SNR=10 | Avg Acc=99.24% | Norm Compute=0.6605\n",
            "  Robustness Drop (Acc): 0.07%\n"
          ]
        }
      ]
    }
  ]
}