{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Variants:\n",
        "  1) Full PADRe (Ours)\n",
        "  2) w/o Gate (Fixed Deg 1/2/3)\n",
        "  3) Gate w/o STE (Soft weighted sum)\n",
        "  4) w/o Hadamard\n",
        "  5) w/o FFN\n",
        "  6) w/o Residual\n",
        "\n",
        "Metrics:\n",
        "  - Clean Val Acc / Macro-F1\n",
        "  - Noisy Val Acc / Macro-F1 (SNR=10 dB)\n",
        "  - Robustness Drop (Clean F1 - Noisy F1)\n",
        "  - Normalized Compute\n",
        "  - Degree Entropy (gate diversity)"
      ],
      "metadata": {
        "id": "7uDu41wCVOhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UCI-HAR"
      ],
      "metadata": {
        "id": "qwqQYpw5VSDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "CLASS_NAMES = [\"WALK\", \"UP\", \"DOWN\", \"SIT\", \"STAND\", \"LAY\"]\n",
        "\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_dir, split=\"train\", normalize=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split    = split\n",
        "        self.X, self.y = self._load_data()\n",
        "        self.X = torch.FloatTensor(self.X)\n",
        "        self.y = torch.LongTensor(self.y) - 1\n",
        "\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def _load_data(self):\n",
        "        split_dir    = self.data_dir / self.split\n",
        "        signal_types = [\n",
        "            \"body_acc_x\",\"body_acc_y\",\"body_acc_z\",\n",
        "            \"body_gyro_x\",\"body_gyro_y\",\"body_gyro_z\",\n",
        "            \"total_acc_x\",\"total_acc_y\",\"total_acc_z\",\n",
        "        ]\n",
        "        signals = []\n",
        "        for st in signal_types:\n",
        "            fname = split_dir / \"Inertial Signals\" / f\"{st}_{self.split}.txt\"\n",
        "            signals.append(np.loadtxt(fname))\n",
        "        X = np.stack(signals, axis=1)\n",
        "        y = np.loadtxt(split_dir / f\"y_{self.split}.txt\", dtype=int)\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):  return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[idx]\n",
        "        y = self.y[idx]\n",
        "        if self.normalize is not None:\n",
        "            mean, std = self.normalize\n",
        "            X = (X - mean.squeeze(0)) / std.squeeze(0)\n",
        "        return X, y\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions (SNR=10 uses this)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate  (★ Variant behavior matches \"예전버전\")\n",
        "#   - use_ste=True  : train=STE(hard fwd, soft bwd), eval=hard onehot\n",
        "#   - use_ste=False : always soft_probs (train/eval 동일)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits = self.gate(x)  # (B,K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx = logits.argmax(dim=-1)\n",
        "                hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                # STE: forward=hard, backward=soft\n",
        "                degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(\n",
        "                    logits.argmax(dim=-1), num_classes=self.max_degree\n",
        "                ).float()\n",
        "        else:\n",
        "            # Gate w/o STE: always soft (train/eval 동일)\n",
        "            degree_w = soft_probs\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block (Ablation switches ONLY; ★ Variant behavior matches \"예전버전\")\n",
        "#   - w/o Gate: fixed_degree (1..K) → build up to d, output Z[d-1]\n",
        "#   - Gate w/o STE: soft weighted sum of ALL degrees (train/eval 동일)\n",
        "#   - w/o Hadamard: prefix sum (Z[i]=Z[i-1]+Y[i])  (예전버전)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlockAblation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,      # 1..K if w/o Gate (fixed)\n",
        "                 use_ste=True,           # Gate w/o STE (soft routing)\n",
        "                 use_hadamard=True,      # w/o Hadamard (prefix-sum)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_gate = bool(use_gate)\n",
        "        self.fixed_degree = fixed_degree  # None or int in [1..K]\n",
        "        self.use_ste = bool(use_ste)\n",
        "        self.use_hadamard = bool(use_hadamard)\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _build_Y(self, x, max_deg):\n",
        "        return [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        \"\"\"\n",
        "        - use_hadamard=True : 예전버전(원본) Hadamard chain\n",
        "            Z0=Y0, Zi = pre(Z_{i-1}) * Yi\n",
        "        - use_hadamard=False: 예전버전 w/o Hadamard (prefix sum)\n",
        "            Z0=Y0, Zi = Z_{i-1} + Yi\n",
        "        \"\"\"\n",
        "        Y = self._build_Y(x, max_deg)\n",
        "\n",
        "        if self.use_hadamard:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(Z_ * Y[i])\n",
        "            return Z\n",
        "        else:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "            return Z\n",
        "\n",
        "    def _hard_select(self, Z_list, sel):\n",
        "        B = Z_list[0].shape[0]\n",
        "        Z_stack = torch.stack(Z_list, dim=0)  # (K,B,C,T)\n",
        "        return Z_stack[sel, torch.arange(B, device=Z_stack.device)]\n",
        "\n",
        "    def _soft_weighted_output(self, x, soft_probs):\n",
        "        \"\"\"\n",
        "        예전버전 Gate w/o STE:\n",
        "          - always compute ALL K degrees\n",
        "          - weighted sum with soft_probs\n",
        "          - hadamard / no_hadamard build rule은 동일하게 적용\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        Z = self._build_Z(x, max_deg=self.max_degree)      # list length K, each (B,C,T)\n",
        "        Z_stack = torch.stack(Z, dim=1)                    # (B,K,C,T)\n",
        "        w = soft_probs.view(B, self.max_degree, 1, 1)      # (B,K,1,1)\n",
        "        out = (Z_stack * w).sum(dim=1)                     # (B,C,T)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # ---- Case A) w/o Gate (fixed degree 1..K) ----\n",
        "        if (not self.use_gate) or (self.fixed_degree is not None):\n",
        "            d = int(self.fixed_degree) if self.fixed_degree is not None else self.max_degree\n",
        "            d = max(1, min(d, self.max_degree))\n",
        "\n",
        "            # build only up to d, output \"degree d\" path (예전버전)\n",
        "            Z = self._build_Z(x, max_deg=d)\n",
        "            out = Z[-1]\n",
        "\n",
        "            # stats payload (예전버전 스타일)\n",
        "            sel = torch.full((B,), d - 1, device=x.device, dtype=torch.long)\n",
        "            K = self.max_degree\n",
        "            sp = F.one_hot(sel, num_classes=K).float()\n",
        "            dw = sp\n",
        "            logits = sp\n",
        "\n",
        "            out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            if return_gate_info:\n",
        "                return out, {\n",
        "                    \"degree_selection\": dw,\n",
        "                    \"soft_probs\": sp,\n",
        "                    \"logits\": logits,\n",
        "                    \"compute_cost\": float(d),\n",
        "                }\n",
        "            return out\n",
        "\n",
        "        # ---- Case B) Gate ON ----\n",
        "        degree_w, logits, soft_probs = self.degree_gate(x, use_ste=self.use_ste)\n",
        "\n",
        "        if (not self.use_ste):\n",
        "            # 예전버전: Gate w/o STE는 train/eval 관계없이 ALWAYS soft weighted sum\n",
        "            out = self._soft_weighted_output(x, degree_w)  # degree_w == soft_probs\n",
        "            # (stats only) 대표 degree\n",
        "            selected = soft_probs.argmax(dim=-1)\n",
        "            # compute_cost (예전코드에선 argmax 기반이었지만, 여기서는 gi에만 들어가므로 그대로 둠)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "        else:\n",
        "            # 원본: hard select (STE는 train에서 degree_w에 반영됨)\n",
        "            selected = degree_w.argmax(dim=-1)\n",
        "            max_deg = max(1, min(int(selected.max().item()) + 1, self.max_degree))\n",
        "            Z = self._build_Z(x, max_deg=max_deg)\n",
        "            out = self._hard_select(Z, selected)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_w,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": compute_cost,\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model (Ablation switches ONLY; otherwise matches your logic)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR_Ablation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,     # for w/o Gate (1/2/3)\n",
        "                 use_ste=True,          # Gate w/o STE\n",
        "                 use_hadamard=True,     # w/o Hadamard\n",
        "                 use_ffn=True,          # w/o FFN\n",
        "                 use_residual=True,     # w/o Residual\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_ffn = bool(use_ffn)\n",
        "        self.use_residual = bool(use_residual)\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlockAblation(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                token_kernel=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "                use_gate=use_gate,\n",
        "                fixed_degree=fixed_degree,\n",
        "                use_ste=use_ste,\n",
        "                use_hadamard=use_hadamard,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms1[i], x + res)\n",
        "            else:\n",
        "                x = self._ln(self.norms1[i], x)\n",
        "\n",
        "            res2 = x\n",
        "            if self.use_ffn:\n",
        "                x = self.ffn[i](x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms2[i], x + res2)\n",
        "            else:\n",
        "                x = self._ln(self.norms2[i], x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_train_stats(train_loader, device=\"cpu\", eps=1e-6):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      mean: (C,1) tensor\n",
        "      std : (C,1) tensor\n",
        "    Note:\n",
        "      X shape from loader: (B,C,T)\n",
        "      We compute stats over (B,T) for each channel.\n",
        "    \"\"\"\n",
        "    sum_x = None\n",
        "    sum_x2 = None\n",
        "    n = 0\n",
        "\n",
        "    for X, _ in train_loader:\n",
        "        X = X.to(device)  # (B,C,T)\n",
        "        B, C, T = X.shape\n",
        "        if sum_x is None:\n",
        "            sum_x = torch.zeros(C, device=device)\n",
        "            sum_x2 = torch.zeros(C, device=device)\n",
        "\n",
        "        # sum over batch and time\n",
        "        sum_x  += X.sum(dim=(0, 2))                 # (C,)\n",
        "        sum_x2 += (X * X).sum(dim=(0, 2))           # (C,)\n",
        "        n += B * T\n",
        "\n",
        "    mean = (sum_x / n)                              # (C,)\n",
        "    var  = (sum_x2 / n) - mean * mean               # (C,)\n",
        "    std  = torch.sqrt(torch.clamp(var, min=eps))    # (C,)\n",
        "\n",
        "    # reshape for broadcasting: (1,C,1) or (C,1)\n",
        "    mean = mean.view(1, -1, 1)\n",
        "    std  = std.view(1, -1, 1)\n",
        "    return mean.detach().cpu(), std.detach().cpu()\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Table Metrics (keep as-is in your \"now code\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_gate_stats(model, loader, device, snr_db=None, max_degree=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      macro_f1, degree_entropy, norm_comp\n",
        "    Definitions:\n",
        "      - degree_entropy: mean over (layers, samples) of normalized entropy of soft_probs\n",
        "                        H(p)/log(K)  where K=max_degree\n",
        "      - norm_comp: mean expected degree / max_degree, averaged over layers\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    ent_sum = 0.0\n",
        "    ent_count = 0\n",
        "    comp_sum = 0.0\n",
        "    comp_count = 0\n",
        "\n",
        "    eps = 1e-12\n",
        "    logK = float(np.log(max_degree))\n",
        "\n",
        "    deg_vals = torch.arange(1, max_degree + 1, device=device).float()\n",
        "\n",
        "    for X, y in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "        # gate stats\n",
        "        for gi in gate_info_list:\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            ent = -(sp * (sp + eps).log()).sum(dim=-1) / logK\n",
        "            ent_sum += ent.mean().item()\n",
        "            ent_count += 1\n",
        "\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "            comp_sum += (exp_deg / max_degree)\n",
        "            comp_count += 1\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    macro_f1 = float(f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    degree_entropy = float(ent_sum / max(ent_count, 1))\n",
        "    norm_comp = float(comp_sum / max(comp_count, 1))\n",
        "    return macro_f1, degree_entropy, norm_comp\n",
        "\n",
        "\n",
        "def format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp):\n",
        "    drop_pct = 100.0 * (clean_f1 - snr10_f1) / max(clean_f1, 1e-12)\n",
        "    return {\n",
        "        \"Variant\": name,\n",
        "        \"CleanF1\": clean_f1,\n",
        "        \"SNR10F1\": snr10_f1,\n",
        "        \"drop(%)\": drop_pct,\n",
        "        \"DegreeEntropy\": deg_ent,\n",
        "        \"NormComp\": norm_comp,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    header = [\"Variant\", \"Clean F1\", \"(SNR=10) F1\", \"drop(%)\", \"Degree Entropy\", \"NormComp\"]\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"UCI-HAR Ablation Table\")\n",
        "    print(\"=\"*110)\n",
        "    print(f\"{header[0]:<22s} | {header[1]:>8s} | {header[2]:>11s} | {header[3]:>7s} | {header[4]:>14s} | {header[5]:>8s}\")\n",
        "    print(\"-\"*110)\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']:<22s} | \"\n",
        "            f\"{r['CleanF1']:>8.4f} | \"\n",
        "            f\"{r['SNR10F1']:>11.4f} | \"\n",
        "            f\"{r['drop(%)']:>7.2f} | \"\n",
        "            f\"{r['DegreeEntropy']:>14.4f} | \"\n",
        "            f\"{r['NormComp']:>8.4f}\"\n",
        "        )\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    print(\"\\n[LaTeX rows]\")\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']} & {r['CleanF1']:.4f} & {r['SNR10F1']:.4f} & \"\n",
        "            f\"{r['drop(%)']:.2f} & {r['DegreeEntropy']:.4f} & {r['NormComp']:.4f} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Experiment Runner (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def build_variant(name, cfg):\n",
        "    model = PADReHAR_Ablation(\n",
        "        in_channels=cfg[\"in_channels\"],\n",
        "        seq_len=cfg[\"seq_len\"],\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_layers=cfg[\"num_layers\"],\n",
        "        max_degree=cfg[\"max_degree\"],\n",
        "        gate_hidden_dim=cfg[\"gate_hidden_dim\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        temperature_initial=cfg[\"temperature_initial\"],\n",
        "        temperature_min=cfg[\"temperature_min\"],\n",
        "        use_gate=cfg.get(\"use_gate\", True),\n",
        "        fixed_degree=cfg.get(\"fixed_degree\", None),\n",
        "        use_ste=cfg.get(\"use_ste\", True),\n",
        "        use_hadamard=cfg.get(\"use_hadamard\", True),\n",
        "        use_ffn=cfg.get(\"use_ffn\", True),\n",
        "        use_residual=cfg.get(\"use_residual\", True),\n",
        "    ).to(cfg[\"device\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg):\n",
        "    variants = []\n",
        "    variants.append((\"Full (Ours)\", dict()))\n",
        "    variants.append((\"w/o Gate-1\", dict(use_gate=False, fixed_degree=1)))\n",
        "    variants.append((\"w/o Gate-2\", dict(use_gate=False, fixed_degree=2)))\n",
        "    variants.append((\"w/o Gate-3\", dict(use_gate=False, fixed_degree=3)))\n",
        "    variants.append((\"Gate w/o STE\", dict(use_gate=True, use_ste=False)))\n",
        "    variants.append((\"w/o Hadamard\", dict(use_hadamard=False)))\n",
        "    variants.append((\"w/o FFN\", dict(use_ffn=False)))\n",
        "    variants.append((\"w/o Residual\", dict(use_residual=False)))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for name, delta in variants:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[Running Variant] {name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        cfg = copy.deepcopy(base_cfg)\n",
        "        cfg.update(delta)\n",
        "\n",
        "        set_seed(train_cfg[\"seed\"])\n",
        "        model = build_variant(name, cfg)\n",
        "        print(f\"Model params: {count_parameters(model):,}\")\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            cfg[\"device\"],\n",
        "            lr=train_cfg[\"lr\"],\n",
        "            weight_decay=train_cfg[\"wd\"],\n",
        "            epochs=train_cfg[\"epochs\"],\n",
        "            seed=train_cfg[\"seed\"],\n",
        "        )\n",
        "\n",
        "        clean_f1, deg_ent, norm_comp = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=None, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "        snr10_f1, _, _ = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=10.0, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "\n",
        "        rows.append(format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/UCI_HAR\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50\n",
        "\n",
        "    NUM_CLASSES = 6\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.3\n",
        "    LR = 2e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    train_dataset_raw = UCIHARDataset(DATA_PATH, split=\"train\", normalize=None)\n",
        "\n",
        "    stats_loader = DataLoader(\n",
        "        train_dataset_raw,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    mean, std = compute_train_stats(stats_loader, device=DEVICE if USE_GPU else \"cpu\")\n",
        "    train_dataset = UCIHARDataset(DATA_PATH, split=\"train\", normalize=(mean, std))\n",
        "    test_dataset  = UCIHARDataset(DATA_PATH, split=\"test\",  normalize=(mean, std))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    base_cfg = dict(\n",
        "        device=DEVICE,\n",
        "        in_channels=9,\n",
        "        seq_len=128,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "        dropout=DROPOUT,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        use_gate=True,\n",
        "        fixed_degree=None,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    )\n",
        "\n",
        "    train_cfg = dict(\n",
        "        seed=SEED,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        wd=WD,\n",
        "    )\n",
        "\n",
        "    rows = run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg)\n",
        "    print_table(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOKMkJivm9m1",
        "outputId": "8bd5cd9c-ab4a-4ff8-8810-357b5c7188e7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Full (Ours)\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.1816 | TestF1=0.9279 | BestF1=0.9279 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.1531 | TestF1=0.9356 | BestF1=0.9363 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.0971 | TestF1=0.9484 | BestF1=0.9501 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.0812 | TestF1=0.9467 | BestF1=0.9501 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.0767 | TestF1=0.9489 | BestF1=0.9512 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.0694 | TestF1=0.9485 | BestF1=0.9512 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0686 | TestF1=0.9463 | BestF1=0.9512 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0683 | TestF1=0.9464 | BestF1=0.9512 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0684 | TestF1=0.9467 | BestF1=0.9512 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0684 | TestF1=0.9463 | BestF1=0.9512 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9512\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-1\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.1847 | TestF1=0.9292 | BestF1=0.9292 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.1618 | TestF1=0.9250 | BestF1=0.9379 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1440 | TestF1=0.9495 | BestF1=0.9495 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1095 | TestF1=0.9403 | BestF1=0.9495 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.0874 | TestF1=0.9537 | BestF1=0.9537 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.0773 | TestF1=0.9536 | BestF1=0.9537 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0736 | TestF1=0.9503 | BestF1=0.9537 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0711 | TestF1=0.9533 | BestF1=0.9545 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0697 | TestF1=0.9497 | BestF1=0.9545 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0693 | TestF1=0.9526 | BestF1=0.9545 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9545\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-2\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.1790 | TestF1=0.9381 | BestF1=0.9381 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.1561 | TestF1=0.9468 | BestF1=0.9468 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1104 | TestF1=0.9514 | BestF1=0.9602 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.0858 | TestF1=0.9601 | BestF1=0.9658 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.0839 | TestF1=0.9592 | BestF1=0.9658 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.0709 | TestF1=0.9583 | BestF1=0.9658 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0686 | TestF1=0.9576 | BestF1=0.9658 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0683 | TestF1=0.9582 | BestF1=0.9658 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0683 | TestF1=0.9562 | BestF1=0.9658 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0682 | TestF1=0.9583 | BestF1=0.9658 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9658\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-3\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.1683 | TestF1=0.9291 | BestF1=0.9291 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.1536 | TestF1=0.9334 | BestF1=0.9334 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1049 | TestF1=0.9545 | BestF1=0.9545 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.0857 | TestF1=0.9514 | BestF1=0.9574 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.0722 | TestF1=0.9610 | BestF1=0.9622 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.0690 | TestF1=0.9644 | BestF1=0.9646 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0683 | TestF1=0.9623 | BestF1=0.9646 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0682 | TestF1=0.9623 | BestF1=0.9646 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0681 | TestF1=0.9623 | BestF1=0.9646 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0682 | TestF1=0.9623 | BestF1=0.9646 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9646\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Gate w/o STE\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.1756 | TestF1=0.9257 | BestF1=0.9259 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.1530 | TestF1=0.9117 | BestF1=0.9517 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1361 | TestF1=0.9281 | BestF1=0.9517 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.0977 | TestF1=0.9440 | BestF1=0.9544 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.0794 | TestF1=0.9400 | BestF1=0.9544 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.0709 | TestF1=0.9405 | BestF1=0.9544 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0714 | TestF1=0.9414 | BestF1=0.9544 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0684 | TestF1=0.9445 | BestF1=0.9544 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0684 | TestF1=0.9515 | BestF1=0.9544 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0683 | TestF1=0.9456 | BestF1=0.9544 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9544\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Hadamard\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.1890 | TestF1=0.9252 | BestF1=0.9252 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.1689 | TestF1=0.9382 | BestF1=0.9382 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1553 | TestF1=0.9208 | BestF1=0.9382 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1315 | TestF1=0.9293 | BestF1=0.9401 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.1068 | TestF1=0.9469 | BestF1=0.9469 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.0845 | TestF1=0.9466 | BestF1=0.9469 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0749 | TestF1=0.9438 | BestF1=0.9491 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0724 | TestF1=0.9428 | BestF1=0.9491 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0704 | TestF1=0.9423 | BestF1=0.9491 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0695 | TestF1=0.9416 | BestF1=0.9491 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9491\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o FFN\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.1770 | TestF1=0.9288 | BestF1=0.9288 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.1590 | TestF1=0.9347 | BestF1=0.9347 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1099 | TestF1=0.9548 | BestF1=0.9548 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.0843 | TestF1=0.9555 | BestF1=0.9622 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.0775 | TestF1=0.9552 | BestF1=0.9622 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.0695 | TestF1=0.9530 | BestF1=0.9622 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0690 | TestF1=0.9559 | BestF1=0.9622 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0685 | TestF1=0.9548 | BestF1=0.9622 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0688 | TestF1=0.9537 | BestF1=0.9622 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0691 | TestF1=0.9537 | BestF1=0.9622 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9622\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Residual\n",
            "================================================================================\n",
            "Model params: 78,591\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.1994 | TestF1=0.9096 | BestF1=0.9148 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.1732 | TestF1=0.9331 | BestF1=0.9331 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1632 | TestF1=0.9395 | BestF1=0.9395 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1448 | TestF1=0.9169 | BestF1=0.9401 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.1078 | TestF1=0.9487 | BestF1=0.9487 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.0810 | TestF1=0.9423 | BestF1=0.9487 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0756 | TestF1=0.9448 | BestF1=0.9503 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0719 | TestF1=0.9423 | BestF1=0.9503 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0697 | TestF1=0.9493 | BestF1=0.9503 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0695 | TestF1=0.9462 | BestF1=0.9503 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9503\n",
            "\n",
            "==============================================================================================================\n",
            "UCI-HAR Ablation Table\n",
            "==============================================================================================================\n",
            "Variant                | Clean F1 | (SNR=10) F1 | drop(%) | Degree Entropy | NormComp\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Full (Ours)            |   0.9512 |      0.9287 |    2.37 |         0.9979 |   0.6664\n",
            "w/o Gate-1             |   0.9545 |      0.9311 |    2.46 |         0.0000 |   0.3333\n",
            "w/o Gate-2             |   0.9658 |      0.9338 |    3.31 |         0.0000 |   0.6667\n",
            "w/o Gate-3             |   0.9646 |      0.9324 |    3.34 |         0.0000 |   1.0000\n",
            "Gate w/o STE           |   0.9544 |      0.9379 |    1.73 |         0.9176 |   0.6618\n",
            "w/o Hadamard           |   0.9491 |      0.9305 |    1.96 |         0.9949 |   0.6638\n",
            "w/o FFN                |   0.9622 |      0.9370 |    2.62 |         0.9982 |   0.6675\n",
            "w/o Residual           |   0.9503 |      0.9292 |    2.22 |         0.9945 |   0.6656\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[LaTeX rows]\n",
            "Full (Ours) & 0.9512 & 0.9287 & 2.37 & 0.9979 & 0.6664 \\\\\n",
            "w/o Gate-1 & 0.9545 & 0.9311 & 2.46 & 0.0000 & 0.3333 \\\\\n",
            "w/o Gate-2 & 0.9658 & 0.9338 & 3.31 & 0.0000 & 0.6667 \\\\\n",
            "w/o Gate-3 & 0.9646 & 0.9324 & 3.34 & 0.0000 & 1.0000 \\\\\n",
            "Gate w/o STE & 0.9544 & 0.9379 & 1.73 & 0.9176 & 0.6618 \\\\\n",
            "w/o Hadamard & 0.9491 & 0.9305 & 1.96 & 0.9949 & 0.6638 \\\\\n",
            "w/o FFN & 0.9622 & 0.9370 & 2.62 & 0.9982 & 0.6675 \\\\\n",
            "w/o Residual & 0.9503 & 0.9292 & 2.22 & 0.9945 & 0.6656 \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PAMAP2"
      ],
      "metadata": {
        "id": "zTlovHkii3An"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def create_pamap2_windows(df: pd.DataFrame, window_size: int, step_size: int):\n",
        "    feature_cols = [\n",
        "        # hand\n",
        "        \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "        \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "        \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "        # chest\n",
        "        \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "        \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "        \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "        # ankle\n",
        "        \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "        \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "        \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "    ]  # C = 27\n",
        "\n",
        "    ORDERED_IDS = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
        "    old2new = {\n",
        "        1: 0,   # Lying\n",
        "        2: 1,   # Sitting\n",
        "        3: 2,   # Standing\n",
        "        4: 3,   # Walking\n",
        "        5: 4,   # Running\n",
        "        6: 5,   # Cycling\n",
        "        7: 6,   # Nordic walking\n",
        "        12: 7,  # Ascending stairs\n",
        "        13: 8,  # Descending stairs\n",
        "        16: 9,  # Vacuum cleaning\n",
        "        17: 10, # Ironing\n",
        "        24: 11, # Rope jumping\n",
        "    }\n",
        "    label_names = [\n",
        "        \"Lying\", \"Sitting\", \"Standing\", \"Walking\",\n",
        "        \"Running\", \"Cycling\", \"Nordic walking\",\n",
        "        \"Ascending stairs\", \"Descending stairs\",\n",
        "        \"Vacuum cleaning\", \"Ironing\", \"Rope jumping\",\n",
        "    ]\n",
        "\n",
        "    X_list, y_list, subj_list = [], [], []\n",
        "\n",
        "    for subj_id, g in df.groupby(\"subject_id\"):\n",
        "        if \"timestamp\" in g.columns:\n",
        "            g = g.sort_values(\"timestamp\")\n",
        "        else:\n",
        "            g = g.sort_index()\n",
        "\n",
        "        data_arr  = g[feature_cols].to_numpy(dtype=np.float32)\n",
        "        label_arr = g[\"activityID\"].to_numpy(dtype=np.int64)\n",
        "        L = data_arr.shape[0]\n",
        "\n",
        "        start = 0\n",
        "        while start + window_size <= L:\n",
        "            end = start + window_size\n",
        "            last_label_orig = int(label_arr[end - 1])\n",
        "\n",
        "            if last_label_orig == 0:\n",
        "                start += step_size\n",
        "                continue\n",
        "            if last_label_orig not in old2new:\n",
        "                start += step_size\n",
        "                continue\n",
        "\n",
        "            window_ct = data_arr[start:end].T\n",
        "            X_list.append(window_ct)\n",
        "            y_list.append(old2new[last_label_orig])\n",
        "            subj_list.append(int(subj_id))\n",
        "            start += step_size\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        raise RuntimeError(\"No windows created. Check window_size/step_size and label filtering.\")\n",
        "\n",
        "    X = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y = np.asarray(y_list, dtype=np.int64)\n",
        "    subj_ids = np.asarray(subj_list, dtype=np.int64)\n",
        "    return X, y, subj_ids, label_names\n",
        "\n",
        "\n",
        "class PAMAP2Dataset(Dataset):\n",
        "    def __init__(self, data_dir, window_size, step_size):\n",
        "        super().__init__()\n",
        "\n",
        "        csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
        "        if len(csv_files) == 0:\n",
        "            raise RuntimeError(f\"No CSV files found under {data_dir}\")\n",
        "\n",
        "        dfs = []\n",
        "        for fpath in sorted(csv_files):\n",
        "            df_i = pd.read_csv(fpath)\n",
        "\n",
        "            if \"subject_id\" not in df_i.columns:\n",
        "                m = re.findall(r\"\\d+\", os.path.basename(fpath))\n",
        "                subj_guess = int(m[0]) if len(m) > 0 else 0\n",
        "                df_i[\"subject_id\"] = subj_guess\n",
        "\n",
        "            dfs.append(df_i)\n",
        "\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        df = df.dropna(subset=[\"activityID\"])\n",
        "        df[\"activityID\"] = df[\"activityID\"].astype(np.int64)\n",
        "        df[\"subject_id\"] = df[\"subject_id\"].astype(np.int64)\n",
        "        if \"timestamp\" in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "        feature_cols = [\n",
        "            # hand\n",
        "            \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "            \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "            \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "            # chest\n",
        "            \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "            \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "            \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "            # ankle\n",
        "            \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "            \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "            \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "        ]\n",
        "\n",
        "        def _fill_subject_group(g):\n",
        "            if \"timestamp\" in g.columns:\n",
        "                g = g.sort_values(\"timestamp\")\n",
        "            else:\n",
        "                g = g.sort_index()\n",
        "            g[feature_cols] = (\n",
        "                g[feature_cols]\n",
        "                .interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
        "                .ffill()\n",
        "                .bfill()\n",
        "            )\n",
        "            return g\n",
        "\n",
        "        df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n",
        "        df[feature_cols] = df[feature_cols].fillna(0.0)\n",
        "\n",
        "        X, y, subj_ids, label_names = create_pamap2_windows(df, window_size, step_size)\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y\n",
        "        self.subject_ids = subj_ids\n",
        "        self.label_names = label_names\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]\n",
        "        N, C, T = Xtr.shape\n",
        "\n",
        "        X2 = np.transpose(Xtr, (0, 2, 1)).reshape(-1, C)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = self.scaler\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = np.transpose(X, (0, 2, 1)).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        X_scaled = X2.reshape(N, T, C).transpose(0, 2, 1)\n",
        "\n",
        "        self.X = X_scaled.astype(np.float32)\n",
        "\n",
        "        print(\"Loaded PAMAP2 dataset\")\n",
        "        print(f\"X shape : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"Classes : {len(self.label_names)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long),\n",
        "            int(self.subject_ids[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions (SNR=10 uses this)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate  (★ Variant behavior matches \"예전버전\")\n",
        "#   - use_ste=True  : train=STE(hard fwd, soft bwd), eval=hard onehot\n",
        "#   - use_ste=False : always soft_probs (train/eval 동일)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits = self.gate(x)  # (B,K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx = logits.argmax(dim=-1)\n",
        "                hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                # STE: forward=hard, backward=soft\n",
        "                degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(\n",
        "                    logits.argmax(dim=-1), num_classes=self.max_degree\n",
        "                ).float()\n",
        "        else:\n",
        "            # Gate w/o STE: always soft (train/eval 동일)\n",
        "            degree_w = soft_probs\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block (Ablation switches ONLY; ★ Variant behavior matches \"예전버전\")\n",
        "#   - w/o Gate: fixed_degree (1..K) → build up to d, output Z[d-1]\n",
        "#   - Gate w/o STE: soft weighted sum of ALL degrees (train/eval 동일)\n",
        "#   - w/o Hadamard: prefix sum (Z[i]=Z[i-1]+Y[i])  (예전버전)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlockAblation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,      # 1..K if w/o Gate (fixed)\n",
        "                 use_ste=True,           # Gate w/o STE (soft routing)\n",
        "                 use_hadamard=True,      # w/o Hadamard (prefix-sum)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_gate = bool(use_gate)\n",
        "        self.fixed_degree = fixed_degree  # None or int in [1..K]\n",
        "        self.use_ste = bool(use_ste)\n",
        "        self.use_hadamard = bool(use_hadamard)\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _build_Y(self, x, max_deg):\n",
        "        return [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        \"\"\"\n",
        "        - use_hadamard=True : 예전버전(원본) Hadamard chain\n",
        "            Z0=Y0, Zi = pre(Z_{i-1}) * Yi\n",
        "        - use_hadamard=False: 예전버전 w/o Hadamard (prefix sum)\n",
        "            Z0=Y0, Zi = Z_{i-1} + Yi\n",
        "        \"\"\"\n",
        "        Y = self._build_Y(x, max_deg)\n",
        "\n",
        "        if self.use_hadamard:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(Z_ * Y[i])\n",
        "            return Z\n",
        "        else:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "            return Z\n",
        "\n",
        "    def _hard_select(self, Z_list, sel):\n",
        "        B = Z_list[0].shape[0]\n",
        "        Z_stack = torch.stack(Z_list, dim=0)  # (K,B,C,T)\n",
        "        return Z_stack[sel, torch.arange(B, device=Z_stack.device)]\n",
        "\n",
        "    def _soft_weighted_output(self, x, soft_probs):\n",
        "        \"\"\"\n",
        "        예전버전 Gate w/o STE:\n",
        "          - always compute ALL K degrees\n",
        "          - weighted sum with soft_probs\n",
        "          - hadamard / no_hadamard build rule은 동일하게 적용\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        Z = self._build_Z(x, max_deg=self.max_degree)      # list length K, each (B,C,T)\n",
        "        Z_stack = torch.stack(Z, dim=1)                    # (B,K,C,T)\n",
        "        w = soft_probs.view(B, self.max_degree, 1, 1)      # (B,K,1,1)\n",
        "        out = (Z_stack * w).sum(dim=1)                     # (B,C,T)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # ---- Case A) w/o Gate (fixed degree 1..K) ----\n",
        "        if (not self.use_gate) or (self.fixed_degree is not None):\n",
        "            d = int(self.fixed_degree) if self.fixed_degree is not None else self.max_degree\n",
        "            d = max(1, min(d, self.max_degree))\n",
        "\n",
        "            # build only up to d, output \"degree d\" path (예전버전)\n",
        "            Z = self._build_Z(x, max_deg=d)\n",
        "            out = Z[-1]\n",
        "\n",
        "            # stats payload (예전버전 스타일)\n",
        "            sel = torch.full((B,), d - 1, device=x.device, dtype=torch.long)\n",
        "            K = self.max_degree\n",
        "            sp = F.one_hot(sel, num_classes=K).float()\n",
        "            dw = sp\n",
        "            logits = sp\n",
        "\n",
        "            out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            if return_gate_info:\n",
        "                return out, {\n",
        "                    \"degree_selection\": dw,\n",
        "                    \"soft_probs\": sp,\n",
        "                    \"logits\": logits,\n",
        "                    \"compute_cost\": float(d),\n",
        "                }\n",
        "            return out\n",
        "\n",
        "        # ---- Case B) Gate ON ----\n",
        "        degree_w, logits, soft_probs = self.degree_gate(x, use_ste=self.use_ste)\n",
        "\n",
        "        if (not self.use_ste):\n",
        "            # 예전버전: Gate w/o STE는 train/eval 관계없이 ALWAYS soft weighted sum\n",
        "            out = self._soft_weighted_output(x, degree_w)  # degree_w == soft_probs\n",
        "            # (stats only) 대표 degree\n",
        "            selected = soft_probs.argmax(dim=-1)\n",
        "            # compute_cost (예전코드에선 argmax 기반이었지만, 여기서는 gi에만 들어가므로 그대로 둠)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "        else:\n",
        "            # 원본: hard select (STE는 train에서 degree_w에 반영됨)\n",
        "            selected = degree_w.argmax(dim=-1)\n",
        "            max_deg = max(1, min(int(selected.max().item()) + 1, self.max_degree))\n",
        "            Z = self._build_Z(x, max_deg=max_deg)\n",
        "            out = self._hard_select(Z, selected)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_w,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": compute_cost,\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model (Ablation switches ONLY; otherwise matches your logic)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR_Ablation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,     # for w/o Gate (1/2/3)\n",
        "                 use_ste=True,          # Gate w/o STE\n",
        "                 use_hadamard=True,     # w/o Hadamard\n",
        "                 use_ffn=True,          # w/o FFN\n",
        "                 use_residual=True,     # w/o Residual\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_ffn = bool(use_ffn)\n",
        "        self.use_residual = bool(use_residual)\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlockAblation(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                token_kernel=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "                use_gate=use_gate,\n",
        "                fixed_degree=fixed_degree,\n",
        "                use_ste=use_ste,\n",
        "                use_hadamard=use_hadamard,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms1[i], x + res)\n",
        "            else:\n",
        "                x = self._ln(self.norms1[i], x)\n",
        "\n",
        "            res2 = x\n",
        "            if self.use_ffn:\n",
        "                x = self.ffn[i](x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms2[i], x + res2)\n",
        "            else:\n",
        "                x = self._ln(self.norms2[i], x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Table Metrics (keep as-is in your \"now code\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_gate_stats(model, loader, device, snr_db=None, max_degree=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      macro_f1, degree_entropy, norm_comp\n",
        "    Definitions:\n",
        "      - degree_entropy: mean over (layers, samples) of normalized entropy of soft_probs\n",
        "                        H(p)/log(K)  where K=max_degree\n",
        "      - norm_comp: mean expected degree / max_degree, averaged over layers\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    ent_sum = 0.0\n",
        "    ent_count = 0\n",
        "    comp_sum = 0.0\n",
        "    comp_count = 0\n",
        "\n",
        "    eps = 1e-12\n",
        "    logK = float(np.log(max_degree))\n",
        "\n",
        "    deg_vals = torch.arange(1, max_degree + 1, device=device).float()\n",
        "\n",
        "    for X, y, _ in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "        # gate stats\n",
        "        for gi in gate_info_list:\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            ent = -(sp * (sp + eps).log()).sum(dim=-1) / logK\n",
        "            ent_sum += ent.mean().item()\n",
        "            ent_count += 1\n",
        "\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "            comp_sum += (exp_deg / max_degree)\n",
        "            comp_count += 1\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    macro_f1 = float(f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    degree_entropy = float(ent_sum / max(ent_count, 1))\n",
        "    norm_comp = float(comp_sum / max(comp_count, 1))\n",
        "    return macro_f1, degree_entropy, norm_comp\n",
        "\n",
        "\n",
        "def format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp):\n",
        "    drop_pct = 100.0 * (clean_f1 - snr10_f1) / max(clean_f1, 1e-12)\n",
        "    return {\n",
        "        \"Variant\": name,\n",
        "        \"CleanF1\": clean_f1,\n",
        "        \"SNR10F1\": snr10_f1,\n",
        "        \"drop(%)\": drop_pct,\n",
        "        \"DegreeEntropy\": deg_ent,\n",
        "        \"NormComp\": norm_comp,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    header = [\"Variant\", \"Clean F1\", \"(SNR=10) F1\", \"drop(%)\", \"Degree Entropy\", \"NormComp\"]\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"UCI-HAR Ablation Table\")\n",
        "    print(\"=\"*110)\n",
        "    print(f\"{header[0]:<22s} | {header[1]:>8s} | {header[2]:>11s} | {header[3]:>7s} | {header[4]:>14s} | {header[5]:>8s}\")\n",
        "    print(\"-\"*110)\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']:<22s} | \"\n",
        "            f\"{r['CleanF1']:>8.4f} | \"\n",
        "            f\"{r['SNR10F1']:>11.4f} | \"\n",
        "            f\"{r['drop(%)']:>7.2f} | \"\n",
        "            f\"{r['DegreeEntropy']:>14.4f} | \"\n",
        "            f\"{r['NormComp']:>8.4f}\"\n",
        "        )\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    print(\"\\n[LaTeX rows]\")\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']} & {r['CleanF1']:.4f} & {r['SNR10F1']:.4f} & \"\n",
        "            f\"{r['drop(%)']:.2f} & {r['DegreeEntropy']:.4f} & {r['NormComp']:.4f} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Experiment Runner (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def build_variant(name, cfg):\n",
        "    model = PADReHAR_Ablation(\n",
        "        in_channels=cfg[\"in_channels\"],\n",
        "        seq_len=cfg[\"seq_len\"],\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_layers=cfg[\"num_layers\"],\n",
        "        max_degree=cfg[\"max_degree\"],\n",
        "        gate_hidden_dim=cfg[\"gate_hidden_dim\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        temperature_initial=cfg[\"temperature_initial\"],\n",
        "        temperature_min=cfg[\"temperature_min\"],\n",
        "        use_gate=cfg.get(\"use_gate\", True),\n",
        "        fixed_degree=cfg.get(\"fixed_degree\", None),\n",
        "        use_ste=cfg.get(\"use_ste\", True),\n",
        "        use_hadamard=cfg.get(\"use_hadamard\", True),\n",
        "        use_ffn=cfg.get(\"use_ffn\", True),\n",
        "        use_residual=cfg.get(\"use_residual\", True),\n",
        "    ).to(cfg[\"device\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg):\n",
        "    variants = []\n",
        "    variants.append((\"Full (Ours)\", dict()))\n",
        "    variants.append((\"w/o Gate-1\", dict(use_gate=False, fixed_degree=1)))\n",
        "    variants.append((\"w/o Gate-2\", dict(use_gate=False, fixed_degree=2)))\n",
        "    variants.append((\"w/o Gate-3\", dict(use_gate=False, fixed_degree=3)))\n",
        "    variants.append((\"Gate w/o STE\", dict(use_gate=True, use_ste=False)))\n",
        "    variants.append((\"w/o Hadamard\", dict(use_hadamard=False)))\n",
        "    variants.append((\"w/o FFN\", dict(use_ffn=False)))\n",
        "    variants.append((\"w/o Residual\", dict(use_residual=False)))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for name, delta in variants:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[Running Variant] {name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        cfg = copy.deepcopy(base_cfg)\n",
        "        cfg.update(delta)\n",
        "\n",
        "        set_seed(train_cfg[\"seed\"])\n",
        "        model = build_variant(name, cfg)\n",
        "        print(f\"Model params: {count_parameters(model):,}\")\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            cfg[\"device\"],\n",
        "            lr=train_cfg[\"lr\"],\n",
        "            weight_decay=train_cfg[\"wd\"],\n",
        "            epochs=train_cfg[\"epochs\"],\n",
        "            seed=train_cfg[\"seed\"],\n",
        "        )\n",
        "\n",
        "        clean_f1, deg_ent, norm_comp = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=None, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "        snr10_f1, _, _ = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=10.0, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "\n",
        "        rows.append(format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH =  \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50\n",
        "\n",
        "    NUM_CLASSES = 12\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.3\n",
        "    LR = 2e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE = 50\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = PAMAP2Dataset(\n",
        "        data_dir=DATA_PATH,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        step_size=STEP_SIZE\n",
        "    )\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    base_cfg = dict(\n",
        "        device=DEVICE,\n",
        "        in_channels=27,\n",
        "        seq_len=100,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "        dropout=DROPOUT,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        use_gate=True,\n",
        "        fixed_degree=None,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    )\n",
        "\n",
        "    train_cfg = dict(\n",
        "        seed=SEED,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        wd=WD,\n",
        "    )\n",
        "\n",
        "    rows = run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg)\n",
        "    print_table(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7Ct94csi4w9",
        "outputId": "73849849-7fd1-4bb4-b65f-8c5656661b2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3028326310.py:166: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded PAMAP2 dataset\n",
            "X shape : (38862, 27, 100)  (N, C, T)\n",
            "y shape : (38862,)  (N,)\n",
            "Classes : 12\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Full (Ours)\n",
            "================================================================================\n",
            "Model params: 79,749\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.3250 | TestF1=0.9172 | BestF1=0.9202 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.2327 | TestF1=0.9389 | BestF1=0.9407 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1741 | TestF1=0.9534 | BestF1=0.9547 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1378 | TestF1=0.9634 | BestF1=0.9634 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.1179 | TestF1=0.9681 | BestF1=0.9683 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.1006 | TestF1=0.9713 | BestF1=0.9730 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0926 | TestF1=0.9771 | BestF1=0.9778 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0887 | TestF1=0.9788 | BestF1=0.9792 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0871 | TestF1=0.9803 | BestF1=0.9803 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0864 | TestF1=0.9801 | BestF1=0.9804 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9804\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-1\n",
            "================================================================================\n",
            "Model params: 79,749\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.3664 | TestF1=0.9086 | BestF1=0.9188 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.2815 | TestF1=0.9328 | BestF1=0.9328 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.2265 | TestF1=0.9443 | BestF1=0.9469 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1894 | TestF1=0.9498 | BestF1=0.9521 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.1621 | TestF1=0.9594 | BestF1=0.9613 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.1387 | TestF1=0.9643 | BestF1=0.9643 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.1218 | TestF1=0.9694 | BestF1=0.9694 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.1138 | TestF1=0.9719 | BestF1=0.9719 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.1072 | TestF1=0.9737 | BestF1=0.9737 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.1048 | TestF1=0.9737 | BestF1=0.9738 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9738\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-2\n",
            "================================================================================\n",
            "Model params: 79,749\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.3210 | TestF1=0.9186 | BestF1=0.9189 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.2241 | TestF1=0.9441 | BestF1=0.9441 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1674 | TestF1=0.9530 | BestF1=0.9568 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1361 | TestF1=0.9615 | BestF1=0.9628 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.1141 | TestF1=0.9716 | BestF1=0.9716 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.1007 | TestF1=0.9699 | BestF1=0.9729 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0938 | TestF1=0.9781 | BestF1=0.9781 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0896 | TestF1=0.9796 | BestF1=0.9797 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0877 | TestF1=0.9802 | BestF1=0.9805 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0870 | TestF1=0.9800 | BestF1=0.9805 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9805\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-3\n",
            "================================================================================\n",
            "Model params: 79,749\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.3133 | TestF1=0.9240 | BestF1=0.9240 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.2195 | TestF1=0.9472 | BestF1=0.9472 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1655 | TestF1=0.9591 | BestF1=0.9591 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1327 | TestF1=0.9670 | BestF1=0.9670 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.1128 | TestF1=0.9695 | BestF1=0.9695 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.1001 | TestF1=0.9745 | BestF1=0.9745 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0897 | TestF1=0.9792 | BestF1=0.9792 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0872 | TestF1=0.9794 | BestF1=0.9794 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0855 | TestF1=0.9809 | BestF1=0.9809 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0850 | TestF1=0.9806 | BestF1=0.9810 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9810\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Gate w/o STE\n",
            "================================================================================\n",
            "Model params: 79,749\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.3043 | TestF1=0.9281 | BestF1=0.9281 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.2124 | TestF1=0.9475 | BestF1=0.9475 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1546 | TestF1=0.9567 | BestF1=0.9567 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1258 | TestF1=0.9674 | BestF1=0.9674 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.1052 | TestF1=0.9734 | BestF1=0.9734 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.0946 | TestF1=0.9738 | BestF1=0.9749 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.0903 | TestF1=0.9764 | BestF1=0.9784 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.0871 | TestF1=0.9777 | BestF1=0.9784 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0855 | TestF1=0.9775 | BestF1=0.9784 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0849 | TestF1=0.9773 | BestF1=0.9784 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9784\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Hadamard\n",
            "================================================================================\n",
            "Model params: 79,749\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.3710 | TestF1=0.9146 | BestF1=0.9148 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.2842 | TestF1=0.9342 | BestF1=0.9342 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.2321 | TestF1=0.9407 | BestF1=0.9430 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1920 | TestF1=0.9497 | BestF1=0.9519 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.1661 | TestF1=0.9590 | BestF1=0.9590 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.1408 | TestF1=0.9640 | BestF1=0.9640 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.1258 | TestF1=0.9651 | BestF1=0.9659 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.1148 | TestF1=0.9687 | BestF1=0.9702 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.1092 | TestF1=0.9696 | BestF1=0.9702 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.1072 | TestF1=0.9701 | BestF1=0.9711 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9711\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o FFN\n",
            "================================================================================\n",
            "Model params: 79,749\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.3195 | TestF1=0.9273 | BestF1=0.9273 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.2317 | TestF1=0.9341 | BestF1=0.9415 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.1874 | TestF1=0.9482 | BestF1=0.9488 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.1582 | TestF1=0.9539 | BestF1=0.9550 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.1345 | TestF1=0.9604 | BestF1=0.9615 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.1192 | TestF1=0.9644 | BestF1=0.9647 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.1079 | TestF1=0.9673 | BestF1=0.9673 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.1029 | TestF1=0.9675 | BestF1=0.9677 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.0983 | TestF1=0.9687 | BestF1=0.9691 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.0971 | TestF1=0.9691 | BestF1=0.9691 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9691\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Residual\n",
            "================================================================================\n",
            "Model params: 79,749\n",
            "Epoch 05/50 | LR=0.0020 | Train Loss=0.4173 | TestF1=0.9030 | BestF1=0.9030 | Temp=4.926\n",
            "Epoch 10/50 | LR=0.0018 | Train Loss=0.3267 | TestF1=0.9086 | BestF1=0.9154 | Temp=4.636\n",
            "Epoch 15/50 | LR=0.0016 | Train Loss=0.2788 | TestF1=0.9161 | BestF1=0.9273 | Temp=4.153\n",
            "Epoch 20/50 | LR=0.0013 | Train Loss=0.2407 | TestF1=0.9311 | BestF1=0.9311 | Temp=3.527\n",
            "Epoch 25/50 | LR=0.0010 | Train Loss=0.2013 | TestF1=0.9432 | BestF1=0.9432 | Temp=2.822\n",
            "Epoch 30/50 | LR=0.0007 | Train Loss=0.1670 | TestF1=0.9533 | BestF1=0.9533 | Temp=2.110\n",
            "Epoch 35/50 | LR=0.0004 | Train Loss=0.1401 | TestF1=0.9545 | BestF1=0.9551 | Temp=1.463\n",
            "Epoch 40/50 | LR=0.0002 | Train Loss=0.1190 | TestF1=0.9601 | BestF1=0.9601 | Temp=0.947\n",
            "Epoch 45/50 | LR=0.0001 | Train Loss=0.1091 | TestF1=0.9627 | BestF1=0.9627 | Temp=0.615\n",
            "Epoch 50/50 | LR=0.0000 | Train Loss=0.1049 | TestF1=0.9632 | BestF1=0.9640 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9640\n",
            "\n",
            "==============================================================================================================\n",
            "UCI-HAR Ablation Table\n",
            "==============================================================================================================\n",
            "Variant                | Clean F1 | (SNR=10) F1 | drop(%) | Degree Entropy | NormComp\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Full (Ours)            |   0.9804 |      0.9769 |    0.36 |         0.9165 |   0.6667\n",
            "w/o Gate-1             |   0.9738 |      0.9700 |    0.39 |         0.0000 |   0.3333\n",
            "w/o Gate-2             |   0.9805 |      0.9769 |    0.36 |         0.0000 |   0.6667\n",
            "w/o Gate-3             |   0.9810 |      0.9763 |    0.48 |         0.0000 |   1.0000\n",
            "Gate w/o STE           |   0.9784 |      0.9762 |    0.23 |         0.8451 |   0.6452\n",
            "w/o Hadamard           |   0.9711 |      0.9682 |    0.30 |         0.9342 |   0.6653\n",
            "w/o FFN                |   0.9691 |      0.9641 |    0.52 |         0.9122 |   0.6646\n",
            "w/o Residual           |   0.9640 |      0.9606 |    0.35 |         0.9162 |   0.6688\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[LaTeX rows]\n",
            "Full (Ours) & 0.9804 & 0.9769 & 0.36 & 0.9165 & 0.6667 \\\\\n",
            "w/o Gate-1 & 0.9738 & 0.9700 & 0.39 & 0.0000 & 0.3333 \\\\\n",
            "w/o Gate-2 & 0.9805 & 0.9769 & 0.36 & 0.0000 & 0.6667 \\\\\n",
            "w/o Gate-3 & 0.9810 & 0.9763 & 0.48 & 0.0000 & 1.0000 \\\\\n",
            "Gate w/o STE & 0.9784 & 0.9762 & 0.23 & 0.8451 & 0.6452 \\\\\n",
            "w/o Hadamard & 0.9711 & 0.9682 & 0.30 & 0.9342 & 0.6653 \\\\\n",
            "w/o FFN & 0.9691 & 0.9641 & 0.52 & 0.9122 & 0.6646 \\\\\n",
            "w/o Residual & 0.9640 & 0.9606 & 0.35 & 0.9162 & 0.6688 \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MHEALTH"
      ],
      "metadata": {
        "id": "ZWMFEU1HwhSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def _load_single_mhealth_log(path: str, feature_cols: list[str]):\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        names=feature_cols + [\"label\"],\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def load_mhealth_dataframe(data_dir: str):\n",
        "    feature_cols = [\n",
        "        \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",      # 0,1,2\n",
        "        \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",      # 5,6,7\n",
        "        \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",   # 8,9,10\n",
        "        \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",            # 14,15,16\n",
        "        \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",         # 17,18,19\n",
        "    ]  # total 15 channels\n",
        "\n",
        "    log_files = glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\"))\n",
        "    if not log_files:\n",
        "        raise FileNotFoundError(f\"No mHealth_subject*.log files found in {data_dir}\")\n",
        "    print(f\"Found {len(log_files)} log files in {data_dir}\")\n",
        "\n",
        "    dfs = []\n",
        "    for fp in sorted(log_files):\n",
        "        dfs.append(_load_single_mhealth_log(fp, feature_cols))\n",
        "\n",
        "    full_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    full_df = full_df[full_df[\"label\"] != 0].copy()\n",
        "\n",
        "    full_df.loc[:, \"label\"] = full_df[\"label\"] - 1\n",
        "\n",
        "    return full_df, feature_cols\n",
        "\n",
        "\n",
        "def create_mhealth_windows(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: list[str],\n",
        "    window_size: int,\n",
        "    step_size: int,\n",
        "):\n",
        "    data_arr = df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    labels_arr = df[\"label\"].to_numpy(dtype=np.int64)\n",
        "    L = data_arr.shape[0]\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "    start = 0\n",
        "    while start + window_size <= L:\n",
        "        end = start + window_size\n",
        "        window_x = data_arr[start:end]\n",
        "        window_label = labels_arr[end - 1]\n",
        "        X_list.append(window_x.T)\n",
        "        y_list.append(int(window_label))\n",
        "        start += step_size\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No windows created. Check window_size / step_size / dataset length.\")\n",
        "\n",
        "    X_np = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y_np = np.array(y_list, dtype=np.int64)\n",
        "    return X_np, y_np\n",
        "\n",
        "\n",
        "class MHEALTHDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, window_size: int = 128, step_size: int = 64):\n",
        "        super().__init__()\n",
        "\n",
        "        full_df, feature_cols = load_mhealth_dataframe(data_dir)\n",
        "        X, y = create_mhealth_windows(full_df, feature_cols, window_size, step_size)\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.subjects = np.zeros(len(self.y), dtype=int)\n",
        "\n",
        "        self.label_names = [\n",
        "            \"Standing still\", \"Sitting and relaxing\", \"Lying down\",\n",
        "            \"Walking\", \"Climbing stairs\", \"Waist bends forward\",\n",
        "            \"Frontal elevation of arms\", \"Knees bending\", \"Cycling\",\n",
        "            \"Jogging\", \"Running\", \"Jump front & back\",\n",
        "        ]\n",
        "\n",
        "        print(\"Loaded MHEALTH dataset\")\n",
        "        print(f\"X shape : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"Classes : {len(self.label_names)}\")\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]\n",
        "        N, C, T = Xtr.shape\n",
        "        X2 = Xtr.transpose(0, 2, 1).reshape(-1, C)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = self.scaler\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = X.transpose(0, 2, 1).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        self.X = X2.reshape(N, T, C).transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx]).long(),\n",
        "            int(self.subjects[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions (SNR=10 uses this)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate  (★ Variant behavior matches \"예전버전\")\n",
        "#   - use_ste=True  : train=STE(hard fwd, soft bwd), eval=hard onehot\n",
        "#   - use_ste=False : always soft_probs (train/eval 동일)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits = self.gate(x)  # (B,K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx = logits.argmax(dim=-1)\n",
        "                hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                # STE: forward=hard, backward=soft\n",
        "                degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(\n",
        "                    logits.argmax(dim=-1), num_classes=self.max_degree\n",
        "                ).float()\n",
        "        else:\n",
        "            # Gate w/o STE: always soft (train/eval 동일)\n",
        "            degree_w = soft_probs\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block (Ablation switches ONLY; ★ Variant behavior matches \"예전버전\")\n",
        "#   - w/o Gate: fixed_degree (1..K) → build up to d, output Z[d-1]\n",
        "#   - Gate w/o STE: soft weighted sum of ALL degrees (train/eval 동일)\n",
        "#   - w/o Hadamard: prefix sum (Z[i]=Z[i-1]+Y[i])  (예전버전)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlockAblation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,      # 1..K if w/o Gate (fixed)\n",
        "                 use_ste=True,           # Gate w/o STE (soft routing)\n",
        "                 use_hadamard=True,      # w/o Hadamard (prefix-sum)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_gate = bool(use_gate)\n",
        "        self.fixed_degree = fixed_degree  # None or int in [1..K]\n",
        "        self.use_ste = bool(use_ste)\n",
        "        self.use_hadamard = bool(use_hadamard)\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _build_Y(self, x, max_deg):\n",
        "        return [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        \"\"\"\n",
        "        - use_hadamard=True : 예전버전(원본) Hadamard chain\n",
        "            Z0=Y0, Zi = pre(Z_{i-1}) * Yi\n",
        "        - use_hadamard=False: 예전버전 w/o Hadamard (prefix sum)\n",
        "            Z0=Y0, Zi = Z_{i-1} + Yi\n",
        "        \"\"\"\n",
        "        Y = self._build_Y(x, max_deg)\n",
        "\n",
        "        if self.use_hadamard:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(Z_ * Y[i])\n",
        "            return Z\n",
        "        else:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "            return Z\n",
        "\n",
        "    def _hard_select(self, Z_list, sel):\n",
        "        B = Z_list[0].shape[0]\n",
        "        Z_stack = torch.stack(Z_list, dim=0)  # (K,B,C,T)\n",
        "        return Z_stack[sel, torch.arange(B, device=Z_stack.device)]\n",
        "\n",
        "    def _soft_weighted_output(self, x, soft_probs):\n",
        "        \"\"\"\n",
        "        예전버전 Gate w/o STE:\n",
        "          - always compute ALL K degrees\n",
        "          - weighted sum with soft_probs\n",
        "          - hadamard / no_hadamard build rule은 동일하게 적용\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        Z = self._build_Z(x, max_deg=self.max_degree)      # list length K, each (B,C,T)\n",
        "        Z_stack = torch.stack(Z, dim=1)                    # (B,K,C,T)\n",
        "        w = soft_probs.view(B, self.max_degree, 1, 1)      # (B,K,1,1)\n",
        "        out = (Z_stack * w).sum(dim=1)                     # (B,C,T)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # ---- Case A) w/o Gate (fixed degree 1..K) ----\n",
        "        if (not self.use_gate) or (self.fixed_degree is not None):\n",
        "            d = int(self.fixed_degree) if self.fixed_degree is not None else self.max_degree\n",
        "            d = max(1, min(d, self.max_degree))\n",
        "\n",
        "            # build only up to d, output \"degree d\" path (예전버전)\n",
        "            Z = self._build_Z(x, max_deg=d)\n",
        "            out = Z[-1]\n",
        "\n",
        "            # stats payload (예전버전 스타일)\n",
        "            sel = torch.full((B,), d - 1, device=x.device, dtype=torch.long)\n",
        "            K = self.max_degree\n",
        "            sp = F.one_hot(sel, num_classes=K).float()\n",
        "            dw = sp\n",
        "            logits = sp\n",
        "\n",
        "            out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            if return_gate_info:\n",
        "                return out, {\n",
        "                    \"degree_selection\": dw,\n",
        "                    \"soft_probs\": sp,\n",
        "                    \"logits\": logits,\n",
        "                    \"compute_cost\": float(d),\n",
        "                }\n",
        "            return out\n",
        "\n",
        "        # ---- Case B) Gate ON ----\n",
        "        degree_w, logits, soft_probs = self.degree_gate(x, use_ste=self.use_ste)\n",
        "\n",
        "        if (not self.use_ste):\n",
        "            # 예전버전: Gate w/o STE는 train/eval 관계없이 ALWAYS soft weighted sum\n",
        "            out = self._soft_weighted_output(x, degree_w)  # degree_w == soft_probs\n",
        "            # (stats only) 대표 degree\n",
        "            selected = soft_probs.argmax(dim=-1)\n",
        "            # compute_cost (예전코드에선 argmax 기반이었지만, 여기서는 gi에만 들어가므로 그대로 둠)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "        else:\n",
        "            # 원본: hard select (STE는 train에서 degree_w에 반영됨)\n",
        "            selected = degree_w.argmax(dim=-1)\n",
        "            max_deg = max(1, min(int(selected.max().item()) + 1, self.max_degree))\n",
        "            Z = self._build_Z(x, max_deg=max_deg)\n",
        "            out = self._hard_select(Z, selected)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_w,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": compute_cost,\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model (Ablation switches ONLY; otherwise matches your logic)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR_Ablation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,     # for w/o Gate (1/2/3)\n",
        "                 use_ste=True,          # Gate w/o STE\n",
        "                 use_hadamard=True,     # w/o Hadamard\n",
        "                 use_ffn=True,          # w/o FFN\n",
        "                 use_residual=True,     # w/o Residual\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_ffn = bool(use_ffn)\n",
        "        self.use_residual = bool(use_residual)\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlockAblation(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                token_kernel=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "                use_gate=use_gate,\n",
        "                fixed_degree=fixed_degree,\n",
        "                use_ste=use_ste,\n",
        "                use_hadamard=use_hadamard,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms1[i], x + res)\n",
        "            else:\n",
        "                x = self._ln(self.norms1[i], x)\n",
        "\n",
        "            res2 = x\n",
        "            if self.use_ffn:\n",
        "                x = self.ffn[i](x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms2[i], x + res2)\n",
        "            else:\n",
        "                x = self._ln(self.norms2[i], x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Table Metrics (keep as-is in your \"now code\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_gate_stats(model, loader, device, snr_db=None, max_degree=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      macro_f1, degree_entropy, norm_comp\n",
        "    Definitions:\n",
        "      - degree_entropy: mean over (layers, samples) of normalized entropy of soft_probs\n",
        "                        H(p)/log(K)  where K=max_degree\n",
        "      - norm_comp: mean expected degree / max_degree, averaged over layers\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    ent_sum = 0.0\n",
        "    ent_count = 0\n",
        "    comp_sum = 0.0\n",
        "    comp_count = 0\n",
        "\n",
        "    eps = 1e-12\n",
        "    logK = float(np.log(max_degree))\n",
        "\n",
        "    deg_vals = torch.arange(1, max_degree + 1, device=device).float()\n",
        "\n",
        "    for X, y, _ in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "        # gate stats\n",
        "        for gi in gate_info_list:\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            ent = -(sp * (sp + eps).log()).sum(dim=-1) / logK\n",
        "            ent_sum += ent.mean().item()\n",
        "            ent_count += 1\n",
        "\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "            comp_sum += (exp_deg / max_degree)\n",
        "            comp_count += 1\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    macro_f1 = float(f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    degree_entropy = float(ent_sum / max(ent_count, 1))\n",
        "    norm_comp = float(comp_sum / max(comp_count, 1))\n",
        "    return macro_f1, degree_entropy, norm_comp\n",
        "\n",
        "\n",
        "def format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp):\n",
        "    drop_pct = 100.0 * (clean_f1 - snr10_f1) / max(clean_f1, 1e-12)\n",
        "    return {\n",
        "        \"Variant\": name,\n",
        "        \"CleanF1\": clean_f1,\n",
        "        \"SNR10F1\": snr10_f1,\n",
        "        \"drop(%)\": drop_pct,\n",
        "        \"DegreeEntropy\": deg_ent,\n",
        "        \"NormComp\": norm_comp,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    header = [\"Variant\", \"Clean F1\", \"(SNR=10) F1\", \"drop(%)\", \"Degree Entropy\", \"NormComp\"]\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"UCI-HAR Ablation Table\")\n",
        "    print(\"=\"*110)\n",
        "    print(f\"{header[0]:<22s} | {header[1]:>8s} | {header[2]:>11s} | {header[3]:>7s} | {header[4]:>14s} | {header[5]:>8s}\")\n",
        "    print(\"-\"*110)\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']:<22s} | \"\n",
        "            f\"{r['CleanF1']:>8.4f} | \"\n",
        "            f\"{r['SNR10F1']:>11.4f} | \"\n",
        "            f\"{r['drop(%)']:>7.2f} | \"\n",
        "            f\"{r['DegreeEntropy']:>14.4f} | \"\n",
        "            f\"{r['NormComp']:>8.4f}\"\n",
        "        )\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    print(\"\\n[LaTeX rows]\")\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']} & {r['CleanF1']:.4f} & {r['SNR10F1']:.4f} & \"\n",
        "            f\"{r['drop(%)']:.2f} & {r['DegreeEntropy']:.4f} & {r['NormComp']:.4f} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Experiment Runner (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def build_variant(name, cfg):\n",
        "    model = PADReHAR_Ablation(\n",
        "        in_channels=cfg[\"in_channels\"],\n",
        "        seq_len=cfg[\"seq_len\"],\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_layers=cfg[\"num_layers\"],\n",
        "        max_degree=cfg[\"max_degree\"],\n",
        "        gate_hidden_dim=cfg[\"gate_hidden_dim\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        temperature_initial=cfg[\"temperature_initial\"],\n",
        "        temperature_min=cfg[\"temperature_min\"],\n",
        "        use_gate=cfg.get(\"use_gate\", True),\n",
        "        fixed_degree=cfg.get(\"fixed_degree\", None),\n",
        "        use_ste=cfg.get(\"use_ste\", True),\n",
        "        use_hadamard=cfg.get(\"use_hadamard\", True),\n",
        "        use_ffn=cfg.get(\"use_ffn\", True),\n",
        "        use_residual=cfg.get(\"use_residual\", True),\n",
        "    ).to(cfg[\"device\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg):\n",
        "    variants = []\n",
        "    variants.append((\"Full (Ours)\", dict()))\n",
        "    variants.append((\"w/o Gate-1\", dict(use_gate=False, fixed_degree=1)))\n",
        "    variants.append((\"w/o Gate-2\", dict(use_gate=False, fixed_degree=2)))\n",
        "    variants.append((\"w/o Gate-3\", dict(use_gate=False, fixed_degree=3)))\n",
        "    variants.append((\"Gate w/o STE\", dict(use_gate=True, use_ste=False)))\n",
        "    variants.append((\"w/o Hadamard\", dict(use_hadamard=False)))\n",
        "    variants.append((\"w/o FFN\", dict(use_ffn=False)))\n",
        "    variants.append((\"w/o Residual\", dict(use_residual=False)))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for name, delta in variants:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[Running Variant] {name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        cfg = copy.deepcopy(base_cfg)\n",
        "        cfg.update(delta)\n",
        "\n",
        "        set_seed(train_cfg[\"seed\"])\n",
        "        model = build_variant(name, cfg)\n",
        "        print(f\"Model params: {count_parameters(model):,}\")\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            cfg[\"device\"],\n",
        "            lr=train_cfg[\"lr\"],\n",
        "            weight_decay=train_cfg[\"wd\"],\n",
        "            epochs=train_cfg[\"epochs\"],\n",
        "            seed=train_cfg[\"seed\"],\n",
        "        )\n",
        "\n",
        "        clean_f1, deg_ent, norm_comp = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=None, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "        snr10_f1, _, _ = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=10.0, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "\n",
        "        rows.append(format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH =  \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 10\n",
        "\n",
        "    NUM_CLASSES = 12\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.3\n",
        "    LR = 2e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE = 50\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = MHEALTHDataset(DATA_PATH, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    base_cfg = dict(\n",
        "        device=DEVICE,\n",
        "        in_channels=15,\n",
        "        seq_len=100,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "        dropout=DROPOUT,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        use_gate=True,\n",
        "        fixed_degree=None,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    )\n",
        "\n",
        "    train_cfg = dict(\n",
        "        seed=SEED,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        wd=WD,\n",
        "    )\n",
        "\n",
        "    rows = run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg)\n",
        "    print_table(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OL7esDTvfEo",
        "outputId": "f44c3e3f-7c50-46cb-c325-49538036865f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "Found 10 log files in /content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\n",
            "Loaded MHEALTH dataset\n",
            "X shape : (6862, 15, 100)  (N, C, T)\n",
            "y shape : (6862,)  (N,)\n",
            "Classes : 12\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Full (Ours)\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/10 | LR=0.0010 | Train Loss=0.1677 | TestF1=0.9790 | BestF1=0.9790 | Temp=3.141\n",
            "Epoch 10/10 | LR=0.0000 | Train Loss=0.1242 | TestF1=0.9846 | BestF1=0.9853 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9853\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-1\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/10 | LR=0.0010 | Train Loss=0.2305 | TestF1=0.9784 | BestF1=0.9784 | Temp=3.141\n",
            "Epoch 10/10 | LR=0.0000 | Train Loss=0.1574 | TestF1=0.9825 | BestF1=0.9825 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9825\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-2\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/10 | LR=0.0010 | Train Loss=0.1653 | TestF1=0.9803 | BestF1=0.9803 | Temp=3.141\n",
            "Epoch 10/10 | LR=0.0000 | Train Loss=0.1235 | TestF1=0.9853 | BestF1=0.9853 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9853\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-3\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/10 | LR=0.0010 | Train Loss=0.1582 | TestF1=0.9832 | BestF1=0.9832 | Temp=3.141\n",
            "Epoch 10/10 | LR=0.0000 | Train Loss=0.1211 | TestF1=0.9846 | BestF1=0.9846 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9846\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Gate w/o STE\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/10 | LR=0.0010 | Train Loss=0.1656 | TestF1=0.9833 | BestF1=0.9833 | Temp=3.141\n",
            "Epoch 10/10 | LR=0.0000 | Train Loss=0.1283 | TestF1=0.9817 | BestF1=0.9833 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9833\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Hadamard\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/10 | LR=0.0010 | Train Loss=0.2282 | TestF1=0.9785 | BestF1=0.9785 | Temp=3.141\n",
            "Epoch 10/10 | LR=0.0000 | Train Loss=0.1557 | TestF1=0.9803 | BestF1=0.9810 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9810\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o FFN\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/10 | LR=0.0010 | Train Loss=0.1676 | TestF1=0.9784 | BestF1=0.9784 | Temp=3.141\n",
            "Epoch 10/10 | LR=0.0000 | Train Loss=0.1343 | TestF1=0.9825 | BestF1=0.9825 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9825\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Residual\n",
            "================================================================================\n",
            "Model params: 79,173\n",
            "Epoch 05/10 | LR=0.0010 | Train Loss=0.3564 | TestF1=0.9608 | BestF1=0.9608 | Temp=3.141\n",
            "Epoch 10/10 | LR=0.0000 | Train Loss=0.1573 | TestF1=0.9832 | BestF1=0.9832 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9832\n",
            "\n",
            "==============================================================================================================\n",
            "UCI-HAR Ablation Table\n",
            "==============================================================================================================\n",
            "Variant                | Clean F1 | (SNR=10) F1 | drop(%) | Degree Entropy | NormComp\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Full (Ours)            |   0.9853 |      0.9854 |   -0.00 |         0.9826 |   0.6664\n",
            "w/o Gate-1             |   0.9825 |      0.9783 |    0.43 |         0.0000 |   0.3333\n",
            "w/o Gate-2             |   0.9853 |      0.9840 |    0.14 |         0.0000 |   0.6667\n",
            "w/o Gate-3             |   0.9846 |      0.9853 |   -0.07 |         0.0000 |   1.0000\n",
            "Gate w/o STE           |   0.9833 |      0.9812 |    0.21 |         0.9757 |   0.6989\n",
            "w/o Hadamard           |   0.9810 |      0.9804 |    0.07 |         0.9470 |   0.6670\n",
            "w/o FFN                |   0.9825 |      0.9825 |    0.00 |         0.9829 |   0.6671\n",
            "w/o Residual           |   0.9832 |      0.9812 |    0.21 |         0.9352 |   0.6625\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[LaTeX rows]\n",
            "Full (Ours) & 0.9853 & 0.9854 & -0.00 & 0.9826 & 0.6664 \\\\\n",
            "w/o Gate-1 & 0.9825 & 0.9783 & 0.43 & 0.0000 & 0.3333 \\\\\n",
            "w/o Gate-2 & 0.9853 & 0.9840 & 0.14 & 0.0000 & 0.6667 \\\\\n",
            "w/o Gate-3 & 0.9846 & 0.9853 & -0.07 & 0.0000 & 1.0000 \\\\\n",
            "Gate w/o STE & 0.9833 & 0.9812 & 0.21 & 0.9757 & 0.6989 \\\\\n",
            "w/o Hadamard & 0.9810 & 0.9804 & 0.07 & 0.9470 & 0.6670 \\\\\n",
            "w/o FFN & 0.9825 & 0.9825 & 0.00 & 0.9829 & 0.6671 \\\\\n",
            "w/o Residual & 0.9832 & 0.9812 & 0.21 & 0.9352 & 0.6625 \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WISDM"
      ],
      "metadata": {
        "id": "Ve_FfVAlwYC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class WISDMDataset(Dataset):\n",
        "    def __init__(self, file_path: str, window_size: int = 80, step_size: int = 40):\n",
        "        super().__init__()\n",
        "        self.file_path = file_path\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            raise FileNotFoundError(f\"WISDM txt file not found: {file_path}\")\n",
        "\n",
        "        df = self._load_file(file_path)\n",
        "        self.X, self.y, self.subjects = self._create_windows(df)\n",
        "        self.unique_subjects = sorted(np.unique(self.subjects))\n",
        "\n",
        "        self.n_classes = int(len(np.unique(self.y)))\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Loaded WISDM dataset (single txt)\")\n",
        "        print(f\"  X shape       : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"  y shape       : {self.y.shape}  (N,)\")\n",
        "        print(f\"  subjects shape: {self.subjects.shape} (N,)\")\n",
        "        print(f\"  num classes   : {self.n_classes}\")\n",
        "        print(f\"  unique subjects: {self.unique_subjects[:10]} ... (total {len(self.unique_subjects)})\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def _load_file(self, file_path: str) -> pd.DataFrame:\n",
        "        WISDM_LABEL_MAP = {\n",
        "            \"walking\": 0,\n",
        "            \"jogging\": 1,\n",
        "            \"sitting\": 2,\n",
        "            \"standing\": 3,\n",
        "            \"upstairs\": 4,\n",
        "            \"downstairs\": 5,\n",
        "        }\n",
        "\n",
        "        with open(file_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        rows = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            line = line.replace(\";\", \"\")\n",
        "            parts = line.split(\",\")\n",
        "\n",
        "            if len(parts) != 6:\n",
        "                continue\n",
        "\n",
        "            subj, act, ts, x, y, z = parts\n",
        "            if x.strip() == \"\" or y.strip() == \"\" or z.strip() == \"\":\n",
        "                continue\n",
        "\n",
        "            act_norm = act.strip().lower()\n",
        "            if act_norm not in WISDM_LABEL_MAP:\n",
        "                continue\n",
        "\n",
        "            rows.append([subj, act_norm, ts, x, y, z])\n",
        "\n",
        "        if not rows:\n",
        "            raise ValueError(f\"No valid rows parsed from file: {file_path}\")\n",
        "\n",
        "        df = pd.DataFrame(rows, columns=[\"subject\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"])\n",
        "        df = df.replace([\"\", \"NaN\", \"nan\"], np.nan).dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        df[\"subject\"] = pd.to_numeric(df[\"subject\"], errors=\"coerce\")\n",
        "        df[\"x\"] = pd.to_numeric(df[\"x\"], errors=\"coerce\")\n",
        "        df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\")\n",
        "        df[\"z\"] = pd.to_numeric(df[\"z\"], errors=\"coerce\")\n",
        "        df = df.dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "        if df.empty:\n",
        "            raise ValueError(\"After cleaning, WISDM DataFrame is empty. Check file format.\")\n",
        "\n",
        "        df[\"subject\"] = df[\"subject\"].astype(int)\n",
        "        df[\"activity_id\"] = df[\"activity\"].map(WISDM_LABEL_MAP).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_windows(self, df: pd.DataFrame):\n",
        "        X_list, y_list, s_list = [], [], []\n",
        "\n",
        "        for subj_id in sorted(df[\"subject\"].unique()):\n",
        "            df_sub = df[df[\"subject\"] == subj_id]\n",
        "            data = df_sub[[\"x\", \"y\", \"z\"]].to_numpy(dtype=np.float32)\n",
        "            labels = df_sub[\"activity_id\"].to_numpy(dtype=np.int64)\n",
        "            L = len(df_sub)\n",
        "\n",
        "            start = 0\n",
        "            while start + self.window_size <= L:\n",
        "                end = start + self.window_size\n",
        "                window_x = data[start:end]\n",
        "                window_y = labels[end - 1]\n",
        "\n",
        "                X_list.append(window_x.T)\n",
        "                y_list.append(window_y)\n",
        "                s_list.append(subj_id)\n",
        "\n",
        "                start += self.step_size\n",
        "\n",
        "        if len(X_list) == 0:\n",
        "            raise ValueError(\"[WISDMDataset] No windows created. Try smaller window_size or check data.\")\n",
        "\n",
        "        X = np.stack(X_list, axis=0).astype(np.float32)\n",
        "        y = np.array(y_list, dtype=np.int64)\n",
        "        s = np.array(s_list, dtype=np.int64)\n",
        "        return X, y, s\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]  # (N,C,T)\n",
        "        N, C, T = Xtr.shape\n",
        "        X2 = np.transpose(Xtr, (0, 2, 1)).reshape(-1, C)  # (N*T, C)\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = getattr(self, \"scaler\", None)\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = np.transpose(X, (0, 2, 1)).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        self.X = X2.reshape(N, T, C).transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.FloatTensor(self.X[idx]),\n",
        "            torch.LongTensor([self.y[idx]])[0],\n",
        "            int(self.subjects[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions (SNR=10 uses this)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate  (★ Variant behavior matches \"예전버전\")\n",
        "#   - use_ste=True  : train=STE(hard fwd, soft bwd), eval=hard onehot\n",
        "#   - use_ste=False : always soft_probs (train/eval 동일)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x, use_ste=True):\n",
        "        logits = self.gate(x)  # (B,K)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if use_ste:\n",
        "            if self.training:\n",
        "                hard_idx = logits.argmax(dim=-1)\n",
        "                hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "                # STE: forward=hard, backward=soft\n",
        "                degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "            else:\n",
        "                degree_w = F.one_hot(\n",
        "                    logits.argmax(dim=-1), num_classes=self.max_degree\n",
        "                ).float()\n",
        "        else:\n",
        "            # Gate w/o STE: always soft (train/eval 동일)\n",
        "            degree_w = soft_probs\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block (Ablation switches ONLY; ★ Variant behavior matches \"예전버전\")\n",
        "#   - w/o Gate: fixed_degree (1..K) → build up to d, output Z[d-1]\n",
        "#   - Gate w/o STE: soft weighted sum of ALL degrees (train/eval 동일)\n",
        "#   - w/o Hadamard: prefix sum (Z[i]=Z[i-1]+Y[i])  (예전버전)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlockAblation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,      # 1..K if w/o Gate (fixed)\n",
        "                 use_ste=True,           # Gate w/o STE (soft routing)\n",
        "                 use_hadamard=True,      # w/o Hadamard (prefix-sum)\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_gate = bool(use_gate)\n",
        "        self.fixed_degree = fixed_degree  # None or int in [1..K]\n",
        "        self.use_ste = bool(use_ste)\n",
        "        self.use_hadamard = bool(use_hadamard)\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _build_Y(self, x, max_deg):\n",
        "        return [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        \"\"\"\n",
        "        - use_hadamard=True : 예전버전(원본) Hadamard chain\n",
        "            Z0=Y0, Zi = pre(Z_{i-1}) * Yi\n",
        "        - use_hadamard=False: 예전버전 w/o Hadamard (prefix sum)\n",
        "            Z0=Y0, Zi = Z_{i-1} + Yi\n",
        "        \"\"\"\n",
        "        Y = self._build_Y(x, max_deg)\n",
        "\n",
        "        if self.use_hadamard:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "                Z.append(Z_ * Y[i])\n",
        "            return Z\n",
        "        else:\n",
        "            Z = [Y[0]]\n",
        "            for i in range(1, max_deg):\n",
        "                Z.append(Z[-1] + Y[i])\n",
        "            return Z\n",
        "\n",
        "    def _hard_select(self, Z_list, sel):\n",
        "        B = Z_list[0].shape[0]\n",
        "        Z_stack = torch.stack(Z_list, dim=0)  # (K,B,C,T)\n",
        "        return Z_stack[sel, torch.arange(B, device=Z_stack.device)]\n",
        "\n",
        "    def _soft_weighted_output(self, x, soft_probs):\n",
        "        \"\"\"\n",
        "        예전버전 Gate w/o STE:\n",
        "          - always compute ALL K degrees\n",
        "          - weighted sum with soft_probs\n",
        "          - hadamard / no_hadamard build rule은 동일하게 적용\n",
        "        \"\"\"\n",
        "        B = x.size(0)\n",
        "        Z = self._build_Z(x, max_deg=self.max_degree)      # list length K, each (B,C,T)\n",
        "        Z_stack = torch.stack(Z, dim=1)                    # (B,K,C,T)\n",
        "        w = soft_probs.view(B, self.max_degree, 1, 1)      # (B,K,1,1)\n",
        "        out = (Z_stack * w).sum(dim=1)                     # (B,C,T)\n",
        "        return out\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # ---- Case A) w/o Gate (fixed degree 1..K) ----\n",
        "        if (not self.use_gate) or (self.fixed_degree is not None):\n",
        "            d = int(self.fixed_degree) if self.fixed_degree is not None else self.max_degree\n",
        "            d = max(1, min(d, self.max_degree))\n",
        "\n",
        "            # build only up to d, output \"degree d\" path (예전버전)\n",
        "            Z = self._build_Z(x, max_deg=d)\n",
        "            out = Z[-1]\n",
        "\n",
        "            # stats payload (예전버전 스타일)\n",
        "            sel = torch.full((B,), d - 1, device=x.device, dtype=torch.long)\n",
        "            K = self.max_degree\n",
        "            sp = F.one_hot(sel, num_classes=K).float()\n",
        "            dw = sp\n",
        "            logits = sp\n",
        "\n",
        "            out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            if return_gate_info:\n",
        "                return out, {\n",
        "                    \"degree_selection\": dw,\n",
        "                    \"soft_probs\": sp,\n",
        "                    \"logits\": logits,\n",
        "                    \"compute_cost\": float(d),\n",
        "                }\n",
        "            return out\n",
        "\n",
        "        # ---- Case B) Gate ON ----\n",
        "        degree_w, logits, soft_probs = self.degree_gate(x, use_ste=self.use_ste)\n",
        "\n",
        "        if (not self.use_ste):\n",
        "            # 예전버전: Gate w/o STE는 train/eval 관계없이 ALWAYS soft weighted sum\n",
        "            out = self._soft_weighted_output(x, degree_w)  # degree_w == soft_probs\n",
        "            # (stats only) 대표 degree\n",
        "            selected = soft_probs.argmax(dim=-1)\n",
        "            # compute_cost (예전코드에선 argmax 기반이었지만, 여기서는 gi에만 들어가므로 그대로 둠)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "        else:\n",
        "            # 원본: hard select (STE는 train에서 degree_w에 반영됨)\n",
        "            selected = degree_w.argmax(dim=-1)\n",
        "            max_deg = max(1, min(int(selected.max().item()) + 1, self.max_degree))\n",
        "            Z = self._build_Z(x, max_deg=max_deg)\n",
        "            out = self._hard_select(Z, selected)\n",
        "            compute_cost = (selected + 1).float().mean().item()\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": degree_w,\n",
        "                \"soft_probs\": soft_probs,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": compute_cost,\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model (Ablation switches ONLY; otherwise matches your logic)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR_Ablation(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "                 # ablations\n",
        "                 use_gate=True,\n",
        "                 fixed_degree=None,     # for w/o Gate (1/2/3)\n",
        "                 use_ste=True,          # Gate w/o STE\n",
        "                 use_hadamard=True,     # w/o Hadamard\n",
        "                 use_ffn=True,          # w/o FFN\n",
        "                 use_residual=True,     # w/o Residual\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.use_ffn = bool(use_ffn)\n",
        "        self.use_residual = bool(use_residual)\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlockAblation(\n",
        "                hidden_dim, seq_len,\n",
        "                max_degree=max_degree,\n",
        "                token_kernel=11,\n",
        "                gate_hidden_dim=gate_hidden_dim,\n",
        "                temperature_initial=temperature_initial,\n",
        "                temperature_min=temperature_min,\n",
        "                use_gate=use_gate,\n",
        "                fixed_degree=fixed_degree,\n",
        "                use_ste=use_ste,\n",
        "                use_hadamard=use_hadamard,\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms1[i], x + res)\n",
        "            else:\n",
        "                x = self._ln(self.norms1[i], x)\n",
        "\n",
        "            res2 = x\n",
        "            if self.use_ffn:\n",
        "                x = self.ffn[i](x)\n",
        "\n",
        "            if self.use_residual:\n",
        "                x = self._ln(self.norms2[i], x + res2)\n",
        "            else:\n",
        "                x = self._ln(self.norms2[i], x)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Table Metrics (keep as-is in your \"now code\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "@torch.no_grad()\n",
        "def eval_f1_and_gate_stats(model, loader, device, snr_db=None, max_degree=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      macro_f1, degree_entropy, norm_comp\n",
        "    Definitions:\n",
        "      - degree_entropy: mean over (layers, samples) of normalized entropy of soft_probs\n",
        "                        H(p)/log(K)  where K=max_degree\n",
        "      - norm_comp: mean expected degree / max_degree, averaged over layers\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "    ent_sum = 0.0\n",
        "    ent_count = 0\n",
        "    comp_sum = 0.0\n",
        "    comp_count = 0\n",
        "\n",
        "    eps = 1e-12\n",
        "    logK = float(np.log(max_degree))\n",
        "\n",
        "    deg_vals = torch.arange(1, max_degree + 1, device=device).float()\n",
        "\n",
        "    for X, y, _ in loader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        if snr_db is not None:\n",
        "            X = add_gaussian_noise(X, float(snr_db))\n",
        "\n",
        "        logits, gate_info_list, _ = model(X, return_gate_info=True)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        all_preds.append(preds.detach().cpu().numpy())\n",
        "        all_labels.append(y.detach().cpu().numpy())\n",
        "\n",
        "        # gate stats\n",
        "        for gi in gate_info_list:\n",
        "            sp = gi[\"soft_probs\"]  # (B,K)\n",
        "            ent = -(sp * (sp + eps).log()).sum(dim=-1) / logK\n",
        "            ent_sum += ent.mean().item()\n",
        "            ent_count += 1\n",
        "\n",
        "            exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "            comp_sum += (exp_deg / max_degree)\n",
        "            comp_count += 1\n",
        "\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    macro_f1 = float(f1_score(all_labels, all_preds, average=\"macro\"))\n",
        "\n",
        "    degree_entropy = float(ent_sum / max(ent_count, 1))\n",
        "    norm_comp = float(comp_sum / max(comp_count, 1))\n",
        "    return macro_f1, degree_entropy, norm_comp\n",
        "\n",
        "\n",
        "def format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp):\n",
        "    drop_pct = 100.0 * (clean_f1 - snr10_f1) / max(clean_f1, 1e-12)\n",
        "    return {\n",
        "        \"Variant\": name,\n",
        "        \"CleanF1\": clean_f1,\n",
        "        \"SNR10F1\": snr10_f1,\n",
        "        \"drop(%)\": drop_pct,\n",
        "        \"DegreeEntropy\": deg_ent,\n",
        "        \"NormComp\": norm_comp,\n",
        "    }\n",
        "\n",
        "\n",
        "def print_table(rows):\n",
        "    header = [\"Variant\", \"Clean F1\", \"(SNR=10) F1\", \"drop(%)\", \"Degree Entropy\", \"NormComp\"]\n",
        "    print(\"\\n\" + \"=\"*110)\n",
        "    print(\"UCI-HAR Ablation Table\")\n",
        "    print(\"=\"*110)\n",
        "    print(f\"{header[0]:<22s} | {header[1]:>8s} | {header[2]:>11s} | {header[3]:>7s} | {header[4]:>14s} | {header[5]:>8s}\")\n",
        "    print(\"-\"*110)\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']:<22s} | \"\n",
        "            f\"{r['CleanF1']:>8.4f} | \"\n",
        "            f\"{r['SNR10F1']:>11.4f} | \"\n",
        "            f\"{r['drop(%)']:>7.2f} | \"\n",
        "            f\"{r['DegreeEntropy']:>14.4f} | \"\n",
        "            f\"{r['NormComp']:>8.4f}\"\n",
        "        )\n",
        "    print(\"-\"*110)\n",
        "\n",
        "    print(\"\\n[LaTeX rows]\")\n",
        "    for r in rows:\n",
        "        print(\n",
        "            f\"{r['Variant']} & {r['CleanF1']:.4f} & {r['SNR10F1']:.4f} & \"\n",
        "            f\"{r['drop(%)']:.2f} & {r['DegreeEntropy']:.4f} & {r['NormComp']:.4f} \\\\\\\\\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Experiment Runner (unchanged)\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def build_variant(name, cfg):\n",
        "    model = PADReHAR_Ablation(\n",
        "        in_channels=cfg[\"in_channels\"],\n",
        "        seq_len=cfg[\"seq_len\"],\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        hidden_dim=cfg[\"hidden_dim\"],\n",
        "        num_layers=cfg[\"num_layers\"],\n",
        "        max_degree=cfg[\"max_degree\"],\n",
        "        gate_hidden_dim=cfg[\"gate_hidden_dim\"],\n",
        "        dropout=cfg[\"dropout\"],\n",
        "        temperature_initial=cfg[\"temperature_initial\"],\n",
        "        temperature_min=cfg[\"temperature_min\"],\n",
        "        use_gate=cfg.get(\"use_gate\", True),\n",
        "        fixed_degree=cfg.get(\"fixed_degree\", None),\n",
        "        use_ste=cfg.get(\"use_ste\", True),\n",
        "        use_hadamard=cfg.get(\"use_hadamard\", True),\n",
        "        use_ffn=cfg.get(\"use_ffn\", True),\n",
        "        use_residual=cfg.get(\"use_residual\", True),\n",
        "    ).to(cfg[\"device\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg):\n",
        "    variants = []\n",
        "    variants.append((\"Full (Ours)\", dict()))\n",
        "    variants.append((\"w/o Gate-1\", dict(use_gate=False, fixed_degree=1)))\n",
        "    variants.append((\"w/o Gate-2\", dict(use_gate=False, fixed_degree=2)))\n",
        "    variants.append((\"w/o Gate-3\", dict(use_gate=False, fixed_degree=3)))\n",
        "    variants.append((\"Gate w/o STE\", dict(use_gate=True, use_ste=False)))\n",
        "    variants.append((\"w/o Hadamard\", dict(use_hadamard=False)))\n",
        "    variants.append((\"w/o FFN\", dict(use_ffn=False)))\n",
        "    variants.append((\"w/o Residual\", dict(use_residual=False)))\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for name, delta in variants:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"[Running Variant] {name}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        cfg = copy.deepcopy(base_cfg)\n",
        "        cfg.update(delta)\n",
        "\n",
        "        set_seed(train_cfg[\"seed\"])\n",
        "        model = build_variant(name, cfg)\n",
        "        print(f\"Model params: {count_parameters(model):,}\")\n",
        "\n",
        "        model = train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            test_loader,\n",
        "            cfg[\"device\"],\n",
        "            lr=train_cfg[\"lr\"],\n",
        "            weight_decay=train_cfg[\"wd\"],\n",
        "            epochs=train_cfg[\"epochs\"],\n",
        "            seed=train_cfg[\"seed\"],\n",
        "        )\n",
        "\n",
        "        clean_f1, deg_ent, norm_comp = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=None, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "        snr10_f1, _, _ = eval_f1_and_gate_stats(\n",
        "            model, test_loader, cfg[\"device\"], snr_db=10.0, max_degree=cfg[\"max_degree\"]\n",
        "        )\n",
        "\n",
        "        rows.append(format_row(name, clean_f1, snr10_f1, deg_ent, norm_comp))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH =  \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/WISDM_ar_v1.1_raw.txt\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 20\n",
        "\n",
        "    NUM_CLASSES = 6\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.3\n",
        "    LR = 2e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 80\n",
        "    STEP_SIZE = 40\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = WISDMDataset(DATA_PATH, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    base_cfg = dict(\n",
        "        device=DEVICE,\n",
        "        in_channels=3,\n",
        "        seq_len=80,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        max_degree=MAX_DEGREE,\n",
        "        gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "        dropout=DROPOUT,\n",
        "        temperature_initial=5.0,\n",
        "        temperature_min=0.5,\n",
        "        use_gate=True,\n",
        "        fixed_degree=None,\n",
        "        use_ste=True,\n",
        "        use_hadamard=True,\n",
        "        use_ffn=True,\n",
        "        use_residual=True,\n",
        "    )\n",
        "\n",
        "    train_cfg = dict(\n",
        "        seed=SEED,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LR,\n",
        "        wd=WD,\n",
        "    )\n",
        "\n",
        "    rows = run_ablation_suite(train_loader, test_loader, base_cfg, train_cfg)\n",
        "    print_table(rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "494zVv0Ywam_",
        "outputId": "098adece-3b09-4aea-c2ca-0e079bc514ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "================================================================================\n",
            "Loaded WISDM dataset (single txt)\n",
            "  X shape       : (27108, 3, 80)  (N, C, T)\n",
            "  y shape       : (27108,)  (N,)\n",
            "  subjects shape: (27108,) (N,)\n",
            "  num classes   : 6\n",
            "  unique subjects: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)] ... (total 36)\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Full (Ours)\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0017 | Train Loss=0.1342 | TestF1=0.9624 | BestF1=0.9629 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0010 | Train Loss=0.0937 | TestF1=0.9745 | BestF1=0.9745 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0003 | Train Loss=0.0781 | TestF1=0.9767 | BestF1=0.9791 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0745 | TestF1=0.9793 | BestF1=0.9793 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9793\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-1\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0017 | Train Loss=0.1915 | TestF1=0.9467 | BestF1=0.9467 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0010 | Train Loss=0.1451 | TestF1=0.9635 | BestF1=0.9635 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0003 | Train Loss=0.1194 | TestF1=0.9698 | BestF1=0.9698 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.1107 | TestF1=0.9746 | BestF1=0.9746 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9746\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-2\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0017 | Train Loss=0.1298 | TestF1=0.9664 | BestF1=0.9664 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0010 | Train Loss=0.0932 | TestF1=0.9728 | BestF1=0.9728 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0003 | Train Loss=0.0775 | TestF1=0.9750 | BestF1=0.9750 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0732 | TestF1=0.9757 | BestF1=0.9758 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9758\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Gate-3\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0017 | Train Loss=0.1264 | TestF1=0.9614 | BestF1=0.9614 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0010 | Train Loss=0.0888 | TestF1=0.9711 | BestF1=0.9745 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0003 | Train Loss=0.0743 | TestF1=0.9769 | BestF1=0.9769 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0721 | TestF1=0.9778 | BestF1=0.9778 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9778\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] Gate w/o STE\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0017 | Train Loss=0.1292 | TestF1=0.9630 | BestF1=0.9650 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0010 | Train Loss=0.0869 | TestF1=0.9751 | BestF1=0.9751 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0003 | Train Loss=0.0746 | TestF1=0.9761 | BestF1=0.9761 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0719 | TestF1=0.9782 | BestF1=0.9782 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9782\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Hadamard\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0017 | Train Loss=0.1902 | TestF1=0.9499 | BestF1=0.9499 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0010 | Train Loss=0.1410 | TestF1=0.9558 | BestF1=0.9636 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0003 | Train Loss=0.1182 | TestF1=0.9698 | BestF1=0.9715 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.1102 | TestF1=0.9724 | BestF1=0.9724 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9724\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o FFN\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0017 | Train Loss=0.1207 | TestF1=0.9664 | BestF1=0.9664 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0010 | Train Loss=0.0850 | TestF1=0.9746 | BestF1=0.9746 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0003 | Train Loss=0.0735 | TestF1=0.9779 | BestF1=0.9781 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0717 | TestF1=0.9775 | BestF1=0.9781 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9781\n",
            "\n",
            "================================================================================\n",
            "[Running Variant] w/o Residual\n",
            "================================================================================\n",
            "Model params: 78,303\n",
            "Epoch 05/20 | LR=0.0017 | Train Loss=0.1798 | TestF1=0.9505 | BestF1=0.9505 | Temp=4.526\n",
            "Epoch 10/20 | LR=0.0010 | Train Loss=0.1319 | TestF1=0.9659 | BestF1=0.9659 | Temp=2.936\n",
            "Epoch 15/20 | LR=0.0003 | Train Loss=0.1030 | TestF1=0.9731 | BestF1=0.9731 | Temp=1.226\n",
            "Epoch 20/20 | LR=0.0000 | Train Loss=0.0955 | TestF1=0.9716 | BestF1=0.9731 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9731\n",
            "\n",
            "==============================================================================================================\n",
            "UCI-HAR Ablation Table\n",
            "==============================================================================================================\n",
            "Variant                | Clean F1 | (SNR=10) F1 | drop(%) | Degree Entropy | NormComp\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "Full (Ours)            |   0.9793 |      0.9226 |    5.79 |         0.9308 |   0.6580\n",
            "w/o Gate-1             |   0.9746 |      0.9411 |    3.44 |         0.0000 |   0.3333\n",
            "w/o Gate-2             |   0.9758 |      0.9037 |    7.38 |         0.0000 |   0.6667\n",
            "w/o Gate-3             |   0.9778 |      0.8071 |   17.46 |         0.0000 |   1.0000\n",
            "Gate w/o STE           |   0.9782 |      0.8418 |   13.95 |         0.6686 |   0.7414\n",
            "w/o Hadamard           |   0.9724 |      0.9492 |    2.39 |         0.9336 |   0.6579\n",
            "w/o FFN                |   0.9781 |      0.9214 |    5.80 |         0.9924 |   0.6618\n",
            "w/o Residual           |   0.9731 |      0.8607 |   11.55 |         0.9858 |   0.6647\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[LaTeX rows]\n",
            "Full (Ours) & 0.9793 & 0.9226 & 5.79 & 0.9308 & 0.6580 \\\\\n",
            "w/o Gate-1 & 0.9746 & 0.9411 & 3.44 & 0.0000 & 0.3333 \\\\\n",
            "w/o Gate-2 & 0.9758 & 0.9037 & 7.38 & 0.0000 & 0.6667 \\\\\n",
            "w/o Gate-3 & 0.9778 & 0.8071 & 17.46 & 0.0000 & 1.0000 \\\\\n",
            "Gate w/o STE & 0.9782 & 0.8418 & 13.95 & 0.6686 & 0.7414 \\\\\n",
            "w/o Hadamard & 0.9724 & 0.9492 & 2.39 & 0.9336 & 0.6579 \\\\\n",
            "w/o FFN & 0.9781 & 0.9214 & 5.80 & 0.9924 & 0.6618 \\\\\n",
            "w/o Residual & 0.9731 & 0.8607 & 11.55 & 0.9858 & 0.6647 \\\\\n"
          ]
        }
      ]
    }
  ]
}