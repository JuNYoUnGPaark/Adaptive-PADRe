{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UCI-HAR"
      ],
      "metadata": {
        "id": "jDQq5CirjoV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obvu1qAriQZx",
        "outputId": "fcb9322a-ff36-4a56-c0fc-ae99e5d995ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "\n",
            "Model params: 78,591\n",
            "\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.1073 | TestF1=0.9187 | BestF1=0.9325 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.0853 | TestF1=0.9088 | BestF1=0.9361 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0701 | TestF1=0.9241 | BestF1=0.9361 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0278 | TestF1=0.9338 | BestF1=0.9382 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0141 | TestF1=0.9471 | BestF1=0.9482 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0060 | TestF1=0.9500 | BestF1=0.9547 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0023 | TestF1=0.9436 | BestF1=0.9547 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0016 | TestF1=0.9469 | BestF1=0.9547 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0027 | TestF1=0.9524 | BestF1=0.9547 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0002 | TestF1=0.9486 | BestF1=0.9547 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9455 | BestF1=0.9547 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0001 | TestF1=0.9454 | BestF1=0.9547 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0012 | TestF1=0.9440 | BestF1=0.9547 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9424 | BestF1=0.9547 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9437 | BestF1=0.9547 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9437 | BestF1=0.9547 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0001 | TestF1=0.9435 | BestF1=0.9547 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0001 | TestF1=0.9438 | BestF1=0.9547 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9438 | BestF1=0.9547 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9438 | BestF1=0.9547 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9547\n",
            "\n",
            "====================================================\n",
            "[Robustness Results] UCI-HAR (Macro-F1)\n",
            "====================================================\n",
            "\n",
            "[Instant noise sweep]\n",
            "  SNR= inf | Macro-F1=0.9547\n",
            "  SNR=  30 | Macro-F1=0.9543\n",
            "  SNR=  20 | Macro-F1=0.9519\n",
            "  SNR=  10 | Macro-F1=0.9446\n",
            "  SNR=   5 | Macro-F1=0.9247\n",
            "  SNR=   0 | Macro-F1=0.8984\n",
            "\n",
            "[LaTeX rows] Instant\n",
            "inf & 0.9547 \\\\\n",
            "30 & 0.9543 \\\\\n",
            "20 & 0.9519 \\\\\n",
            "10 & 0.9446 \\\\\n",
            "5 & 0.9247 \\\\\n",
            "0 & 0.8984 \\\\\n",
            "\n",
            "[Linear SNR drift]\n",
            "  SNR=30.0 | Macro-F1=0.9543\n",
            "  SNR=26.7 | Macro-F1=0.9529\n",
            "  SNR=23.3 | Macro-F1=0.9526\n",
            "  SNR=20.0 | Macro-F1=0.9519\n",
            "  SNR=16.7 | Macro-F1=0.9503\n",
            "  SNR=13.3 | Macro-F1=0.9476\n",
            "  SNR=10.0 | Macro-F1=0.9446\n",
            "  SNR= 6.7 | Macro-F1=0.9337\n",
            "  SNR= 3.3 | Macro-F1=0.9179\n",
            "  SNR= 0.0 | Macro-F1=0.8984\n",
            "\n",
            "[LaTeX rows] Linear\n",
            "30.0 & 0.9543 \\\\\n",
            "26.7 & 0.9529 \\\\\n",
            "23.3 & 0.9526 \\\\\n",
            "20.0 & 0.9519 \\\\\n",
            "16.7 & 0.9503 \\\\\n",
            "13.3 & 0.9476 \\\\\n",
            "10.0 & 0.9446 \\\\\n",
            "6.7 & 0.9337 \\\\\n",
            "3.3 & 0.9179 \\\\\n",
            "0.0 & 0.8984 \\\\\n",
            "\n",
            "[Bias drift]\n",
            "  beta=0.05 | Macro-F1=0.9525\n",
            "  beta=0.10 | Macro-F1=0.9459\n",
            "  beta=0.20 | Macro-F1=0.9133\n",
            "\n",
            "[LaTeX rows] Bias\n",
            "0.05 & 0.9525 \\\\\n",
            "0.10 & 0.9459 \\\\\n",
            "0.20 & 0.9133 \\\\\n"
          ]
        }
      ],
      "source": [
        "import os, copy, random, time\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "CLASS_NAMES = [\"WALK\", \"UP\", \"DOWN\", \"SIT\", \"STAND\", \"LAY\"]\n",
        "\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_dir, split=\"train\", normalize=None):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split    = split\n",
        "        self.X, self.y = self._load_data()\n",
        "        self.X = torch.FloatTensor(self.X)\n",
        "        self.y = torch.LongTensor(self.y) - 1\n",
        "\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def _load_data(self):\n",
        "        split_dir    = self.data_dir / self.split\n",
        "        signal_types = [\n",
        "            \"body_acc_x\",\"body_acc_y\",\"body_acc_z\",\n",
        "            \"body_gyro_x\",\"body_gyro_y\",\"body_gyro_z\",\n",
        "            \"total_acc_x\",\"total_acc_y\",\"total_acc_z\",\n",
        "        ]\n",
        "        signals = []\n",
        "        for st in signal_types:\n",
        "            fname = split_dir / \"Inertial Signals\" / f\"{st}_{self.split}.txt\"\n",
        "            signals.append(np.loadtxt(fname))\n",
        "        X = np.stack(signals, axis=1)\n",
        "        y = np.loadtxt(split_dir / f\"y_{self.split}.txt\", dtype=int)\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):  return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.X[idx]\n",
        "        y = self.y[idx]\n",
        "        if self.normalize is not None:\n",
        "            mean, std = self.normalize\n",
        "            X = (X - mean.squeeze(0)) / std.squeeze(0)\n",
        "        return X, y\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "            Z.append(Z_ * Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        dw, logits, sp = self.degree_gate(x)\n",
        "        sel = dw.argmax(dim=-1)\n",
        "        out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": dw,\n",
        "                \"soft_probs\": sp,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (sel + 1).float().mean().item()\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, token_kernel=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res)\n",
        "\n",
        "            res2 = x\n",
        "            x = self.ffn[i](x)\n",
        "            x = self._ln(self.norms2[i], x + res2)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_train_stats(train_loader, device=\"cpu\", eps=1e-6):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      mean: (C,1) tensor\n",
        "      std : (C,1) tensor\n",
        "    Note:\n",
        "      X shape from loader: (B,C,T)\n",
        "      We compute stats over (B,T) for each channel.\n",
        "    \"\"\"\n",
        "    sum_x = None\n",
        "    sum_x2 = None\n",
        "    n = 0\n",
        "\n",
        "    for X, _ in train_loader:\n",
        "        X = X.to(device)  # (B,C,T)\n",
        "        B, C, T = X.shape\n",
        "        if sum_x is None:\n",
        "            sum_x = torch.zeros(C, device=device)\n",
        "            sum_x2 = torch.zeros(C, device=device)\n",
        "\n",
        "        # sum over batch and time\n",
        "        sum_x  += X.sum(dim=(0, 2))                 # (C,)\n",
        "        sum_x2 += (X * X).sum(dim=(0, 2))           # (C,)\n",
        "        n += B * T\n",
        "\n",
        "    mean = (sum_x / n)                              # (C,)\n",
        "    var  = (sum_x2 / n) - mean * mean               # (C,)\n",
        "    std  = torch.sqrt(torch.clamp(var, min=eps))    # (C,)\n",
        "\n",
        "    # reshape for broadcasting: (1,C,1) or (C,1)\n",
        "    mean = mean.view(1, -1, 1)\n",
        "    std  = std.view(1, -1, 1)\n",
        "    return mean.detach().cpu(), std.detach().cpu()\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "def add_bias_drift(X, beta, mode=\"constant\", per_channel=True):\n",
        "    \"\"\"\n",
        "    beta: drift scale\n",
        "    mode: [\"constant\",\"brownian\"]\n",
        "    \"\"\"\n",
        "    B, C, T = X.shape\n",
        "    device = X.device\n",
        "\n",
        "    if mode == \"constant\":\n",
        "        if per_channel:\n",
        "            b = torch.randn(B, C, 1, device=device) * beta\n",
        "        else:\n",
        "            b = torch.randn(B, 1, 1, device=device) * beta\n",
        "        return X + b\n",
        "\n",
        "    if mode == \"brownian\":\n",
        "        if per_channel:\n",
        "            eps = torch.randn(B, C, T, device=device) * beta\n",
        "        else:\n",
        "            eps = torch.randn(B, 1, T, device=device) * beta\n",
        "        drift = torch.cumsum(eps, dim=-1)\n",
        "        return X + drift\n",
        "\n",
        "    raise ValueError(\"mode must be one of ['constant','brownian']\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# RobustEvaluator: instant SNR sweep, linear SNR drift, bias drift sweep\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class RobustEvaluator:\n",
        "    def __init__(self, model, val_loader, device):\n",
        "        self.model = model\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_once(self, snr_db=None, bias_beta=None, eval_seed=None, bias_mode=\"constant\", measure_gate=True):\n",
        "        set_seed(eval_seed)\n",
        "        self.model.eval()\n",
        "\n",
        "        all_preds, all_labels = [], []\n",
        "        layer_sum = None\n",
        "        layer_count = 0\n",
        "\n",
        "        for X, y in self.val_loader:\n",
        "            X = X.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "\n",
        "            if snr_db is not None:\n",
        "                X = add_gaussian_noise(X, float(snr_db))\n",
        "            if bias_beta is not None:\n",
        "                X = add_bias_drift(X, beta=bias_beta, mode=bias_mode, per_channel=True)\n",
        "\n",
        "            if measure_gate:\n",
        "                logits, gate_info_list, _ = self.model(X, return_gate_info=True)\n",
        "\n",
        "                layer_exp = []\n",
        "                K = gate_info_list[0][\"soft_probs\"].shape[-1]\n",
        "                deg_vals = torch.arange(1, K + 1, device=X.device).float()\n",
        "\n",
        "                for gi in gate_info_list:\n",
        "                    sp = gi[\"soft_probs\"]\n",
        "                    exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "                    layer_exp.append(exp_deg)\n",
        "\n",
        "                if layer_sum is None:\n",
        "                    layer_sum = np.zeros(len(layer_exp), dtype=np.float64)\n",
        "                layer_sum += np.array(layer_exp, dtype=np.float64)\n",
        "                layer_count += 1\n",
        "            else:\n",
        "                logits = self.model(X)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_preds = np.array(all_preds)\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "        per_class_f1 = f1_score(all_labels, all_preds, average=None)\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        gate_layer_exp = None\n",
        "        if measure_gate and layer_count > 0:\n",
        "            gate_layer_exp = (layer_sum / layer_count).tolist()\n",
        "\n",
        "        return {\n",
        "            \"acc\": acc,\n",
        "            \"macro_f1\": macro_f1,\n",
        "            \"per_class_f1\": per_class_f1,\n",
        "            \"cm\": cm,\n",
        "            \"gate_layer_expected_degree\": gate_layer_exp,\n",
        "        }\n",
        "\n",
        "    def instant_noise_sweep(self, snr_list_db):\n",
        "        results = []\n",
        "        for snr in snr_list_db:\n",
        "            out = self.eval_once(snr_db=snr, measure_gate=False, eval_seed=42)\n",
        "            out[\"snr_db\"] = (\"inf\" if snr is None else snr)\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "    def linear_snr_drift(self, snr_start=30, snr_end=0, steps=10):\n",
        "        snrs = np.linspace(snr_start, snr_end, steps).tolist()\n",
        "        time_series = []\n",
        "        for t, snr in enumerate(snrs):\n",
        "            out = self.eval_once(snr_db=float(snr), measure_gate=False, eval_seed=42)\n",
        "            time_series.append({\n",
        "                \"t\": t,\n",
        "                \"snr_db\": float(snr),\n",
        "                \"acc\": out[\"acc\"],\n",
        "                \"macro_f1\": out[\"macro_f1\"],\n",
        "                \"gate_layer_expected_degree\": out[\"gate_layer_expected_degree\"],\n",
        "            })\n",
        "        return time_series\n",
        "\n",
        "    def bias_drift_sweep(self, beta_list, mode=\"constant\"):\n",
        "        results = []\n",
        "        for beta in beta_list:\n",
        "            out = self.eval_once(bias_beta=beta, bias_mode=mode, measure_gate=False, eval_seed=42)\n",
        "            out[\"bias_beta\"] = beta\n",
        "            out[\"bias_mode\"] = mode\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# ROBUSTNESS TABLE UTILS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def _snr_label(snr):\n",
        "    return \"inf\" if snr is None else (f\"{snr:g}\" if isinstance(snr, (int, float)) else str(snr))\n",
        "\n",
        "def run_robustness_suite(model, test_loader, device,\n",
        "                         snr_instant=(None, 30, 20, 10, 5, 0),\n",
        "                         snr_drift_start=30.0, snr_drift_end=0.0, snr_drift_steps=10,\n",
        "                         beta_list=(0.05, 0.10, 0.20),\n",
        "                         bias_mode=\"constant\"):\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      instant: list of (snr_label, macro_f1)\n",
        "      linear : list of (snr_value(float), macro_f1)\n",
        "      bias   : list of (beta, macro_f1)\n",
        "    \"\"\"\n",
        "    evaluator = RobustEvaluator(model, test_loader, device)\n",
        "\n",
        "    # A) Instant noise sweep\n",
        "    A = evaluator.instant_noise_sweep(list(snr_instant))\n",
        "    instant = [(_snr_label(r[\"snr_db\"] if r[\"snr_db\"] == \"inf\" else r[\"snr_db\"]), float(r[\"macro_f1\"])) for r in A]\n",
        "    # NOTE: evaluator already sets \"snr_db\" to \"inf\" or number. 위는 그 포맷 유지.\n",
        "\n",
        "    # B) Linear SNR drift\n",
        "    B = evaluator.linear_snr_drift(snr_start=snr_drift_start, snr_end=snr_drift_end, steps=snr_drift_steps)\n",
        "    linear = [(float(row[\"snr_db\"]), float(row[\"macro_f1\"])) for row in B]\n",
        "\n",
        "    # C) Bias drift\n",
        "    C = evaluator.bias_drift_sweep(list(beta_list), mode=bias_mode)\n",
        "    bias = [(float(r[\"bias_beta\"]), float(r[\"macro_f1\"])) for r in C]\n",
        "\n",
        "    return {\"instant\": instant, \"linear\": linear, \"bias\": bias}\n",
        "\n",
        "\n",
        "def print_table_ready_rows(results, dataset_name=\"UCI-HAR\", value_name=\"Macro-F1\"):\n",
        "    \"\"\"\n",
        "    Prints:\n",
        "      - human-readable blocks\n",
        "      - LaTeX-ready rows (SNR/beta & value)\n",
        "    \"\"\"\n",
        "    print(\"\\n====================================================\")\n",
        "    print(f\"[Robustness Results] {dataset_name} ({value_name})\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    # Instant\n",
        "    print(\"\\n[Instant noise sweep]\")\n",
        "    for snr, v in results[\"instant\"]:\n",
        "        print(f\"  SNR={snr:>4s} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Instant\")\n",
        "    for snr, v in results[\"instant\"]:\n",
        "        # example: inf & 0.9671 \\\\\n",
        "        print(f\"{snr} & {v:.4f} \\\\\\\\\")\n",
        "\n",
        "    # Linear\n",
        "    print(\"\\n[Linear SNR drift]\")\n",
        "    for snr, v in results[\"linear\"]:\n",
        "        print(f\"  SNR={snr:>4.1f} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Linear\")\n",
        "    for snr, v in results[\"linear\"]:\n",
        "        print(f\"{snr:.1f} & {v:.4f} \\\\\\\\\")\n",
        "\n",
        "    # Bias\n",
        "    print(\"\\n[Bias drift]\")\n",
        "    for beta, v in results[\"bias\"]:\n",
        "        print(f\"  beta={beta:.2f} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Bias\")\n",
        "    for beta, v in results[\"bias\"]:\n",
        "        print(f\"{beta:.2f} & {v:.4f} \\\\\\\\\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/UCI_HAR\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    NUM_CLASSES = 6\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    train_dataset_raw = UCIHARDataset(DATA_PATH, split=\"train\", normalize=None)\n",
        "\n",
        "    stats_loader = DataLoader(\n",
        "        train_dataset_raw,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    mean, std = compute_train_stats(stats_loader, device=\"cpu\")\n",
        "    train_dataset = UCIHARDataset(DATA_PATH, split=\"train\", normalize=(mean, std))\n",
        "    test_dataset  = UCIHARDataset(DATA_PATH, split=\"test\",  normalize=(mean, std))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    set_seed(SEED)\n",
        "    model = PADReHAR(in_channels=9,\n",
        "                     seq_len=128,\n",
        "                     num_classes=NUM_CLASSES,\n",
        "                     hidden_dim=HIDDEN_DIM,\n",
        "                     num_layers=NUM_LAYERS,\n",
        "                     max_degree=MAX_DEGREE,\n",
        "                     gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "                     dropout=DROPOUT,\n",
        "                     temperature_initial=5.0,\n",
        "                     temperature_min=0.5\n",
        "                     ).to(DEVICE)\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\\n\")\n",
        "\n",
        "    model = train_model(model, train_loader, test_loader, DEVICE,\n",
        "                        lr=LR, weight_decay=WD, epochs=EPOCHS, seed=SEED)\n",
        "\n",
        "    model.eval()\n",
        "    results = run_robustness_suite(\n",
        "        model, test_loader, DEVICE,\n",
        "        snr_instant=(None, 30, 20, 10, 5, 0),\n",
        "        snr_drift_start=30.0, snr_drift_end=0.0, snr_drift_steps=10,\n",
        "        beta_list=(0.05, 0.10, 0.20),\n",
        "        bias_mode=\"constant\"\n",
        "    )\n",
        "\n",
        "    print_table_ready_rows(results, dataset_name=\"UCI-HAR\", value_name=\"Macro-F1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PAMAP2"
      ],
      "metadata": {
        "id": "mhWYKTt0lYOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def create_pamap2_windows(df: pd.DataFrame, window_size: int, step_size: int):\n",
        "    feature_cols = [\n",
        "        # hand\n",
        "        \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "        \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "        \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "        # chest\n",
        "        \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "        \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "        \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "        # ankle\n",
        "        \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "        \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "        \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "    ]  # C = 27\n",
        "\n",
        "    ORDERED_IDS = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
        "    old2new = {\n",
        "        1: 0,   # Lying\n",
        "        2: 1,   # Sitting\n",
        "        3: 2,   # Standing\n",
        "        4: 3,   # Walking\n",
        "        5: 4,   # Running\n",
        "        6: 5,   # Cycling\n",
        "        7: 6,   # Nordic walking\n",
        "        12: 7,  # Ascending stairs\n",
        "        13: 8,  # Descending stairs\n",
        "        16: 9,  # Vacuum cleaning\n",
        "        17: 10, # Ironing\n",
        "        24: 11, # Rope jumping\n",
        "    }\n",
        "    label_names = [\n",
        "        \"Lying\", \"Sitting\", \"Standing\", \"Walking\",\n",
        "        \"Running\", \"Cycling\", \"Nordic walking\",\n",
        "        \"Ascending stairs\", \"Descending stairs\",\n",
        "        \"Vacuum cleaning\", \"Ironing\", \"Rope jumping\",\n",
        "    ]\n",
        "\n",
        "    X_list, y_list, subj_list = [], [], []\n",
        "\n",
        "    for subj_id, g in df.groupby(\"subject_id\"):\n",
        "        if \"timestamp\" in g.columns:\n",
        "            g = g.sort_values(\"timestamp\")\n",
        "        else:\n",
        "            g = g.sort_index()\n",
        "\n",
        "        data_arr  = g[feature_cols].to_numpy(dtype=np.float32)\n",
        "        label_arr = g[\"activityID\"].to_numpy(dtype=np.int64)\n",
        "        L = data_arr.shape[0]\n",
        "\n",
        "        start = 0\n",
        "        while start + window_size <= L:\n",
        "            end = start + window_size\n",
        "            last_label_orig = int(label_arr[end - 1])\n",
        "\n",
        "            if last_label_orig == 0:\n",
        "                start += step_size\n",
        "                continue\n",
        "            if last_label_orig not in old2new:\n",
        "                start += step_size\n",
        "                continue\n",
        "\n",
        "            window_ct = data_arr[start:end].T\n",
        "            X_list.append(window_ct)\n",
        "            y_list.append(old2new[last_label_orig])\n",
        "            subj_list.append(int(subj_id))\n",
        "            start += step_size\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        raise RuntimeError(\"No windows created. Check window_size/step_size and label filtering.\")\n",
        "\n",
        "    X = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y = np.asarray(y_list, dtype=np.int64)\n",
        "    subj_ids = np.asarray(subj_list, dtype=np.int64)\n",
        "    return X, y, subj_ids, label_names\n",
        "\n",
        "\n",
        "class PAMAP2Dataset(Dataset):\n",
        "    def __init__(self, data_dir, window_size, step_size):\n",
        "        super().__init__()\n",
        "\n",
        "        csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
        "        if len(csv_files) == 0:\n",
        "            raise RuntimeError(f\"No CSV files found under {data_dir}\")\n",
        "\n",
        "        dfs = []\n",
        "        for fpath in sorted(csv_files):\n",
        "            df_i = pd.read_csv(fpath)\n",
        "\n",
        "            if \"subject_id\" not in df_i.columns:\n",
        "                m = re.findall(r\"\\d+\", os.path.basename(fpath))\n",
        "                subj_guess = int(m[0]) if len(m) > 0 else 0\n",
        "                df_i[\"subject_id\"] = subj_guess\n",
        "\n",
        "            dfs.append(df_i)\n",
        "\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        df = df.dropna(subset=[\"activityID\"])\n",
        "        df[\"activityID\"] = df[\"activityID\"].astype(np.int64)\n",
        "        df[\"subject_id\"] = df[\"subject_id\"].astype(np.int64)\n",
        "        if \"timestamp\" in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "        feature_cols = [\n",
        "            # hand\n",
        "            \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "            \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "            \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "            # chest\n",
        "            \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "            \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "            \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "            # ankle\n",
        "            \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "            \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "            \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "        ]\n",
        "\n",
        "        def _fill_subject_group(g):\n",
        "            if \"timestamp\" in g.columns:\n",
        "                g = g.sort_values(\"timestamp\")\n",
        "            else:\n",
        "                g = g.sort_index()\n",
        "            g[feature_cols] = (\n",
        "                g[feature_cols]\n",
        "                .interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
        "                .ffill()\n",
        "                .bfill()\n",
        "            )\n",
        "            return g\n",
        "\n",
        "        df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n",
        "        df[feature_cols] = df[feature_cols].fillna(0.0)\n",
        "\n",
        "        X, y, subj_ids, label_names = create_pamap2_windows(df, window_size, step_size)\n",
        "\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = y\n",
        "        self.subject_ids = subj_ids\n",
        "        self.label_names = label_names\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]\n",
        "        N, C, T = Xtr.shape\n",
        "\n",
        "        X2 = np.transpose(Xtr, (0, 2, 1)).reshape(-1, C)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = self.scaler\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = np.transpose(X, (0, 2, 1)).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        X_scaled = X2.reshape(N, T, C).transpose(0, 2, 1)\n",
        "\n",
        "        self.X = X_scaled.astype(np.float32)\n",
        "\n",
        "        print(\"Loaded PAMAP2 dataset\")\n",
        "        print(f\"X shape : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"Classes : {len(self.label_names)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long),\n",
        "            int(self.subject_ids[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "            Z.append(Z_ * Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        dw, logits, sp = self.degree_gate(x)\n",
        "        sel = dw.argmax(dim=-1)\n",
        "        out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": dw,\n",
        "                \"soft_probs\": sp,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (sel + 1).float().mean().item()\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, token_kernel=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res)\n",
        "\n",
        "            res2 = x\n",
        "            x = self.ffn[i](x)\n",
        "            x = self._ln(self.norms2[i], x + res2)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "def add_bias_drift(X, beta, mode=\"constant\", per_channel=True):\n",
        "    \"\"\"\n",
        "    beta: drift scale\n",
        "    mode: [\"constant\",\"brownian\"]\n",
        "    \"\"\"\n",
        "    B, C, T = X.shape\n",
        "    device = X.device\n",
        "\n",
        "    if mode == \"constant\":\n",
        "        if per_channel:\n",
        "            b = torch.randn(B, C, 1, device=device) * beta\n",
        "        else:\n",
        "            b = torch.randn(B, 1, 1, device=device) * beta\n",
        "        return X + b\n",
        "\n",
        "    if mode == \"brownian\":\n",
        "        if per_channel:\n",
        "            eps = torch.randn(B, C, T, device=device) * beta\n",
        "        else:\n",
        "            eps = torch.randn(B, 1, T, device=device) * beta\n",
        "        drift = torch.cumsum(eps, dim=-1)\n",
        "        return X + drift\n",
        "\n",
        "    raise ValueError(\"mode must be one of ['constant','brownian']\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# RobustEvaluator: instant SNR sweep, linear SNR drift, bias drift sweep\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class RobustEvaluator:\n",
        "    def __init__(self, model, val_loader, device):\n",
        "        self.model = model\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_once(self, snr_db=None, bias_beta=None, bias_mode=\"constant\", measure_gate=True):\n",
        "        self.model.eval()\n",
        "\n",
        "        all_preds, all_labels = [], []\n",
        "        layer_sum = None\n",
        "        layer_count = 0\n",
        "\n",
        "        for X, y, _ in self.val_loader:\n",
        "            X = X.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "\n",
        "            if snr_db is not None:\n",
        "                X = add_gaussian_noise(X, float(snr_db))\n",
        "            if bias_beta is not None:\n",
        "                X = add_bias_drift(X, beta=bias_beta, mode=bias_mode, per_channel=True)\n",
        "\n",
        "            if measure_gate:\n",
        "                logits, gate_info_list, _ = self.model(X, return_gate_info=True)\n",
        "\n",
        "                layer_exp = []\n",
        "                K = gate_info_list[0][\"soft_probs\"].shape[-1]\n",
        "                deg_vals = torch.arange(1, K + 1, device=X.device).float()\n",
        "\n",
        "                for gi in gate_info_list:\n",
        "                    sp = gi[\"soft_probs\"]\n",
        "                    exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "                    layer_exp.append(exp_deg)\n",
        "\n",
        "                if layer_sum is None:\n",
        "                    layer_sum = np.zeros(len(layer_exp), dtype=np.float64)\n",
        "                layer_sum += np.array(layer_exp, dtype=np.float64)\n",
        "                layer_count += 1\n",
        "            else:\n",
        "                logits = self.model(X)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_preds = np.array(all_preds)\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "        per_class_f1 = f1_score(all_labels, all_preds, average=None)\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        gate_layer_exp = None\n",
        "        if measure_gate and layer_count > 0:\n",
        "            gate_layer_exp = (layer_sum / layer_count).tolist()\n",
        "\n",
        "        return {\n",
        "            \"acc\": acc,\n",
        "            \"macro_f1\": macro_f1,\n",
        "            \"per_class_f1\": per_class_f1,\n",
        "            \"cm\": cm,\n",
        "            \"gate_layer_expected_degree\": gate_layer_exp,\n",
        "        }\n",
        "\n",
        "    def instant_noise_sweep(self, snr_list_db):\n",
        "        results = []\n",
        "        for snr in snr_list_db:\n",
        "            out = self.eval_once(snr_db=snr, measure_gate=False)\n",
        "            out[\"snr_db\"] = (\"inf\" if snr is None else snr)\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "    def linear_snr_drift(self, snr_start=30, snr_end=0, steps=10):\n",
        "        snrs = np.linspace(snr_start, snr_end, steps).tolist()\n",
        "        time_series = []\n",
        "        for t, snr in enumerate(snrs):\n",
        "            out = self.eval_once(snr_db=float(snr), measure_gate=False)\n",
        "            time_series.append({\n",
        "                \"t\": t,\n",
        "                \"snr_db\": float(snr),\n",
        "                \"acc\": out[\"acc\"],\n",
        "                \"macro_f1\": out[\"macro_f1\"],\n",
        "                \"gate_layer_expected_degree\": out[\"gate_layer_expected_degree\"],\n",
        "            })\n",
        "        return time_series\n",
        "\n",
        "    def bias_drift_sweep(self, beta_list, mode=\"constant\"):\n",
        "        results = []\n",
        "        for beta in beta_list:\n",
        "            out = self.eval_once(bias_beta=beta, bias_mode=mode, measure_gate=False)\n",
        "            out[\"bias_beta\"] = beta\n",
        "            out[\"bias_mode\"] = mode\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# ROBUSTNESS TABLE UTILS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def _snr_label(snr):\n",
        "    return \"inf\" if snr is None else (f\"{snr:g}\" if isinstance(snr, (int, float)) else str(snr))\n",
        "\n",
        "def run_robustness_suite(model, test_loader, device,\n",
        "                         snr_instant=(None, 30, 20, 10, 5, 0),\n",
        "                         snr_drift_start=30.0, snr_drift_end=0.0, snr_drift_steps=10,\n",
        "                         beta_list=(0.05, 0.10, 0.20),\n",
        "                         bias_mode=\"constant\"):\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      instant: list of (snr_label, macro_f1)\n",
        "      linear : list of (snr_value(float), macro_f1)\n",
        "      bias   : list of (beta, macro_f1)\n",
        "    \"\"\"\n",
        "    evaluator = RobustEvaluator(model, test_loader, device)\n",
        "\n",
        "    # A) Instant noise sweep\n",
        "    A = evaluator.instant_noise_sweep(list(snr_instant))\n",
        "    instant = [(_snr_label(r[\"snr_db\"] if r[\"snr_db\"] == \"inf\" else r[\"snr_db\"]), float(r[\"macro_f1\"])) for r in A]\n",
        "    # NOTE: evaluator already sets \"snr_db\" to \"inf\" or number. 위는 그 포맷 유지.\n",
        "\n",
        "    # B) Linear SNR drift\n",
        "    B = evaluator.linear_snr_drift(snr_start=snr_drift_start, snr_end=snr_drift_end, steps=snr_drift_steps)\n",
        "    linear = [(float(row[\"snr_db\"]), float(row[\"macro_f1\"])) for row in B]\n",
        "\n",
        "    # C) Bias drift\n",
        "    C = evaluator.bias_drift_sweep(list(beta_list), mode=bias_mode)\n",
        "    bias = [(float(r[\"bias_beta\"]), float(r[\"macro_f1\"])) for r in C]\n",
        "\n",
        "    return {\"instant\": instant, \"linear\": linear, \"bias\": bias}\n",
        "\n",
        "\n",
        "def print_table_ready_rows(results, dataset_name=\"UCI-HAR\", value_name=\"Macro-F1\"):\n",
        "    \"\"\"\n",
        "    Prints:\n",
        "      - human-readable blocks\n",
        "      - LaTeX-ready rows (SNR/beta & value)\n",
        "    \"\"\"\n",
        "    print(\"\\n====================================================\")\n",
        "    print(f\"[Robustness Results] {dataset_name} ({value_name})\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    # Instant\n",
        "    print(\"\\n[Instant noise sweep]\")\n",
        "    for snr, v in results[\"instant\"]:\n",
        "        print(f\"  SNR={snr:>4s} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Instant\")\n",
        "    for snr, v in results[\"instant\"]:\n",
        "        # example: inf & 0.9671 \\\\\n",
        "        print(f\"{snr} & {v:.4f} \\\\\\\\\")\n",
        "\n",
        "    # Linear\n",
        "    print(\"\\n[Linear SNR drift]\")\n",
        "    for snr, v in results[\"linear\"]:\n",
        "        print(f\"  SNR={snr:>4.1f} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Linear\")\n",
        "    for snr, v in results[\"linear\"]:\n",
        "        print(f\"{snr:.1f} & {v:.4f} \\\\\\\\\")\n",
        "\n",
        "    # Bias\n",
        "    print(\"\\n[Bias drift]\")\n",
        "    for beta, v in results[\"bias\"]:\n",
        "        print(f\"  beta={beta:.2f} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Bias\")\n",
        "    for beta, v in results[\"bias\"]:\n",
        "        print(f\"{beta:.2f} & {v:.4f} \\\\\\\\\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    NUM_CLASSES = 12\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE = 50\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = PAMAP2Dataset(\n",
        "        data_dir=DATA_PATH,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        step_size=STEP_SIZE\n",
        "    )\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    set_seed(SEED)\n",
        "    model = PADReHAR(in_channels=27,\n",
        "                     seq_len=100,\n",
        "                     num_classes=NUM_CLASSES,\n",
        "                     hidden_dim=HIDDEN_DIM,\n",
        "                     num_layers=NUM_LAYERS,\n",
        "                     max_degree=MAX_DEGREE,\n",
        "                     gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "                     dropout=DROPOUT,\n",
        "                     temperature_initial=5.0,\n",
        "                     temperature_min=0.5\n",
        "                     ).to(DEVICE)\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\\n\")\n",
        "\n",
        "    model = train_model(model, train_loader, test_loader, DEVICE,\n",
        "                        lr=LR, weight_decay=WD, epochs=EPOCHS, seed=SEED)\n",
        "\n",
        "    model.eval()\n",
        "    results = run_robustness_suite(\n",
        "        model, test_loader, DEVICE,\n",
        "        snr_instant=(None, 30, 20, 10, 5, 0),\n",
        "        snr_drift_start=30.0, snr_drift_end=0.0, snr_drift_steps=10,\n",
        "        beta_list=(0.05, 0.10, 0.20),\n",
        "        bias_mode=\"constant\"\n",
        "    )\n",
        "\n",
        "    print_table_ready_rows(results, dataset_name=\"PAMAP2\", value_name=\"Macro-F1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D96WjUotlaWb",
        "outputId": "c85dfffd-ea37-4296-b274-3fd2f493a2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2888806997.py:166: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded PAMAP2 dataset\n",
            "X shape : (38862, 27, 100)  (N, C, T)\n",
            "y shape : (38862,)  (N,)\n",
            "Classes : 12\n",
            "\n",
            "Model params: 79,749\n",
            "\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.2486 | TestF1=0.9229 | BestF1=0.9243 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.1551 | TestF1=0.9438 | BestF1=0.9438 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.1070 | TestF1=0.9524 | BestF1=0.9564 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0727 | TestF1=0.9576 | BestF1=0.9609 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0537 | TestF1=0.9654 | BestF1=0.9654 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0429 | TestF1=0.9644 | BestF1=0.9654 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0351 | TestF1=0.9664 | BestF1=0.9699 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0284 | TestF1=0.9690 | BestF1=0.9699 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0206 | TestF1=0.9671 | BestF1=0.9711 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0172 | TestF1=0.9711 | BestF1=0.9726 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0127 | TestF1=0.9721 | BestF1=0.9740 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0087 | TestF1=0.9751 | BestF1=0.9770 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0060 | TestF1=0.9792 | BestF1=0.9792 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0039 | TestF1=0.9769 | BestF1=0.9792 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0035 | TestF1=0.9798 | BestF1=0.9802 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0016 | TestF1=0.9807 | BestF1=0.9811 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0011 | TestF1=0.9807 | BestF1=0.9811 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0008 | TestF1=0.9818 | BestF1=0.9818 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0004 | TestF1=0.9818 | BestF1=0.9820 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0003 | TestF1=0.9819 | BestF1=0.9820 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9820\n",
            "\n",
            "====================================================\n",
            "[Robustness Results] PAMAP2 (Macro-F1)\n",
            "====================================================\n",
            "\n",
            "[Instant noise sweep]\n",
            "  SNR= inf | Macro-F1=0.9820\n",
            "  SNR=  30 | Macro-F1=0.9819\n",
            "  SNR=  20 | Macro-F1=0.9815\n",
            "  SNR=  10 | Macro-F1=0.9771\n",
            "  SNR=   5 | Macro-F1=0.9678\n",
            "  SNR=   0 | Macro-F1=0.9107\n",
            "\n",
            "[LaTeX rows] Instant\n",
            "inf & 0.9820 \\\\\n",
            "30 & 0.9819 \\\\\n",
            "20 & 0.9815 \\\\\n",
            "10 & 0.9771 \\\\\n",
            "5 & 0.9678 \\\\\n",
            "0 & 0.9107 \\\\\n",
            "\n",
            "[Linear SNR drift]\n",
            "  SNR=30.0 | Macro-F1=0.9818\n",
            "  SNR=26.7 | Macro-F1=0.9813\n",
            "  SNR=23.3 | Macro-F1=0.9818\n",
            "  SNR=20.0 | Macro-F1=0.9807\n",
            "  SNR=16.7 | Macro-F1=0.9811\n",
            "  SNR=13.3 | Macro-F1=0.9805\n",
            "  SNR=10.0 | Macro-F1=0.9787\n",
            "  SNR= 6.7 | Macro-F1=0.9741\n",
            "  SNR= 3.3 | Macro-F1=0.9590\n",
            "  SNR= 0.0 | Macro-F1=0.9136\n",
            "\n",
            "[LaTeX rows] Linear\n",
            "30.0 & 0.9818 \\\\\n",
            "26.7 & 0.9813 \\\\\n",
            "23.3 & 0.9818 \\\\\n",
            "20.0 & 0.9807 \\\\\n",
            "16.7 & 0.9811 \\\\\n",
            "13.3 & 0.9805 \\\\\n",
            "10.0 & 0.9787 \\\\\n",
            "6.7 & 0.9741 \\\\\n",
            "3.3 & 0.9590 \\\\\n",
            "0.0 & 0.9136 \\\\\n",
            "\n",
            "[Bias drift]\n",
            "  beta=0.05 | Macro-F1=0.9701\n",
            "  beta=0.10 | Macro-F1=0.9425\n",
            "  beta=0.20 | Macro-F1=0.8832\n",
            "\n",
            "[LaTeX rows] Bias\n",
            "0.05 & 0.9701 \\\\\n",
            "0.10 & 0.9425 \\\\\n",
            "0.20 & 0.8832 \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MHEALTH"
      ],
      "metadata": {
        "id": "CQIAD1sxm21F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def _load_single_mhealth_log(path: str, feature_cols: list[str]):\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        names=feature_cols + [\"label\"],\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def load_mhealth_dataframe(data_dir: str):\n",
        "    feature_cols = [\n",
        "        \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",      # 0,1,2\n",
        "        \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",      # 5,6,7\n",
        "        \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",   # 8,9,10\n",
        "        \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",            # 14,15,16\n",
        "        \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",         # 17,18,19\n",
        "    ]  # total 15 channels\n",
        "\n",
        "    log_files = glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\"))\n",
        "    if not log_files:\n",
        "        raise FileNotFoundError(f\"No mHealth_subject*.log files found in {data_dir}\")\n",
        "    print(f\"Found {len(log_files)} log files in {data_dir}\")\n",
        "\n",
        "    dfs = []\n",
        "    for fp in sorted(log_files):\n",
        "        dfs.append(_load_single_mhealth_log(fp, feature_cols))\n",
        "\n",
        "    full_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    full_df = full_df[full_df[\"label\"] != 0].copy()\n",
        "\n",
        "    full_df.loc[:, \"label\"] = full_df[\"label\"] - 1\n",
        "\n",
        "    return full_df, feature_cols\n",
        "\n",
        "\n",
        "def create_mhealth_windows(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: list[str],\n",
        "    window_size: int,\n",
        "    step_size: int,\n",
        "):\n",
        "    data_arr = df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    labels_arr = df[\"label\"].to_numpy(dtype=np.int64)\n",
        "    L = data_arr.shape[0]\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "    start = 0\n",
        "    while start + window_size <= L:\n",
        "        end = start + window_size\n",
        "        window_x = data_arr[start:end]\n",
        "        window_label = labels_arr[end - 1]\n",
        "        X_list.append(window_x.T)\n",
        "        y_list.append(int(window_label))\n",
        "        start += step_size\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No windows created. Check window_size / step_size / dataset length.\")\n",
        "\n",
        "    X_np = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y_np = np.array(y_list, dtype=np.int64)\n",
        "    return X_np, y_np\n",
        "\n",
        "\n",
        "class MHEALTHDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, window_size: int = 128, step_size: int = 64):\n",
        "        super().__init__()\n",
        "\n",
        "        full_df, feature_cols = load_mhealth_dataframe(data_dir)\n",
        "        X, y = create_mhealth_windows(full_df, feature_cols, window_size, step_size)\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.subjects = np.zeros(len(self.y), dtype=int)\n",
        "\n",
        "        self.label_names = [\n",
        "            \"Standing still\", \"Sitting and relaxing\", \"Lying down\",\n",
        "            \"Walking\", \"Climbing stairs\", \"Waist bends forward\",\n",
        "            \"Frontal elevation of arms\", \"Knees bending\", \"Cycling\",\n",
        "            \"Jogging\", \"Running\", \"Jump front & back\",\n",
        "        ]\n",
        "\n",
        "        print(\"Loaded MHEALTH dataset\")\n",
        "        print(f\"X shape : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"Classes : {len(self.label_names)}\")\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]\n",
        "        N, C, T = Xtr.shape\n",
        "        X2 = Xtr.transpose(0, 2, 1).reshape(-1, C)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = self.scaler\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = X.transpose(0, 2, 1).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        self.X = X2.reshape(N, T, C).transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx]).long(),\n",
        "            int(self.subjects[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "            Z.append(Z_ * Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        dw, logits, sp = self.degree_gate(x)\n",
        "        sel = dw.argmax(dim=-1)\n",
        "        out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": dw,\n",
        "                \"soft_probs\": sp,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (sel + 1).float().mean().item()\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, token_kernel=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res)\n",
        "\n",
        "            res2 = x\n",
        "            x = self.ffn[i](x)\n",
        "            x = self._ln(self.norms2[i], x + res2)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "def add_bias_drift(X, beta, mode=\"constant\", per_channel=True):\n",
        "    \"\"\"\n",
        "    beta: drift scale\n",
        "    mode: [\"constant\",\"brownian\"]\n",
        "    \"\"\"\n",
        "    B, C, T = X.shape\n",
        "    device = X.device\n",
        "\n",
        "    if mode == \"constant\":\n",
        "        if per_channel:\n",
        "            b = torch.randn(B, C, 1, device=device) * beta\n",
        "        else:\n",
        "            b = torch.randn(B, 1, 1, device=device) * beta\n",
        "        return X + b\n",
        "\n",
        "    if mode == \"brownian\":\n",
        "        if per_channel:\n",
        "            eps = torch.randn(B, C, T, device=device) * beta\n",
        "        else:\n",
        "            eps = torch.randn(B, 1, T, device=device) * beta\n",
        "        drift = torch.cumsum(eps, dim=-1)\n",
        "        return X + drift\n",
        "\n",
        "    raise ValueError(\"mode must be one of ['constant','brownian']\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# RobustEvaluator: instant SNR sweep, linear SNR drift, bias drift sweep\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class RobustEvaluator:\n",
        "    def __init__(self, model, val_loader, device):\n",
        "        self.model = model\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_once(self, snr_db=None, bias_beta=None, bias_mode=\"constant\", measure_gate=True):\n",
        "        self.model.eval()\n",
        "\n",
        "        all_preds, all_labels = [], []\n",
        "        layer_sum = None\n",
        "        layer_count = 0\n",
        "\n",
        "        for X, y, _ in self.val_loader:\n",
        "            X = X.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "\n",
        "            if snr_db is not None:\n",
        "                X = add_gaussian_noise(X, float(snr_db))\n",
        "            if bias_beta is not None:\n",
        "                X = add_bias_drift(X, beta=bias_beta, mode=bias_mode, per_channel=True)\n",
        "\n",
        "            if measure_gate:\n",
        "                logits, gate_info_list, _ = self.model(X, return_gate_info=True)\n",
        "\n",
        "                layer_exp = []\n",
        "                K = gate_info_list[0][\"soft_probs\"].shape[-1]\n",
        "                deg_vals = torch.arange(1, K + 1, device=X.device).float()\n",
        "\n",
        "                for gi in gate_info_list:\n",
        "                    sp = gi[\"soft_probs\"]\n",
        "                    exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "                    layer_exp.append(exp_deg)\n",
        "\n",
        "                if layer_sum is None:\n",
        "                    layer_sum = np.zeros(len(layer_exp), dtype=np.float64)\n",
        "                layer_sum += np.array(layer_exp, dtype=np.float64)\n",
        "                layer_count += 1\n",
        "            else:\n",
        "                logits = self.model(X)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_preds = np.array(all_preds)\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "        per_class_f1 = f1_score(all_labels, all_preds, average=None)\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        gate_layer_exp = None\n",
        "        if measure_gate and layer_count > 0:\n",
        "            gate_layer_exp = (layer_sum / layer_count).tolist()\n",
        "\n",
        "        return {\n",
        "            \"acc\": acc,\n",
        "            \"macro_f1\": macro_f1,\n",
        "            \"per_class_f1\": per_class_f1,\n",
        "            \"cm\": cm,\n",
        "            \"gate_layer_expected_degree\": gate_layer_exp,\n",
        "        }\n",
        "\n",
        "    def instant_noise_sweep(self, snr_list_db):\n",
        "        results = []\n",
        "        for snr in snr_list_db:\n",
        "            out = self.eval_once(snr_db=snr, measure_gate=False)\n",
        "            out[\"snr_db\"] = (\"inf\" if snr is None else snr)\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "    def linear_snr_drift(self, snr_start=30, snr_end=0, steps=10):\n",
        "        snrs = np.linspace(snr_start, snr_end, steps).tolist()\n",
        "        time_series = []\n",
        "        for t, snr in enumerate(snrs):\n",
        "            out = self.eval_once(snr_db=float(snr), measure_gate=False)\n",
        "            time_series.append({\n",
        "                \"t\": t,\n",
        "                \"snr_db\": float(snr),\n",
        "                \"acc\": out[\"acc\"],\n",
        "                \"macro_f1\": out[\"macro_f1\"],\n",
        "                \"gate_layer_expected_degree\": out[\"gate_layer_expected_degree\"],\n",
        "            })\n",
        "        return time_series\n",
        "\n",
        "    def bias_drift_sweep(self, beta_list, mode=\"constant\"):\n",
        "        results = []\n",
        "        for beta in beta_list:\n",
        "            out = self.eval_once(bias_beta=beta, bias_mode=mode, measure_gate=False)\n",
        "            out[\"bias_beta\"] = beta\n",
        "            out[\"bias_mode\"] = mode\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# ROBUSTNESS TABLE UTILS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def _snr_label(snr):\n",
        "    return \"inf\" if snr is None else (f\"{snr:g}\" if isinstance(snr, (int, float)) else str(snr))\n",
        "\n",
        "def run_robustness_suite(model, test_loader, device,\n",
        "                         snr_instant=(None, 30, 20, 10, 5, 0),\n",
        "                         snr_drift_start=30.0, snr_drift_end=0.0, snr_drift_steps=10,\n",
        "                         beta_list=(0.05, 0.10, 0.20),\n",
        "                         bias_mode=\"constant\"):\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      instant: list of (snr_label, macro_f1)\n",
        "      linear : list of (snr_value(float), macro_f1)\n",
        "      bias   : list of (beta, macro_f1)\n",
        "    \"\"\"\n",
        "    evaluator = RobustEvaluator(model, test_loader, device)\n",
        "\n",
        "    # A) Instant noise sweep\n",
        "    A = evaluator.instant_noise_sweep(list(snr_instant))\n",
        "    instant = [(_snr_label(r[\"snr_db\"] if r[\"snr_db\"] == \"inf\" else r[\"snr_db\"]), float(r[\"macro_f1\"])) for r in A]\n",
        "    # NOTE: evaluator already sets \"snr_db\" to \"inf\" or number. 위는 그 포맷 유지.\n",
        "\n",
        "    # B) Linear SNR drift\n",
        "    B = evaluator.linear_snr_drift(snr_start=snr_drift_start, snr_end=snr_drift_end, steps=snr_drift_steps)\n",
        "    linear = [(float(row[\"snr_db\"]), float(row[\"macro_f1\"])) for row in B]\n",
        "\n",
        "    # C) Bias drift\n",
        "    C = evaluator.bias_drift_sweep(list(beta_list), mode=bias_mode)\n",
        "    bias = [(float(r[\"bias_beta\"]), float(r[\"macro_f1\"])) for r in C]\n",
        "\n",
        "    return {\"instant\": instant, \"linear\": linear, \"bias\": bias}\n",
        "\n",
        "\n",
        "def print_table_ready_rows(results, dataset_name=\"UCI-HAR\", value_name=\"Macro-F1\"):\n",
        "    \"\"\"\n",
        "    Prints:\n",
        "      - human-readable blocks\n",
        "      - LaTeX-ready rows (SNR/beta & value)\n",
        "    \"\"\"\n",
        "    print(\"\\n====================================================\")\n",
        "    print(f\"[Robustness Results] {dataset_name} ({value_name})\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    # Instant\n",
        "    print(\"\\n[Instant noise sweep]\")\n",
        "    for snr, v in results[\"instant\"]:\n",
        "        print(f\"  SNR={snr:>4s} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Instant\")\n",
        "    for snr, v in results[\"instant\"]:\n",
        "        # example: inf & 0.9671 \\\\\n",
        "        print(f\"{snr} & {v:.4f} \\\\\\\\\")\n",
        "\n",
        "    # Linear\n",
        "    print(\"\\n[Linear SNR drift]\")\n",
        "    for snr, v in results[\"linear\"]:\n",
        "        print(f\"  SNR={snr:>4.1f} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Linear\")\n",
        "    for snr, v in results[\"linear\"]:\n",
        "        print(f\"{snr:.1f} & {v:.4f} \\\\\\\\\")\n",
        "\n",
        "    # Bias\n",
        "    print(\"\\n[Bias drift]\")\n",
        "    for beta, v in results[\"bias\"]:\n",
        "        print(f\"  beta={beta:.2f} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Bias\")\n",
        "    for beta, v in results[\"bias\"]:\n",
        "        print(f\"{beta:.2f} & {v:.4f} \\\\\\\\\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    NUM_CLASSES = 12\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 100\n",
        "    STEP_SIZE = 50\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = MHEALTHDataset(DATA_PATH, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    set_seed(SEED)\n",
        "    model = PADReHAR(in_channels=15,\n",
        "                     seq_len=100,\n",
        "                     num_classes=NUM_CLASSES,\n",
        "                     hidden_dim=HIDDEN_DIM,\n",
        "                     num_layers=NUM_LAYERS,\n",
        "                     max_degree=MAX_DEGREE,\n",
        "                     gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "                     dropout=DROPOUT,\n",
        "                     temperature_initial=5.0,\n",
        "                     temperature_min=0.5\n",
        "                     ).to(DEVICE)\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\\n\")\n",
        "\n",
        "    model = train_model(model, train_loader, test_loader, DEVICE,\n",
        "                        lr=LR, weight_decay=WD, epochs=EPOCHS, seed=SEED)\n",
        "\n",
        "    model.eval()\n",
        "    results = run_robustness_suite(\n",
        "        model, test_loader, DEVICE,\n",
        "        snr_instant=(None, 30, 20, 10, 5, 0),\n",
        "        snr_drift_start=30.0, snr_drift_end=0.0, snr_drift_steps=10,\n",
        "        beta_list=(0.05, 0.10, 0.20),\n",
        "        bias_mode=\"constant\"\n",
        "    )\n",
        "\n",
        "    print_table_ready_rows(results, dataset_name=\"MHEALTH\", value_name=\"Macro-F1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebMu531Am4It",
        "outputId": "318e81f2-f8df-4900-9d06-616a70d7569b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "Found 10 log files in /content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/MHEALTHDATASET\n",
            "Loaded MHEALTH dataset\n",
            "X shape : (6862, 15, 100)  (N, C, T)\n",
            "y shape : (6862,)  (N,)\n",
            "Classes : 12\n",
            "\n",
            "Model params: 79,173\n",
            "\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.0885 | TestF1=0.9819 | BestF1=0.9819 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.0457 | TestF1=0.9812 | BestF1=0.9819 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0314 | TestF1=0.9846 | BestF1=0.9860 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0232 | TestF1=0.9818 | BestF1=0.9860 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0174 | TestF1=0.9767 | BestF1=0.9860 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0076 | TestF1=0.9859 | BestF1=0.9860 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0090 | TestF1=0.9866 | BestF1=0.9873 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0021 | TestF1=0.9860 | BestF1=0.9873 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0024 | TestF1=0.9880 | BestF1=0.9880 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0018 | TestF1=0.9866 | BestF1=0.9880 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0007 | TestF1=0.9873 | BestF1=0.9880 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0006 | TestF1=0.9873 | BestF1=0.9880 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0004 | TestF1=0.9880 | BestF1=0.9880 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0005 | TestF1=0.9880 | BestF1=0.9880 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0003 | TestF1=0.9880 | BestF1=0.9880 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0005 | TestF1=0.9880 | BestF1=0.9880 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0004 | TestF1=0.9880 | BestF1=0.9880 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0003 | TestF1=0.9880 | BestF1=0.9880 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0002 | TestF1=0.9880 | BestF1=0.9880 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0004 | TestF1=0.9880 | BestF1=0.9880 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9880\n",
            "\n",
            "====================================================\n",
            "[Robustness Results] MHEALTH (Macro-F1)\n",
            "====================================================\n",
            "\n",
            "[Instant noise sweep]\n",
            "  SNR= inf | Macro-F1=0.9880\n",
            "  SNR=  30 | Macro-F1=0.9880\n",
            "  SNR=  20 | Macro-F1=0.9873\n",
            "  SNR=  10 | Macro-F1=0.9854\n",
            "  SNR=   5 | Macro-F1=0.8851\n",
            "  SNR=   0 | Macro-F1=0.6763\n",
            "\n",
            "[LaTeX rows] Instant\n",
            "inf & 0.9880 \\\\\n",
            "30 & 0.9880 \\\\\n",
            "20 & 0.9873 \\\\\n",
            "10 & 0.9854 \\\\\n",
            "5 & 0.8851 \\\\\n",
            "0 & 0.6763 \\\\\n",
            "\n",
            "[Linear SNR drift]\n",
            "  SNR=30.0 | Macro-F1=0.9880\n",
            "  SNR=26.7 | Macro-F1=0.9873\n",
            "  SNR=23.3 | Macro-F1=0.9880\n",
            "  SNR=20.0 | Macro-F1=0.9880\n",
            "  SNR=16.7 | Macro-F1=0.9880\n",
            "  SNR=13.3 | Macro-F1=0.9873\n",
            "  SNR=10.0 | Macro-F1=0.9873\n",
            "  SNR= 6.7 | Macro-F1=0.9388\n",
            "  SNR= 3.3 | Macro-F1=0.8322\n",
            "  SNR= 0.0 | Macro-F1=0.6808\n",
            "\n",
            "[LaTeX rows] Linear\n",
            "30.0 & 0.9880 \\\\\n",
            "26.7 & 0.9873 \\\\\n",
            "23.3 & 0.9880 \\\\\n",
            "20.0 & 0.9880 \\\\\n",
            "16.7 & 0.9880 \\\\\n",
            "13.3 & 0.9873 \\\\\n",
            "10.0 & 0.9873 \\\\\n",
            "6.7 & 0.9388 \\\\\n",
            "3.3 & 0.8322 \\\\\n",
            "0.0 & 0.6808 \\\\\n",
            "\n",
            "[Bias drift]\n",
            "  beta=0.05 | Macro-F1=0.9880\n",
            "  beta=0.10 | Macro-F1=0.9812\n",
            "  beta=0.20 | Macro-F1=0.9020\n",
            "\n",
            "[LaTeX rows] Bias\n",
            "0.05 & 0.9880 \\\\\n",
            "0.10 & 0.9812 \\\\\n",
            "0.20 & 0.9020 \\\\\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WISDM"
      ],
      "metadata": {
        "id": "RK5iFFsunPIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, copy, random, time, re, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Seed / Device\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def set_seed(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "DEVICE  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_GPU = DEVICE.type == \"cuda\"\n",
        "print(f\"Device: {DEVICE} | pin_memory: {USE_GPU}\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Dataset\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class WISDMDataset(Dataset):\n",
        "    def __init__(self, file_path: str, window_size: int = 80, step_size: int = 40):\n",
        "        super().__init__()\n",
        "        self.file_path = file_path\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            raise FileNotFoundError(f\"WISDM txt file not found: {file_path}\")\n",
        "\n",
        "        df = self._load_file(file_path)\n",
        "        self.X, self.y, self.subjects = self._create_windows(df)\n",
        "        self.unique_subjects = sorted(np.unique(self.subjects))\n",
        "\n",
        "        self.n_classes = int(len(np.unique(self.y)))\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Loaded WISDM dataset (single txt)\")\n",
        "        print(f\"  X shape       : {self.X.shape}  (N, C, T)\")\n",
        "        print(f\"  y shape       : {self.y.shape}  (N,)\")\n",
        "        print(f\"  subjects shape: {self.subjects.shape} (N,)\")\n",
        "        print(f\"  num classes   : {self.n_classes}\")\n",
        "        print(f\"  unique subjects: {self.unique_subjects[:10]} ... (total {len(self.unique_subjects)})\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def _load_file(self, file_path: str) -> pd.DataFrame:\n",
        "        WISDM_LABEL_MAP = {\n",
        "            \"walking\": 0,\n",
        "            \"jogging\": 1,\n",
        "            \"sitting\": 2,\n",
        "            \"standing\": 3,\n",
        "            \"upstairs\": 4,\n",
        "            \"downstairs\": 5,\n",
        "        }\n",
        "\n",
        "        with open(file_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        rows = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            line = line.replace(\";\", \"\")\n",
        "            parts = line.split(\",\")\n",
        "\n",
        "            if len(parts) != 6:\n",
        "                continue\n",
        "\n",
        "            subj, act, ts, x, y, z = parts\n",
        "            if x.strip() == \"\" or y.strip() == \"\" or z.strip() == \"\":\n",
        "                continue\n",
        "\n",
        "            act_norm = act.strip().lower()\n",
        "            if act_norm not in WISDM_LABEL_MAP:\n",
        "                continue\n",
        "\n",
        "            rows.append([subj, act_norm, ts, x, y, z])\n",
        "\n",
        "        if not rows:\n",
        "            raise ValueError(f\"No valid rows parsed from file: {file_path}\")\n",
        "\n",
        "        df = pd.DataFrame(rows, columns=[\"subject\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"])\n",
        "        df = df.replace([\"\", \"NaN\", \"nan\"], np.nan).dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        df[\"subject\"] = pd.to_numeric(df[\"subject\"], errors=\"coerce\")\n",
        "        df[\"x\"] = pd.to_numeric(df[\"x\"], errors=\"coerce\")\n",
        "        df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\")\n",
        "        df[\"z\"] = pd.to_numeric(df[\"z\"], errors=\"coerce\")\n",
        "        df = df.dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "        if df.empty:\n",
        "            raise ValueError(\"After cleaning, WISDM DataFrame is empty. Check file format.\")\n",
        "\n",
        "        df[\"subject\"] = df[\"subject\"].astype(int)\n",
        "        df[\"activity_id\"] = df[\"activity\"].map(WISDM_LABEL_MAP).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_windows(self, df: pd.DataFrame):\n",
        "        X_list, y_list, s_list = [], [], []\n",
        "\n",
        "        for subj_id in sorted(df[\"subject\"].unique()):\n",
        "            df_sub = df[df[\"subject\"] == subj_id]\n",
        "            data = df_sub[[\"x\", \"y\", \"z\"]].to_numpy(dtype=np.float32)\n",
        "            labels = df_sub[\"activity_id\"].to_numpy(dtype=np.int64)\n",
        "            L = len(df_sub)\n",
        "\n",
        "            start = 0\n",
        "            while start + self.window_size <= L:\n",
        "                end = start + self.window_size\n",
        "                window_x = data[start:end]\n",
        "                window_y = labels[end - 1]\n",
        "\n",
        "                X_list.append(window_x.T)\n",
        "                y_list.append(window_y)\n",
        "                s_list.append(subj_id)\n",
        "\n",
        "                start += self.step_size\n",
        "\n",
        "        if len(X_list) == 0:\n",
        "            raise ValueError(\"[WISDMDataset] No windows created. Try smaller window_size or check data.\")\n",
        "\n",
        "        X = np.stack(X_list, axis=0).astype(np.float32)\n",
        "        y = np.array(y_list, dtype=np.int64)\n",
        "        s = np.array(s_list, dtype=np.int64)\n",
        "        return X, y, s\n",
        "\n",
        "    def fit_scaler(self, indices):\n",
        "        Xtr = self.X[indices]  # (N,C,T)\n",
        "        N, C, T = Xtr.shape\n",
        "        X2 = np.transpose(Xtr, (0, 2, 1)).reshape(-1, C)  # (N*T, C)\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X2)\n",
        "        self.scaler = scaler\n",
        "        return scaler\n",
        "\n",
        "    def apply_scaler(self, scaler=None):\n",
        "        if scaler is None:\n",
        "            scaler = getattr(self, \"scaler\", None)\n",
        "        assert scaler is not None, \"Scaler is not fitted. Call fit_scaler() first.\"\n",
        "\n",
        "        X = self.X\n",
        "        N, C, T = X.shape\n",
        "        X2 = np.transpose(X, (0, 2, 1)).reshape(-1, C)\n",
        "        X2 = scaler.transform(X2)\n",
        "        self.X = X2.reshape(N, T, C).transpose(0, 2, 1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.FloatTensor(self.X[idx]),\n",
        "            torch.LongTensor([self.y[idx]])[0],\n",
        "            int(self.subjects[idx]),\n",
        "        )\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Compute-Aware Degree Gate\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class ComputeAwareDegreeGate(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels, gate_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(gate_hidden_dim, max_degree),\n",
        "        )\n",
        "\n",
        "        nn.init.zeros_(self.gate[-1].bias)\n",
        "        if max_degree >= 3:\n",
        "            self.gate[-1].bias.data[1] = 0.4\n",
        "        self.register_buffer(\"temperature\", torch.tensor(float(temperature_initial)))\n",
        "        self.temperature_min = float(temperature_min)\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        self.temperature.fill_(max(float(t), self.temperature_min))\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.gate(x)\n",
        "        soft_probs = F.softmax(logits / self.temperature, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            hard_idx = logits.argmax(dim=-1)\n",
        "            hard_oh = F.one_hot(hard_idx, num_classes=self.max_degree).float()\n",
        "            degree_w = hard_oh - soft_probs.detach() + soft_probs\n",
        "        else:\n",
        "            degree_w = F.one_hot(logits.argmax(dim=-1), num_classes=self.max_degree).float()\n",
        "\n",
        "        return degree_w, logits, soft_probs\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# PADRe Block\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 channels,\n",
        "                 seq_len,\n",
        "                 max_degree=3,\n",
        "                 token_kernel=11,\n",
        "                 gate_hidden_dim=16,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.degree_gate = ComputeAwareDegreeGate(\n",
        "            channels,\n",
        "            max_degree=max_degree,\n",
        "            gate_hidden_dim=gate_hidden_dim,\n",
        "            temperature_initial=temperature_initial,\n",
        "            temperature_min=temperature_min\n",
        "        )\n",
        "\n",
        "        self.channel_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.token_mixing = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_channel = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=1) for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.pre_hadamard_token = nn.ModuleList([\n",
        "            nn.Conv1d(channels, channels, kernel_size=token_kernel,\n",
        "                      padding=token_kernel // 2, groups=channels)\n",
        "            for _ in range(max_degree-1)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(channels)\n",
        "\n",
        "    def _build_Z(self, x, max_deg):\n",
        "        Y = [self.token_mixing[i](self.channel_mixing[i](x)) for i in range(max_deg)]\n",
        "        Z = [Y[0]]\n",
        "        for i in range(1, max_deg):\n",
        "            Z_ = self.pre_hadamard_token[i-1](self.pre_hadamard_channel[i-1](Z[-1]))\n",
        "            Z.append(Z_ * Y[i])\n",
        "        return Z\n",
        "\n",
        "    def _hard_forward(self, x, sel):\n",
        "        B = x.shape[0]\n",
        "        max_deg = max(1, min(int(sel.max().item()) + 1, self.max_degree))\n",
        "        Z = self._build_Z(x, max_deg)\n",
        "        Z_stack = torch.stack(Z, dim=0)\n",
        "        return Z_stack[sel, torch.arange(B, device=x.device)]\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        dw, logits, sp = self.degree_gate(x)\n",
        "        sel = dw.argmax(dim=-1)\n",
        "        out = self._hard_forward(x, sel)\n",
        "\n",
        "        out = self.norm(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        if return_gate_info:\n",
        "            return out, {\n",
        "                \"degree_selection\": dw,\n",
        "                \"soft_probs\": sp,\n",
        "                \"logits\": logits,\n",
        "                \"compute_cost\": (sel + 1).float().mean().item()\n",
        "            }\n",
        "        return out\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Adaptive PADRe Model\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class PADReHAR(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=9,\n",
        "                 seq_len=128,\n",
        "                 num_classes=6,\n",
        "                 hidden_dim=48,\n",
        "                 num_layers=3,\n",
        "                 max_degree=3,\n",
        "                 gate_hidden_dim=16,\n",
        "                 dropout=0.2,\n",
        "                 temperature_initial=5.0,\n",
        "                 temperature_min=0.5,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.max_degree = max_degree\n",
        "\n",
        "        self.input_proj = nn.Conv1d(in_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "        self.padre_blocks = nn.ModuleList([\n",
        "            PADReBlock(hidden_dim, seq_len, max_degree=max_degree, token_kernel=11,\n",
        "                       gate_hidden_dim=gate_hidden_dim,\n",
        "                       temperature_initial=temperature_initial,\n",
        "                       temperature_min=temperature_min)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.ffn = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=1),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=1),\n",
        "                nn.Dropout(dropout),\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def set_temperature(self, t):\n",
        "        for b in self.padre_blocks:\n",
        "            b.degree_gate.set_temperature(t)\n",
        "\n",
        "    def _ln(self, norm, x):\n",
        "        return norm(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "    def forward(self, x, return_gate_info=False):\n",
        "        x = self.input_proj(x)\n",
        "        gate_info_list = [] if return_gate_info else None\n",
        "        total_compute  = 0.0\n",
        "\n",
        "        for i, block in enumerate(self.padre_blocks):\n",
        "            res = x\n",
        "\n",
        "            if return_gate_info:\n",
        "                x, gi = block(x, return_gate_info=True)\n",
        "                gate_info_list.append(gi)\n",
        "                total_compute += gi[\"compute_cost\"]\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "            x = self._ln(self.norms1[i], x + res)\n",
        "\n",
        "            res2 = x\n",
        "            x = self.ffn[i](x)\n",
        "            x = self._ln(self.norms2[i], x + res2)\n",
        "\n",
        "        logits = self.classifier(self.global_pool(x).squeeze(-1))\n",
        "\n",
        "        return (logits, gate_info_list, total_compute) if return_gate_info else logits\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Utils\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def cosine_temperature(ep, total, tmax=5.0, tmin=0.5):\n",
        "    r = ep / max(total - 1, 1)\n",
        "    return tmin + (tmax - tmin) * 0.5 * (1.0 + np.cos(np.pi * r))\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Corruptions\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def add_gaussian_noise(X, snr_db):\n",
        "    \"\"\"\n",
        "    X: (B,C,T)\n",
        "    snr_db: float\n",
        "    \"\"\"\n",
        "    signal_power = (X ** 2).mean(dim=(1, 2), keepdim=True)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = torch.randn_like(X) * torch.sqrt(noise_power)\n",
        "    return X + noise\n",
        "\n",
        "\n",
        "def add_bias_drift(X, beta, mode=\"constant\", per_channel=True):\n",
        "    \"\"\"\n",
        "    beta: drift scale\n",
        "    mode: [\"constant\",\"brownian\"]\n",
        "    \"\"\"\n",
        "    B, C, T = X.shape\n",
        "    device = X.device\n",
        "\n",
        "    if mode == \"constant\":\n",
        "        if per_channel:\n",
        "            b = torch.randn(B, C, 1, device=device) * beta\n",
        "        else:\n",
        "            b = torch.randn(B, 1, 1, device=device) * beta\n",
        "        return X + b\n",
        "\n",
        "    if mode == \"brownian\":\n",
        "        if per_channel:\n",
        "            eps = torch.randn(B, C, T, device=device) * beta\n",
        "        else:\n",
        "            eps = torch.randn(B, 1, T, device=device) * beta\n",
        "        drift = torch.cumsum(eps, dim=-1)\n",
        "        return X + drift\n",
        "\n",
        "    raise ValueError(\"mode must be one of ['constant','brownian']\")\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# RobustEvaluator: instant SNR sweep, linear SNR drift, bias drift sweep\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "class RobustEvaluator:\n",
        "    def __init__(self, model, val_loader, device):\n",
        "        self.model = model\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def eval_once(self, snr_db=None, bias_beta=None, bias_mode=\"constant\", measure_gate=True):\n",
        "        self.model.eval()\n",
        "\n",
        "        all_preds, all_labels = [], []\n",
        "        layer_sum = None\n",
        "        layer_count = 0\n",
        "\n",
        "        for X, y, _ in self.val_loader:\n",
        "            X = X.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "\n",
        "            if snr_db is not None:\n",
        "                X = add_gaussian_noise(X, float(snr_db))\n",
        "            if bias_beta is not None:\n",
        "                X = add_bias_drift(X, beta=bias_beta, mode=bias_mode, per_channel=True)\n",
        "\n",
        "            if measure_gate:\n",
        "                logits, gate_info_list, _ = self.model(X, return_gate_info=True)\n",
        "\n",
        "                layer_exp = []\n",
        "                K = gate_info_list[0][\"soft_probs\"].shape[-1]\n",
        "                deg_vals = torch.arange(1, K + 1, device=X.device).float()\n",
        "\n",
        "                for gi in gate_info_list:\n",
        "                    sp = gi[\"soft_probs\"]\n",
        "                    exp_deg = (sp * deg_vals).sum(dim=-1).mean().item()\n",
        "                    layer_exp.append(exp_deg)\n",
        "\n",
        "                if layer_sum is None:\n",
        "                    layer_sum = np.zeros(len(layer_exp), dtype=np.float64)\n",
        "                layer_sum += np.array(layer_exp, dtype=np.float64)\n",
        "                layer_count += 1\n",
        "            else:\n",
        "                logits = self.model(X)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_preds = np.array(all_preds)\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "        per_class_f1 = f1_score(all_labels, all_preds, average=None)\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "        gate_layer_exp = None\n",
        "        if measure_gate and layer_count > 0:\n",
        "            gate_layer_exp = (layer_sum / layer_count).tolist()\n",
        "\n",
        "        return {\n",
        "            \"acc\": acc,\n",
        "            \"macro_f1\": macro_f1,\n",
        "            \"per_class_f1\": per_class_f1,\n",
        "            \"cm\": cm,\n",
        "            \"gate_layer_expected_degree\": gate_layer_exp,\n",
        "        }\n",
        "\n",
        "    def instant_noise_sweep(self, snr_list_db):\n",
        "        results = []\n",
        "        for snr in snr_list_db:\n",
        "            out = self.eval_once(snr_db=snr, measure_gate=False)\n",
        "            out[\"snr_db\"] = (\"inf\" if snr is None else snr)\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "    def linear_snr_drift(self, snr_start=30, snr_end=0, steps=10):\n",
        "        snrs = np.linspace(snr_start, snr_end, steps).tolist()\n",
        "        time_series = []\n",
        "        for t, snr in enumerate(snrs):\n",
        "            out = self.eval_once(snr_db=float(snr), measure_gate=False)\n",
        "            time_series.append({\n",
        "                \"t\": t,\n",
        "                \"snr_db\": float(snr),\n",
        "                \"acc\": out[\"acc\"],\n",
        "                \"macro_f1\": out[\"macro_f1\"],\n",
        "                \"gate_layer_expected_degree\": out[\"gate_layer_expected_degree\"],\n",
        "            })\n",
        "        return time_series\n",
        "\n",
        "    def bias_drift_sweep(self, beta_list, mode=\"constant\"):\n",
        "        results = []\n",
        "        for beta in beta_list:\n",
        "            out = self.eval_once(bias_beta=beta, bias_mode=mode, measure_gate=False)\n",
        "            out[\"bias_beta\"] = beta\n",
        "            out[\"bias_mode\"] = mode\n",
        "            results.append(out)\n",
        "        return results\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# ROBUSTNESS TABLE UTILS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def _snr_label(snr):\n",
        "    return \"inf\" if snr is None else (f\"{snr:g}\" if isinstance(snr, (int, float)) else str(snr))\n",
        "\n",
        "def run_robustness_suite(model, test_loader, device,\n",
        "                         snr_instant=(None, 30, 20, 10, 5, 0),\n",
        "                         snr_drift_start=30.0, snr_drift_end=0.0, snr_drift_steps=10,\n",
        "                         beta_list=(0.05, 0.10, 0.20),\n",
        "                         bias_mode=\"constant\"):\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      instant: list of (snr_label, macro_f1)\n",
        "      linear : list of (snr_value(float), macro_f1)\n",
        "      bias   : list of (beta, macro_f1)\n",
        "    \"\"\"\n",
        "    evaluator = RobustEvaluator(model, test_loader, device)\n",
        "\n",
        "    # A) Instant noise sweep\n",
        "    A = evaluator.instant_noise_sweep(list(snr_instant))\n",
        "    instant = [(_snr_label(r[\"snr_db\"] if r[\"snr_db\"] == \"inf\" else r[\"snr_db\"]), float(r[\"macro_f1\"])) for r in A]\n",
        "    # NOTE: evaluator already sets \"snr_db\" to \"inf\" or number. 위는 그 포맷 유지.\n",
        "\n",
        "    # B) Linear SNR drift\n",
        "    B = evaluator.linear_snr_drift(snr_start=snr_drift_start, snr_end=snr_drift_end, steps=snr_drift_steps)\n",
        "    linear = [(float(row[\"snr_db\"]), float(row[\"macro_f1\"])) for row in B]\n",
        "\n",
        "    # C) Bias drift\n",
        "    C = evaluator.bias_drift_sweep(list(beta_list), mode=bias_mode)\n",
        "    bias = [(float(r[\"bias_beta\"]), float(r[\"macro_f1\"])) for r in C]\n",
        "\n",
        "    return {\"instant\": instant, \"linear\": linear, \"bias\": bias}\n",
        "\n",
        "\n",
        "def print_table_ready_rows(results, dataset_name=\"UCI-HAR\", value_name=\"Macro-F1\"):\n",
        "    \"\"\"\n",
        "    Prints:\n",
        "      - human-readable blocks\n",
        "      - LaTeX-ready rows (SNR/beta & value)\n",
        "    \"\"\"\n",
        "    print(\"\\n====================================================\")\n",
        "    print(f\"[Robustness Results] {dataset_name} ({value_name})\")\n",
        "    print(\"====================================================\")\n",
        "\n",
        "    # Instant\n",
        "    print(\"\\n[Instant noise sweep]\")\n",
        "    for snr, v in results[\"instant\"]:\n",
        "        print(f\"  SNR={snr:>4s} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Instant\")\n",
        "    for snr, v in results[\"instant\"]:\n",
        "        # example: inf & 0.9671 \\\\\n",
        "        print(f\"{snr} & {v:.4f} \\\\\\\\\")\n",
        "\n",
        "    # Linear\n",
        "    print(\"\\n[Linear SNR drift]\")\n",
        "    for snr, v in results[\"linear\"]:\n",
        "        print(f\"  SNR={snr:>4.1f} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Linear\")\n",
        "    for snr, v in results[\"linear\"]:\n",
        "        print(f\"{snr:.1f} & {v:.4f} \\\\\\\\\")\n",
        "\n",
        "    # Bias\n",
        "    print(\"\\n[Bias drift]\")\n",
        "    for beta, v in results[\"bias\"]:\n",
        "        print(f\"  beta={beta:.2f} | {value_name}={v:.4f}\")\n",
        "\n",
        "    print(\"\\n[LaTeX rows] Bias\")\n",
        "    for beta, v in results[\"bias\"]:\n",
        "        print(f\"{beta:.2f} & {v:.4f} \\\\\\\\\")\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Train & Eval\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                lr=1e-3,\n",
        "                weight_decay=1e-4,\n",
        "                epochs=30,\n",
        "                seed=42\n",
        "    ):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_f1 = -1.0\n",
        "    best_state = None\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        temp = cosine_temperature(ep, epochs, tmax=5.0, tmin=0.5)\n",
        "        model.set_temperature(temp)\n",
        "\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        train_n = 0\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X), y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            bs = y.size(0)\n",
        "            train_loss_sum += loss.item() * bs\n",
        "            train_n += bs\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss = train_loss_sum / max(train_n, 1)\n",
        "\n",
        "        model.eval()\n",
        "        preds_all, labels_all = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                preds_all.extend(model(X).argmax(1).cpu().numpy())\n",
        "                labels_all.extend(y.cpu().numpy())\n",
        "        test_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (ep + 1) % 5 == 0:\n",
        "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Epoch {ep+1:02d}/{epochs} | LR={cur_lr:.4f} | Train Loss={train_loss:.4f} | TestF1={test_f1:.4f} | BestF1={best_f1:.4f} | Temp={temp:.3f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    print(f\"\\nBest Test Macro-F1: {best_f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# Main\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    DATA_PATH = \"/content/drive/MyDrive/Colab Notebooks/HAR/har_orig_datasets/WISDM_ar_v1.1_raw.txt\"\n",
        "    SEED = 42\n",
        "    NUM_WORKERS = 2 if USE_GPU else 0\n",
        "    PIN_MEMORY = USE_GPU\n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    EPOCHS = 100\n",
        "    NUM_CLASSES = 6\n",
        "    HIDDEN_DIM = 48\n",
        "    NUM_LAYERS = 3\n",
        "    MAX_DEGREE = 3\n",
        "    GATE_HIDDEN_DIM = 16\n",
        "    DROPOUT = 0.25\n",
        "    LR = 1e-3\n",
        "    WD = 1e-2\n",
        "\n",
        "    WINDOW_SIZE = 80\n",
        "    STEP_SIZE = 40\n",
        "\n",
        "    set_seed(SEED)\n",
        "\n",
        "    full_dataset = WISDMDataset(DATA_PATH, window_size=WINDOW_SIZE, step_size=STEP_SIZE)\n",
        "\n",
        "    n_total = len(full_dataset)\n",
        "    n_test = int(0.2 * n_total)\n",
        "    n_train = n_total - n_test\n",
        "\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    train_idx = np.array(train_dataset.indices, dtype=np.int64)\n",
        "    scaler = full_dataset.fit_scaler(train_idx)\n",
        "    full_dataset.apply_scaler(scaler)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "    set_seed(SEED)\n",
        "    model = PADReHAR(in_channels=3,\n",
        "                     seq_len=80,\n",
        "                     num_classes=NUM_CLASSES,\n",
        "                     hidden_dim=HIDDEN_DIM,\n",
        "                     num_layers=NUM_LAYERS,\n",
        "                     max_degree=MAX_DEGREE,\n",
        "                     gate_hidden_dim=GATE_HIDDEN_DIM,\n",
        "                     dropout=DROPOUT,\n",
        "                     temperature_initial=5.0,\n",
        "                     temperature_min=0.5\n",
        "                     ).to(DEVICE)\n",
        "    print(f\"\\nModel params: {count_parameters(model):,}\\n\")\n",
        "\n",
        "    model = train_model(model, train_loader, test_loader, DEVICE,\n",
        "                        lr=LR, weight_decay=WD, epochs=EPOCHS, seed=SEED)\n",
        "\n",
        "    model.eval()\n",
        "    results = run_robustness_suite(\n",
        "        model, test_loader, DEVICE,\n",
        "        snr_instant=(None, 30, 20, 10, 5, 0),\n",
        "        snr_drift_start=30.0, snr_drift_end=0.0, snr_drift_steps=10,\n",
        "        beta_list=(0.05, 0.10, 0.20),\n",
        "        bias_mode=\"constant\"\n",
        "    )\n",
        "\n",
        "    print_table_ready_rows(results, dataset_name=\"WISDM\", value_name=\"Macro-F1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz0sqzbynP19",
        "outputId": "e62ac4f5-b982-48da-fce7-1b2dfeccdd36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda | pin_memory: True\n",
            "================================================================================\n",
            "Loaded WISDM dataset (single txt)\n",
            "  X shape       : (27108, 3, 80)  (N, C, T)\n",
            "  y shape       : (27108,)  (N,)\n",
            "  subjects shape: (27108,) (N,)\n",
            "  num classes   : 6\n",
            "  unique subjects: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)] ... (total 36)\n",
            "================================================================================\n",
            "\n",
            "Model params: 78,303\n",
            "\n",
            "Epoch 05/100 | LR=0.0010 | Train Loss=0.0696 | TestF1=0.9606 | BestF1=0.9644 | Temp=4.982\n",
            "Epoch 10/100 | LR=0.0010 | Train Loss=0.0384 | TestF1=0.9706 | BestF1=0.9720 | Temp=4.909\n",
            "Epoch 15/100 | LR=0.0009 | Train Loss=0.0251 | TestF1=0.9719 | BestF1=0.9728 | Temp=4.782\n",
            "Epoch 20/100 | LR=0.0009 | Train Loss=0.0171 | TestF1=0.9746 | BestF1=0.9759 | Temp=4.603\n",
            "Epoch 25/100 | LR=0.0009 | Train Loss=0.0151 | TestF1=0.9731 | BestF1=0.9759 | Temp=4.378\n",
            "Epoch 30/100 | LR=0.0008 | Train Loss=0.0092 | TestF1=0.9775 | BestF1=0.9775 | Temp=4.113\n",
            "Epoch 35/100 | LR=0.0007 | Train Loss=0.0046 | TestF1=0.9754 | BestF1=0.9788 | Temp=3.813\n",
            "Epoch 40/100 | LR=0.0007 | Train Loss=0.0058 | TestF1=0.9732 | BestF1=0.9788 | Temp=3.486\n",
            "Epoch 45/100 | LR=0.0006 | Train Loss=0.0044 | TestF1=0.9710 | BestF1=0.9799 | Temp=3.141\n",
            "Epoch 50/100 | LR=0.0005 | Train Loss=0.0021 | TestF1=0.9778 | BestF1=0.9799 | Temp=2.786\n",
            "Epoch 55/100 | LR=0.0004 | Train Loss=0.0003 | TestF1=0.9786 | BestF1=0.9799 | Temp=2.430\n",
            "Epoch 60/100 | LR=0.0004 | Train Loss=0.0016 | TestF1=0.9765 | BestF1=0.9811 | Temp=2.082\n",
            "Epoch 65/100 | LR=0.0003 | Train Loss=0.0011 | TestF1=0.9775 | BestF1=0.9811 | Temp=1.751\n",
            "Epoch 70/100 | LR=0.0002 | Train Loss=0.0001 | TestF1=0.9777 | BestF1=0.9811 | Temp=1.445\n",
            "Epoch 75/100 | LR=0.0002 | Train Loss=0.0003 | TestF1=0.9785 | BestF1=0.9811 | Temp=1.172\n",
            "Epoch 80/100 | LR=0.0001 | Train Loss=0.0000 | TestF1=0.9798 | BestF1=0.9811 | Temp=0.938\n",
            "Epoch 85/100 | LR=0.0001 | Train Loss=0.0000 | TestF1=0.9803 | BestF1=0.9811 | Temp=0.750\n",
            "Epoch 90/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9803 | BestF1=0.9811 | Temp=0.612\n",
            "Epoch 95/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9788 | BestF1=0.9811 | Temp=0.528\n",
            "Epoch 100/100 | LR=0.0000 | Train Loss=0.0000 | TestF1=0.9794 | BestF1=0.9811 | Temp=0.500\n",
            "\n",
            "Best Test Macro-F1: 0.9811\n",
            "\n",
            "====================================================\n",
            "[Robustness Results] WISDM (Macro-F1)\n",
            "====================================================\n",
            "\n",
            "[Instant noise sweep]\n",
            "  SNR= inf | Macro-F1=0.9811\n",
            "  SNR=  30 | Macro-F1=0.9805\n",
            "  SNR=  20 | Macro-F1=0.9801\n",
            "  SNR=  10 | Macro-F1=0.9177\n",
            "  SNR=   5 | Macro-F1=0.6552\n",
            "  SNR=   0 | Macro-F1=0.3930\n",
            "\n",
            "[LaTeX rows] Instant\n",
            "inf & 0.9811 \\\\\n",
            "30 & 0.9805 \\\\\n",
            "20 & 0.9801 \\\\\n",
            "10 & 0.9177 \\\\\n",
            "5 & 0.6552 \\\\\n",
            "0 & 0.3930 \\\\\n",
            "\n",
            "[Linear SNR drift]\n",
            "  SNR=30.0 | Macro-F1=0.9804\n",
            "  SNR=26.7 | Macro-F1=0.9816\n",
            "  SNR=23.3 | Macro-F1=0.9797\n",
            "  SNR=20.0 | Macro-F1=0.9796\n",
            "  SNR=16.7 | Macro-F1=0.9778\n",
            "  SNR=13.3 | Macro-F1=0.9642\n",
            "  SNR=10.0 | Macro-F1=0.9102\n",
            "  SNR= 6.7 | Macro-F1=0.7371\n",
            "  SNR= 3.3 | Macro-F1=0.5751\n",
            "  SNR= 0.0 | Macro-F1=0.3971\n",
            "\n",
            "[LaTeX rows] Linear\n",
            "30.0 & 0.9804 \\\\\n",
            "26.7 & 0.9816 \\\\\n",
            "23.3 & 0.9797 \\\\\n",
            "20.0 & 0.9796 \\\\\n",
            "16.7 & 0.9778 \\\\\n",
            "13.3 & 0.9642 \\\\\n",
            "10.0 & 0.9102 \\\\\n",
            "6.7 & 0.7371 \\\\\n",
            "3.3 & 0.5751 \\\\\n",
            "0.0 & 0.3971 \\\\\n",
            "\n",
            "[Bias drift]\n",
            "  beta=0.05 | Macro-F1=0.9807\n",
            "  beta=0.10 | Macro-F1=0.9759\n",
            "  beta=0.20 | Macro-F1=0.9696\n",
            "\n",
            "[LaTeX rows] Bias\n",
            "0.05 & 0.9807 \\\\\n",
            "0.10 & 0.9759 \\\\\n",
            "0.20 & 0.9696 \\\\\n"
          ]
        }
      ]
    }
  ]
}